{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7ft8aR5TrJp"
      },
      "source": [
        "# **Assignment#3**\n",
        "\n",
        "Conducting a simple experiment for multi-classification using CIFAR-10 with MLP\n",
        "\n",
        "*   Hyperparameter tuning\n",
        "    - Find the combination of hyperparameters based on your own idea\n",
        "\n",
        "1.   No. of hidden layers, No. of hidden units\n",
        "2.   Learning rate\n",
        "3.   Weight initialization depending on activation function\n",
        "     - (Glorot) Xavier, (Kaiming) He\n",
        "4.   Regularization\n",
        "     - L2\n",
        "     - Dropout\n",
        "\n",
        "Note that you can report your final results to use other frameworks, such as Tensorflow, Theano, Julia, and etc.\n",
        "\n",
        "# **Submitting your final results about \"Accuracy\" in terms of training, validation and Testing based on the optimal combination of hyperparameters you set up (by 11/04/24 23:59)**\n",
        "\n",
        " - The report should include codes and final results (figures)\n",
        "----------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUjP9UsSMu9S"
      },
      "source": [
        "## Creating a directory \"results\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "n8OuOPC2Mne0"
      },
      "outputs": [],
      "source": [
        "!mkdir results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TilIT4qkba2P"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import argparse\n",
        "import numpy as np\n",
        "import time\n",
        "from copy import deepcopy\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQSb1eXLbk42"
      },
      "source": [
        "# **Data Preparation**\n",
        "- Use CIFAR10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qL7WI_SBbkJK",
        "outputId": "60cfc4e6-0878-4c43-ba6b-2300a519c953"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100.0%\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainset, valset = torch.utils.data.random_split(trainset, [40000, 10000])\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "partition = {'train': trainset, 'val':valset, 'test':testset}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUHZNA-eb7d6"
      },
      "source": [
        "# **Model Architecture**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXRPGdFsb3Zn",
        "outputId": "050b03d1-c410-4a22-a0c8-a6670163dca0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "MLP(\n",
              "  (fc1): Linear(in_features=3072, out_features=100, bias=True)\n",
              "  (linears): ModuleList(\n",
              "    (0-2): 3 x Linear(in_features=100, out_features=100, bias=True)\n",
              "  )\n",
              "  (bns): ModuleList(\n",
              "    (0-2): 3 x BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (fc2): Linear(in_features=100, out_features=10, bias=True)\n",
              "  (act): LeakyReLU(negative_slope=0.01)\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              ")"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, hid_dim, n_layer, act, dropout, use_bn, w_initial):\n",
        "        super(MLP, self).__init__()\n",
        "        self.in_dim = in_dim\n",
        "        self.out_dim = out_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layer = n_layer\n",
        "        self.act = act\n",
        "        self.dropout = dropout\n",
        "        self.use_bn = use_bn\n",
        "        self.w_initial = w_initial\n",
        "\n",
        "        # ====== Create Linear Layers ====== #\n",
        "        self.fc1 = nn.Linear(self.in_dim, self.hid_dim)\n",
        "\n",
        "        self.linears = nn.ModuleList()\n",
        "        self.bns = nn.ModuleList()\n",
        "        for i in range(self.n_layer-1):\n",
        "            self.linears.append(nn.Linear(self.hid_dim, self.hid_dim))\n",
        "            if self.use_bn:\n",
        "                self.bns.append(nn.BatchNorm1d(self.hid_dim))\n",
        "\n",
        "        self.fc2 = nn.Linear(self.hid_dim, self.out_dim)\n",
        "\n",
        "        # ====== Create Activation Function ====== #\n",
        "        if self.act == 'relu':\n",
        "            self.act = nn.ReLU()\n",
        "        elif self.act == 'leakyrelu':\n",
        "            self.act = nn.LeakyReLU()\n",
        "        elif self.act == 'tanh':\n",
        "            self.act = nn.Tanh()\n",
        "        elif self.act == 'sigmoid':\n",
        "            self.act = nn.Sigmoid()\n",
        "        else:\n",
        "            raise ValueError('no valid activation function selected!')\n",
        "\n",
        "        # ====== Create Regularization Layer ======= #\n",
        "        self.dropout = nn.Dropout(self.dropout)\n",
        "\n",
        "        if self.w_initial == 'xavier':\n",
        "            self.xavier_init()\n",
        "        elif self.w_initial == 'he':\n",
        "            self.he_init()\n",
        "        else:\n",
        "            raise ValueError(\"no valid initialization method selected!\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.act(self.fc1(x))\n",
        "        for i in range(len(self.linears)):\n",
        "            x = self.act(self.linears[i](x))\n",
        "            if self.use_bn:\n",
        "                x = self.bns[i](x)\n",
        "            x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "    def xavier_init(self):\n",
        "        for linear in self.linears:\n",
        "            nn.init.xavier_normal_(linear.weight)\n",
        "            linear.bias.data.fill_(0.01)\n",
        "\n",
        "    def he_init(self):\n",
        "        for linear in self.linears:\n",
        "            nn.init.kaiming_normal_(linear.weight, nonlinearity='leaky_relu') # nonlinearity='leaky_relu' (default)\n",
        "            linear.bias.data.fill_(0.01)\n",
        "\n",
        "\n",
        "net = MLP(3072, 10, 100, 4, 'leakyrelu', 0.1, True, 'he') # in_dim, out_dim, hid_dim, n_layer, act, dropout, use_bn, w_initial\n",
        "net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3mbuVBtccas"
      },
      "source": [
        "# **train, validation, test and experiment**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "O2TY5RMgcPyt"
      },
      "outputs": [],
      "source": [
        "def train(net, partition, optimizer, criterion, args):\n",
        "    trainloader = torch.utils.data.DataLoader(partition['train'],\n",
        "                                              batch_size=args.train_batch_size,\n",
        "                                              shuffle=True, num_workers=2)\n",
        "    net.train()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    train_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        optimizer.zero_grad()\n",
        "        # get the inputs\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.view(-1, 3072)\n",
        "        inputs = inputs.cuda()\n",
        "        labels = labels.cuda()\n",
        "        outputs = net(inputs)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    train_loss = train_loss / len(trainloader)\n",
        "    train_acc = 100 * correct / total\n",
        "    return net, train_loss, train_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "qPu1p1BgkRwy"
      },
      "outputs": [],
      "source": [
        "def validate(net, partition, criterion, args):\n",
        "    valloader = torch.utils.data.DataLoader(partition['val'],\n",
        "                                            batch_size=args.test_batch_size,\n",
        "                                            shuffle=False, num_workers=2)\n",
        "    net.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for data in valloader:\n",
        "            images, labels = data\n",
        "            images = images.view(-1, 3072)\n",
        "            images = images.cuda()\n",
        "            labels = labels.cuda()\n",
        "            outputs = net(images)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_loss = val_loss / len(valloader)\n",
        "        val_acc = 100 * correct / total\n",
        "    return val_loss, val_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "rG3ibHoBkV7w"
      },
      "outputs": [],
      "source": [
        "def test(net, partition, args):\n",
        "    testloader = torch.utils.data.DataLoader(partition['test'],\n",
        "                                             batch_size=args.test_batch_size,\n",
        "                                             shuffle=False, num_workers=2)\n",
        "    net.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            images = images.view(-1, 3072)\n",
        "            images = images.cuda()\n",
        "            labels = labels.cuda()\n",
        "\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        test_acc = 100 * correct / total\n",
        "    return test_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "64uRlb2UkayS"
      },
      "outputs": [],
      "source": [
        "def experiment(partition, args):\n",
        "    # net = MLP(3072, 10, 100, 4, 'relu', 0.1, True, 'he') # in_dim, out_dim, hid_dim, n_layer, act, dropout, use_bn, w_initial\n",
        "    net = MLP(args.in_dim, args.out_dim, args.hid_dim, args.n_layer, args.act, args.dropout, args.use_bn, args.w_initial)\n",
        "    net.cuda()\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    if args.optim == 'SGD':\n",
        "        optimizer = optim.SGD(net.parameters(), lr=args.lr, weight_decay=args.l2)\n",
        "    elif args.optim == 'RMSprop':\n",
        "        optimizer = optim.RMSprop(net.parameters(), lr=args.lr, weight_decay=args.l2)\n",
        "    elif args.optim == 'ADAM':\n",
        "        optimizer = optim.Adam(net.parameters(), lr=args.lr, weight_decay=args.l2)\n",
        "    else:\n",
        "        raise ValueError('In-valid optimizer choice')\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_accs = []\n",
        "    val_accs = []\n",
        "\n",
        "    for epoch in range(args.epoch):\n",
        "        ts = time.time()\n",
        "        net, train_loss, train_acc = train(net, partition, optimizer, criterion, args)\n",
        "        val_loss, val_acc = validate(net, partition, criterion, args)\n",
        "        te = time.time()\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        train_accs.append(train_acc)\n",
        "        val_accs.append(val_acc)\n",
        "\n",
        "        print('Epoch {}, Acc(train/val): {:2.2f}/{:2.2f}, Loss(train/val) {:2.2f}/{:2.2f}. Took {:2.2f} sec'.format(epoch, train_acc, val_acc, train_loss, val_loss, te-ts))\n",
        "\n",
        "    test_acc = test(net, partition, args)\n",
        "\n",
        "    result = {}\n",
        "    result['train_losses'] = train_losses\n",
        "    result['val_losses'] = val_losses\n",
        "    result['train_accs'] = train_accs\n",
        "    result['val_accs'] = val_accs\n",
        "    result['train_acc'] = train_acc\n",
        "    result['val_acc'] = val_acc\n",
        "    result['test_acc'] = test_acc\n",
        "    return vars(args), result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5kMxJFmeDWy"
      },
      "source": [
        "# **Managing experimental results**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "plWRyL4SeUXS"
      },
      "outputs": [],
      "source": [
        "import hashlib\n",
        "import json\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import pandas as pd\n",
        "\n",
        "def save_exp_result(setting, result):\n",
        "    exp_name = setting['exp_name']\n",
        "    del setting['epoch']\n",
        "    del setting['test_batch_size']\n",
        "\n",
        "    hash_key = hashlib.sha1(str(setting).encode()).hexdigest()[:6]\n",
        "    filename = './results/{}-{}.json'.format(exp_name, hash_key)\n",
        "    result.update(setting)\n",
        "    with open(filename, 'w') as f:\n",
        "        json.dump(result, f)\n",
        "\n",
        "\n",
        "def load_exp_result(exp_name):\n",
        "    dir_path = './results'\n",
        "    filenames = [f for f in listdir(dir_path) if isfile(join(dir_path, f)) if '.json' in f]\n",
        "    list_result = []\n",
        "    for filename in filenames:\n",
        "        if exp_name in filename:\n",
        "            with open(join(dir_path, filename), 'r') as infile:\n",
        "                results = json.load(infile)\n",
        "                list_result.append(results)\n",
        "    df = pd.DataFrame(list_result) # .drop(columns=[])\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRLhupB6lTSO"
      },
      "source": [
        "# **Visualizing the result figures**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "6rO5HpCLlSW1"
      },
      "outputs": [],
      "source": [
        "def plot_acc(var1, var2, df):\n",
        "\n",
        "    fig, ax = plt.subplots(1, 3)\n",
        "    fig.set_size_inches(15, 6)\n",
        "    sns.set_style(\"darkgrid\", {\"axes.facecolor\": \".9\"})\n",
        "\n",
        "    sns.barplot(x=var1, y='train_acc', hue=var2, data=df, ax=ax[0])\n",
        "    sns.barplot(x=var1, y='val_acc', hue=var2, data=df, ax=ax[1])\n",
        "    sns.barplot(x=var1, y='test_acc', hue=var2, data=df, ax=ax[2])\n",
        "\n",
        "    ax[0].set_title('Train Accuracy')\n",
        "    ax[1].set_title('Validation Accuracy')\n",
        "    ax[2].set_title('Test Accuracy')\n",
        "\n",
        "\n",
        "def plot_loss_variation(var1, var2, df, **kwargs):\n",
        "\n",
        "    list_v1 = df[var1].unique()\n",
        "    list_v2 = df[var2].unique()\n",
        "    list_data = []\n",
        "\n",
        "    for value1 in list_v1:\n",
        "        for value2 in list_v2:\n",
        "            row = df.loc[df[var1]==value1]\n",
        "            row = row.loc[df[var2]==value2]\n",
        "\n",
        "            train_losses = list(row.train_losses)[0]\n",
        "            val_losses = list(row.val_losses)[0]\n",
        "\n",
        "            for epoch, train_loss in enumerate(train_losses):\n",
        "                list_data.append({'type':'train', 'loss':train_loss, 'epoch':epoch, var1:value1, var2:value2})\n",
        "            for epoch, val_loss in enumerate(val_losses):\n",
        "                list_data.append({'type':'val', 'loss':val_loss, 'epoch':epoch, var1:value1, var2:value2})\n",
        "\n",
        "    df = pd.DataFrame(list_data)\n",
        "    g = sns.FacetGrid(df, row=var2, col=var1, hue='type', **kwargs)\n",
        "    g = g.map(plt.plot, 'epoch', 'loss', marker='.')\n",
        "    g.add_legend()\n",
        "    g.fig.suptitle('Train loss vs Val loss')\n",
        "    plt.subplots_adjust(top=0.83) # Adjusting the value of top if the title and figure overlap each other\n",
        "\n",
        "\n",
        "def plot_acc_variation(var1, var2, df, **kwargs):\n",
        "    list_v1 = df[var1].unique()\n",
        "    list_v2 = df[var2].unique()\n",
        "    list_data = []\n",
        "\n",
        "    for value1 in list_v1:\n",
        "        for value2 in list_v2:\n",
        "            row = df.loc[df[var1]==value1]\n",
        "            row = row.loc[df[var2]==value2]\n",
        "\n",
        "            train_accs = list(row.train_accs)[0]\n",
        "            val_accs = list(row.val_accs)[0]\n",
        "            test_acc = list(row.test_acc)[0]\n",
        "\n",
        "            for epoch, train_acc in enumerate(train_accs):\n",
        "                list_data.append({'type':'train', 'Acc':train_acc, 'test_acc':test_acc, 'epoch':epoch, var1:value1, var2:value2})\n",
        "            for epoch, val_acc in enumerate(val_accs):\n",
        "                list_data.append({'type':'val', 'Acc':val_acc, 'test_acc':test_acc, 'epoch':epoch, var1:value1, var2:value2})\n",
        "\n",
        "    df = pd.DataFrame(list_data)\n",
        "    g = sns.FacetGrid(df, row=var2, col=var1, hue='type', **kwargs)\n",
        "    g = g.map(plt.plot, 'epoch', 'Acc', marker='.')\n",
        "\n",
        "    def show_acc(x, y, metric, **kwargs):\n",
        "        plt.scatter(x, y, alpha=0.3, s=1)\n",
        "        metric = \"Test Acc: {:1.3f}\".format(list(metric.values)[0])\n",
        "        plt.text(0.05, 0.95, metric,  horizontalalignment='left', verticalalignment='center', transform=plt.gca().transAxes, bbox=dict(facecolor='yellow', alpha=0.5, boxstyle=\"round,pad=0.1\"))\n",
        "    g = g.map(show_acc, 'epoch', 'Acc', 'test_acc')\n",
        "\n",
        "    g.add_legend()\n",
        "    g.fig.suptitle('Train Accuracy vs Val Accuracy')\n",
        "    plt.subplots_adjust(top=0.89)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WflNh2vzl9Eb"
      },
      "source": [
        "# **Experiment No.1 : No. of hidden layers and hidden units**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "PF9Z-SgJl59v",
        "outputId": "fff855e3-cff9-4c52-983d-2d2365c64e91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "|   iter    |  target   |    act    |  dropout  |  hid_dim  |    l2     |    lr     |  n_layer  |   optim   | test_b... | train_... |  use_bn   | w_initial |\n",
            "-------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=3, hid_dim=757, act='tanh', dropout=np.float64(0.14306966747518973), use_bn=True, l2=np.float64(0.0005517634543138084), w_initial='he', optim='ADAM', lr=np.float64(0.007194970228885846), epoch=10, train_batch_size=125, test_batch_size=176)\n",
            "Epoch 0, Acc(train/val): 24.28/29.99, Loss(train/val) 2.13/1.95. Took 7.88 sec\n",
            "Epoch 1, Acc(train/val): 31.20/31.66, Loss(train/val) 1.90/1.89. Took 7.78 sec\n",
            "Epoch 2, Acc(train/val): 32.94/32.50, Loss(train/val) 1.86/1.88. Took 7.20 sec\n",
            "Epoch 3, Acc(train/val): 32.02/31.19, Loss(train/val) 1.88/1.90. Took 8.26 sec\n",
            "Epoch 4, Acc(train/val): 32.16/32.03, Loss(train/val) 1.89/1.88. Took 8.38 sec\n",
            "Epoch 5, Acc(train/val): 31.74/33.14, Loss(train/val) 1.89/1.86. Took 8.15 sec\n",
            "Epoch 6, Acc(train/val): 31.80/30.50, Loss(train/val) 1.89/1.92. Took 7.57 sec\n",
            "Epoch 7, Acc(train/val): 31.36/28.76, Loss(train/val) 1.90/1.96. Took 6.74 sec\n",
            "Epoch 8, Acc(train/val): 31.22/31.44, Loss(train/val) 1.89/1.89. Took 6.90 sec\n",
            "Epoch 9, Acc(train/val): 31.32/33.30, Loss(train/val) 1.89/1.87. Took 7.18 sec\n",
            "| \u001b[39m1        \u001b[39m | \u001b[39m-1.867   \u001b[39m | \u001b[39m0.6965   \u001b[39m | \u001b[39m0.1431   \u001b[39m | \u001b[39m757.9    \u001b[39m | \u001b[39m0.0005518\u001b[39m | \u001b[39m0.007195 \u001b[39m | \u001b[39m3.116    \u001b[39m | \u001b[39m2.942    \u001b[39m | \u001b[39m176.6    \u001b[39m | \u001b[39m125.2    \u001b[39m | \u001b[39m0.3921   \u001b[39m | \u001b[39m0.3432   \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=1, hid_dim=273, act='tanh', dropout=np.float64(0.2192861223398122), use_bn=False, l2=np.float64(0.00039864621107510103), w_initial='xavier', optim='SGD', lr=np.float64(0.0073802160619146255), epoch=10, train_batch_size=138, test_batch_size=137, exp_name='exp0')\n",
            "Epoch 0, Acc(train/val): 30.52/34.37, Loss(train/val) 2.00/1.90. Took 7.67 sec\n",
            "Epoch 1, Acc(train/val): 36.53/36.73, Loss(train/val) 1.85/1.84. Took 7.16 sec\n",
            "Epoch 2, Acc(train/val): 38.20/37.97, Loss(train/val) 1.81/1.81. Took 6.79 sec\n",
            "Epoch 3, Acc(train/val): 39.32/38.43, Loss(train/val) 1.78/1.79. Took 7.22 sec\n",
            "Epoch 4, Acc(train/val): 39.91/38.64, Loss(train/val) 1.76/1.78. Took 6.62 sec\n",
            "Epoch 5, Acc(train/val): 40.46/39.11, Loss(train/val) 1.74/1.77. Took 7.04 sec\n",
            "Epoch 6, Acc(train/val): 41.11/39.44, Loss(train/val) 1.72/1.75. Took 7.39 sec\n",
            "Epoch 7, Acc(train/val): 41.55/39.85, Loss(train/val) 1.71/1.75. Took 7.71 sec\n",
            "Epoch 8, Acc(train/val): 41.96/40.13, Loss(train/val) 1.70/1.74. Took 7.35 sec\n",
            "Epoch 9, Acc(train/val): 42.42/40.33, Loss(train/val) 1.69/1.73. Took 7.28 sec\n",
            "| \u001b[35m2        \u001b[39m | \u001b[35m-1.73    \u001b[39m | \u001b[35m0.729    \u001b[39m | \u001b[35m0.2193   \u001b[39m | \u001b[35m273.1    \u001b[39m | \u001b[35m0.0003986\u001b[39m | \u001b[35m0.00738  \u001b[39m | \u001b[35m1.912    \u001b[39m | \u001b[35m0.5264   \u001b[39m | \u001b[35m138.0    \u001b[39m | \u001b[35m138.0    \u001b[39m | \u001b[35m0.6344   \u001b[39m | \u001b[35m0.8494   \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=2, hid_dim=2195, act='tanh', dropout=np.float64(0.30551175533879144), use_bn=True, l2=np.float64(0.0003236359549393251), w_initial='he', optim='SGD', lr=np.float64(0.0036185247675675192), epoch=10, train_batch_size=27, test_batch_size=163, exp_name='exp1')\n",
            "Epoch 0, Acc(train/val): 33.84/35.88, Loss(train/val) 1.90/1.88. Took 9.06 sec\n",
            "Epoch 1, Acc(train/val): 37.88/35.80, Loss(train/val) 1.79/1.80. Took 11.65 sec\n",
            "Epoch 2, Acc(train/val): 39.69/38.35, Loss(train/val) 1.75/1.75. Took 10.64 sec\n",
            "Epoch 3, Acc(train/val): 40.70/39.68, Loss(train/val) 1.71/1.74. Took 9.08 sec\n",
            "Epoch 4, Acc(train/val): 42.06/41.07, Loss(train/val) 1.68/1.72. Took 10.20 sec\n",
            "Epoch 5, Acc(train/val): 42.84/42.29, Loss(train/val) 1.66/1.68. Took 10.15 sec\n",
            "Epoch 6, Acc(train/val): 44.06/43.38, Loss(train/val) 1.63/1.65. Took 10.68 sec\n",
            "Epoch 7, Acc(train/val): 44.74/43.03, Loss(train/val) 1.60/1.67. Took 9.63 sec\n",
            "Epoch 8, Acc(train/val): 45.48/43.70, Loss(train/val) 1.58/1.64. Took 9.99 sec\n",
            "Epoch 9, Acc(train/val): 46.03/43.52, Loss(train/val) 1.56/1.64. Took 9.42 sec\n",
            "| \u001b[35m3        \u001b[39m | \u001b[35m-1.642   \u001b[39m | \u001b[35m0.7245   \u001b[39m | \u001b[35m0.3055   \u001b[39m | \u001b[35m2.195e+03\u001b[39m | \u001b[35m0.0003236\u001b[39m | \u001b[35m0.003619 \u001b[39m | \u001b[35m2.141    \u001b[39m | \u001b[35m0.8811   \u001b[39m | \u001b[35m163.0    \u001b[39m | \u001b[35m27.21    \u001b[39m | \u001b[35m0.4337   \u001b[39m | \u001b[35m0.4309   \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=5, hid_dim=1005, act='leakyrelu', dropout=np.float64(0.212915145147914), use_bn=True, l2=np.float64(0.0004269249556558455), w_initial='he', optim='RMSprop', lr=np.float64(0.008933998242008231), epoch=10, train_batch_size=33, test_batch_size=161, exp_name='exp2')\n",
            "Epoch 0, Acc(train/val): 20.09/10.24, Loss(train/val) 2.63/2.57. Took 11.39 sec\n",
            "Epoch 1, Acc(train/val): 22.35/15.52, Loss(train/val) 2.06/2.30. Took 9.65 sec\n",
            "Epoch 2, Acc(train/val): 22.91/8.67, Loss(train/val) 2.04/3.51. Took 10.81 sec\n",
            "Epoch 3, Acc(train/val): 23.43/14.96, Loss(train/val) 2.04/2.40. Took 9.81 sec\n",
            "Epoch 4, Acc(train/val): 22.89/13.43, Loss(train/val) 2.04/2.62. Took 9.80 sec\n",
            "Epoch 5, Acc(train/val): 23.32/14.74, Loss(train/val) 2.04/2.65. Took 10.85 sec\n",
            "Epoch 6, Acc(train/val): 23.90/20.21, Loss(train/val) 2.03/2.34. Took 10.16 sec\n",
            "Epoch 7, Acc(train/val): 23.24/12.47, Loss(train/val) 2.04/2.69. Took 9.17 sec\n",
            "Epoch 8, Acc(train/val): 23.12/20.11, Loss(train/val) 2.04/2.13. Took 9.81 sec\n",
            "Epoch 9, Acc(train/val): 23.38/19.01, Loss(train/val) 2.03/2.13. Took 10.10 sec\n",
            "| \u001b[39m4        \u001b[39m | \u001b[39m-2.13    \u001b[39m | \u001b[39m0.4937   \u001b[39m | \u001b[39m0.2129   \u001b[39m | \u001b[39m1.006e+03\u001b[39m | \u001b[39m0.0004269\u001b[39m | \u001b[39m0.008934 \u001b[39m | \u001b[39m5.721    \u001b[39m | \u001b[39m1.506    \u001b[39m | \u001b[39m161.2    \u001b[39m | \u001b[39m33.14    \u001b[39m | \u001b[39m0.3173   \u001b[39m | \u001b[39m0.4148   \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=4, hid_dim=1500, act='sigmoid', dropout=np.float64(0.12522768269825335), use_bn=False, l2=np.float64(0.0009855742258250944), w_initial='he', optim='SGD', lr=np.float64(0.005195331707478834), epoch=10, train_batch_size=155, test_batch_size=212, exp_name='exp3')\n",
            "Epoch 0, Acc(train/val): 10.09/10.08, Loss(train/val) 2.31/2.32. Took 7.19 sec\n",
            "Epoch 1, Acc(train/val): 10.14/10.11, Loss(train/val) 2.31/2.33. Took 7.15 sec\n",
            "Epoch 2, Acc(train/val): 10.01/9.12, Loss(train/val) 2.31/2.35. Took 7.01 sec\n",
            "Epoch 3, Acc(train/val): 9.97/11.66, Loss(train/val) 2.31/2.30. Took 7.17 sec\n",
            "Epoch 4, Acc(train/val): 10.12/10.11, Loss(train/val) 2.31/2.33. Took 6.98 sec\n",
            "Epoch 5, Acc(train/val): 9.91/10.08, Loss(train/val) 2.31/2.33. Took 7.41 sec\n",
            "Epoch 6, Acc(train/val): 10.07/10.41, Loss(train/val) 2.31/2.31. Took 7.13 sec\n",
            "Epoch 7, Acc(train/val): 10.03/9.12, Loss(train/val) 2.31/2.31. Took 7.23 sec\n",
            "Epoch 8, Acc(train/val): 10.01/10.08, Loss(train/val) 2.31/2.31. Took 7.38 sec\n",
            "Epoch 9, Acc(train/val): 10.42/10.41, Loss(train/val) 2.31/2.31. Took 7.13 sec\n",
            "| \u001b[39m5        \u001b[39m | \u001b[39m-2.314   \u001b[39m | \u001b[39m0.8663   \u001b[39m | \u001b[39m0.1252   \u001b[39m | \u001b[39m1.501e+03\u001b[39m | \u001b[39m0.0009856\u001b[39m | \u001b[39m0.005195 \u001b[39m | \u001b[39m4.064    \u001b[39m | \u001b[39m0.3619   \u001b[39m | \u001b[39m212.2    \u001b[39m | \u001b[39m156.0    \u001b[39m | \u001b[39m0.5451   \u001b[39m | \u001b[39m0.3428   \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=5, hid_dim=2619, act='sigmoid', dropout=np.float64(0.4274913525307773), use_bn=False, l2=np.float64(0.00046924493940575133), w_initial='xavier', optim='ADAM', lr=np.float64(0.006641529145233584), epoch=10, train_batch_size=11, test_batch_size=177, exp_name='exp4')\n",
            "Epoch 0, Acc(train/val): 9.79/10.34, Loss(train/val) 6.06/3.80. Took 36.05 sec\n",
            "Epoch 1, Acc(train/val): 10.22/10.11, Loss(train/val) 6.15/4.61. Took 35.48 sec\n",
            "Epoch 2, Acc(train/val): 9.99/10.08, Loss(train/val) 6.10/6.68. Took 35.83 sec\n",
            "Epoch 3, Acc(train/val): 9.91/10.11, Loss(train/val) 6.13/3.91. Took 36.77 sec\n",
            "Epoch 4, Acc(train/val): 9.86/9.61, Loss(train/val) 6.17/5.13. Took 36.64 sec\n",
            "Epoch 5, Acc(train/val): 9.97/10.11, Loss(train/val) 6.16/4.40. Took 36.26 sec\n",
            "Epoch 6, Acc(train/val): 10.05/9.88, Loss(train/val) 6.11/5.91. Took 34.82 sec\n",
            "Epoch 7, Acc(train/val): 9.98/10.34, Loss(train/val) 6.04/4.45. Took 34.01 sec\n",
            "Epoch 8, Acc(train/val): 10.14/9.88, Loss(train/val) 6.06/4.39. Took 34.97 sec\n",
            "Epoch 9, Acc(train/val): 9.94/10.11, Loss(train/val) 5.99/3.69. Took 33.97 sec\n",
            "| \u001b[39m6        \u001b[39m | \u001b[39m-3.686   \u001b[39m | \u001b[39m0.8806   \u001b[39m | \u001b[39m0.4275   \u001b[39m | \u001b[39m2.619e+03\u001b[39m | \u001b[39m0.0004692\u001b[39m | \u001b[39m0.006642 \u001b[39m | \u001b[39m5.461    \u001b[39m | \u001b[39m2.971    \u001b[39m | \u001b[39m177.3    \u001b[39m | \u001b[39m11.26    \u001b[39m | \u001b[39m0.825    \u001b[39m | \u001b[39m0.882    \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=3, hid_dim=2231, act='leakyrelu', dropout=np.float64(0.4059523470627015), use_bn=True, l2=np.float64(0.0006653218340142246), w_initial='xavier', optim='ADAM', lr=np.float64(0.007986634686133648), epoch=10, train_batch_size=53, test_batch_size=198, exp_name='exp5')\n",
            "Epoch 0, Acc(train/val): 24.67/29.97, Loss(train/val) 2.68/1.90. Took 8.33 sec\n",
            "Epoch 1, Acc(train/val): 31.31/29.78, Loss(train/val) 1.89/1.96. Took 7.41 sec\n",
            "Epoch 2, Acc(train/val): 31.34/29.68, Loss(train/val) 1.88/1.92. Took 7.71 sec\n",
            "Epoch 3, Acc(train/val): 30.45/29.44, Loss(train/val) 1.90/1.91. Took 7.60 sec\n",
            "Epoch 4, Acc(train/val): 30.07/29.84, Loss(train/val) 1.91/1.91. Took 7.78 sec\n",
            "Epoch 5, Acc(train/val): 29.55/30.19, Loss(train/val) 1.93/1.92. Took 8.50 sec\n",
            "Epoch 6, Acc(train/val): 29.16/28.72, Loss(train/val) 1.93/1.95. Took 7.76 sec\n",
            "Epoch 7, Acc(train/val): 29.10/30.30, Loss(train/val) 1.94/1.93. Took 7.57 sec\n",
            "Epoch 8, Acc(train/val): 28.50/30.65, Loss(train/val) 1.95/1.89. Took 6.79 sec\n",
            "Epoch 9, Acc(train/val): 28.69/28.81, Loss(train/val) 1.94/1.98. Took 7.65 sec\n",
            "| \u001b[39m7        \u001b[39m | \u001b[39m-1.977   \u001b[39m | \u001b[39m0.3243   \u001b[39m | \u001b[39m0.406    \u001b[39m | \u001b[39m2.231e+03\u001b[39m | \u001b[39m0.0006653\u001b[39m | \u001b[39m0.007987 \u001b[39m | \u001b[39m3.219    \u001b[39m | \u001b[39m2.442    \u001b[39m | \u001b[39m198.2    \u001b[39m | \u001b[39m53.07    \u001b[39m | \u001b[39m0.3192   \u001b[39m | \u001b[39m0.8246   \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=3, hid_dim=2187, act='sigmoid', dropout=np.float64(0.1074744514711875), use_bn=False, l2=np.float64(0.0009266998352946566), w_initial='xavier', optim='RMSprop', lr=np.float64(0.009045050541565333), epoch=10, train_batch_size=35, test_batch_size=161, exp_name='exp6')\n",
            "Epoch 0, Acc(train/val): 10.12/10.11, Loss(train/val) 13.42/17.21. Took 8.46 sec\n",
            "Epoch 1, Acc(train/val): 9.95/10.41, Loss(train/val) 11.40/9.04. Took 8.82 sec\n",
            "Epoch 2, Acc(train/val): 9.84/10.28, Loss(train/val) 11.15/12.04. Took 10.02 sec\n",
            "Epoch 3, Acc(train/val): 10.19/10.41, Loss(train/val) 11.26/10.96. Took 8.03 sec\n",
            "Epoch 4, Acc(train/val): 10.27/10.08, Loss(train/val) 10.76/9.34. Took 8.91 sec\n",
            "Epoch 5, Acc(train/val): 9.99/10.11, Loss(train/val) 11.13/9.97. Took 8.61 sec\n",
            "Epoch 6, Acc(train/val): 10.21/10.41, Loss(train/val) 11.23/11.96. Took 9.07 sec\n",
            "Epoch 7, Acc(train/val): 10.21/9.88, Loss(train/val) 11.26/8.95. Took 9.28 sec\n",
            "Epoch 8, Acc(train/val): 10.19/10.11, Loss(train/val) 11.04/11.56. Took 9.24 sec\n",
            "Epoch 9, Acc(train/val): 10.05/10.34, Loss(train/val) 11.07/7.84. Took 9.05 sec\n",
            "| \u001b[39m8        \u001b[39m | \u001b[39m-7.838   \u001b[39m | \u001b[39m0.8906   \u001b[39m | \u001b[39m0.1075   \u001b[39m | \u001b[39m2.187e+03\u001b[39m | \u001b[39m0.0009267\u001b[39m | \u001b[39m0.009045 \u001b[39m | \u001b[39m3.1      \u001b[39m | \u001b[39m1.819    \u001b[39m | \u001b[39m161.3    \u001b[39m | \u001b[39m35.17    \u001b[39m | \u001b[39m0.9117   \u001b[39m | \u001b[39m0.9605   \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=5, hid_dim=2097, act='tanh', dropout=np.float64(0.061729653215423286), use_bn=False, l2=np.float64(0.0007379442698885152), w_initial='xavier', optim='RMSprop', lr=np.float64(0.002905723580288868), epoch=10, train_batch_size=28, test_batch_size=69, exp_name='exp7')\n",
            "Epoch 0, Acc(train/val): 13.85/13.90, Loss(train/val) 5.21/8.45. Took 12.00 sec\n",
            "Epoch 1, Acc(train/val): 14.05/7.77, Loss(train/val) 4.65/12.33. Took 12.04 sec\n",
            "Epoch 2, Acc(train/val): 13.93/9.95, Loss(train/val) 4.65/6.00. Took 11.34 sec\n",
            "Epoch 3, Acc(train/val): 14.14/10.55, Loss(train/val) 4.69/6.06. Took 12.73 sec\n",
            "Epoch 4, Acc(train/val): 14.00/13.03, Loss(train/val) 4.65/5.73. Took 11.55 sec\n",
            "Epoch 5, Acc(train/val): 13.52/13.59, Loss(train/val) 4.73/4.33. Took 11.55 sec\n",
            "Epoch 6, Acc(train/val): 13.68/10.04, Loss(train/val) 4.69/3.78. Took 11.30 sec\n",
            "Epoch 7, Acc(train/val): 13.65/8.98, Loss(train/val) 4.67/8.73. Took 11.44 sec\n",
            "Epoch 8, Acc(train/val): 13.76/15.82, Loss(train/val) 4.67/6.27. Took 11.90 sec\n",
            "Epoch 9, Acc(train/val): 13.57/12.94, Loss(train/val) 4.73/4.42. Took 11.57 sec\n",
            "| \u001b[39m9        \u001b[39m | \u001b[39m-4.418   \u001b[39m | \u001b[39m0.5884   \u001b[39m | \u001b[39m0.06173  \u001b[39m | \u001b[39m2.097e+03\u001b[39m | \u001b[39m0.0007379\u001b[39m | \u001b[39m0.002906 \u001b[39m | \u001b[39m5.034    \u001b[39m | \u001b[39m1.364    \u001b[39m | \u001b[39m69.15    \u001b[39m | \u001b[39m28.68    \u001b[39m | \u001b[39m0.6534   \u001b[39m | \u001b[39m0.5999   \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=2, hid_dim=406, act='sigmoid', dropout=np.float64(0.32643471138702435), use_bn=False, l2=np.float64(0.000979621032402956), w_initial='xavier', optim='ADAM', lr=np.float64(0.0005175886752309847), epoch=10, train_batch_size=39, test_batch_size=43, exp_name='exp8')\n",
            "Epoch 0, Acc(train/val): 33.00/35.65, Loss(train/val) 1.86/1.78. Took 8.84 sec\n",
            "Epoch 1, Acc(train/val): 37.67/39.30, Loss(train/val) 1.74/1.72. Took 10.07 sec\n",
            "Epoch 2, Acc(train/val): 39.44/39.68, Loss(train/val) 1.70/1.69. Took 8.67 sec\n",
            "Epoch 3, Acc(train/val): 40.23/40.48, Loss(train/val) 1.68/1.67. Took 9.27 sec\n",
            "Epoch 4, Acc(train/val): 40.87/41.16, Loss(train/val) 1.65/1.66. Took 8.88 sec\n",
            "Epoch 5, Acc(train/val): 41.31/41.92, Loss(train/val) 1.64/1.64. Took 9.88 sec\n",
            "Epoch 6, Acc(train/val): 42.35/41.69, Loss(train/val) 1.62/1.64. Took 9.48 sec\n",
            "Epoch 7, Acc(train/val): 42.80/41.44, Loss(train/val) 1.61/1.64. Took 9.99 sec\n",
            "Epoch 8, Acc(train/val): 42.77/42.51, Loss(train/val) 1.60/1.61. Took 9.76 sec\n",
            "Epoch 9, Acc(train/val): 43.12/42.89, Loss(train/val) 1.60/1.60. Took 9.72 sec\n",
            "| \u001b[35m10       \u001b[39m | \u001b[35m-1.601   \u001b[39m | \u001b[35m0.7621   \u001b[39m | \u001b[35m0.3264   \u001b[39m | \u001b[35m407.0    \u001b[39m | \u001b[35m0.0009796\u001b[39m | \u001b[35m0.0005176\u001b[39m | \u001b[35m2.502    \u001b[39m | \u001b[35m2.723    \u001b[39m | \u001b[35m43.8     \u001b[39m | \u001b[35m39.96    \u001b[39m | \u001b[35m0.9472   \u001b[39m | \u001b[35m0.9274   \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=1, hid_dim=559, act='leakyrelu', dropout=np.float64(0.3678395360159804), use_bn=True, l2=np.float64(8.019000542876398e-06), w_initial='he', optim='RMSprop', lr=np.float64(0.00979408881754932), epoch=10, train_batch_size=217, test_batch_size=215, exp_name='exp9')\n",
            "Epoch 0, Acc(train/val): 28.97/25.83, Loss(train/val) 33.85/8.43. Took 6.41 sec\n",
            "Epoch 1, Acc(train/val): 33.67/16.38, Loss(train/val) 4.61/33.59. Took 6.54 sec\n",
            "Epoch 2, Acc(train/val): 32.84/29.67, Loss(train/val) 8.13/4.44. Took 6.32 sec\n",
            "Epoch 3, Acc(train/val): 33.69/24.10, Loss(train/val) 7.68/5.95. Took 7.04 sec\n",
            "Epoch 4, Acc(train/val): 34.07/33.65, Loss(train/val) 7.69/3.08. Took 6.45 sec\n",
            "Epoch 5, Acc(train/val): 34.75/32.59, Loss(train/val) 9.75/4.47. Took 6.85 sec\n",
            "Epoch 6, Acc(train/val): 35.58/29.04, Loss(train/val) 8.78/7.58. Took 6.17 sec\n",
            "Epoch 7, Acc(train/val): 38.12/23.71, Loss(train/val) 5.72/24.04. Took 6.44 sec\n",
            "Epoch 8, Acc(train/val): 39.55/37.04, Loss(train/val) 4.15/2.59. Took 6.47 sec\n",
            "Epoch 9, Acc(train/val): 37.22/39.83, Loss(train/val) 9.84/2.82. Took 6.44 sec\n",
            "| \u001b[39m11       \u001b[39m | \u001b[39m-2.816   \u001b[39m | \u001b[39m0.2759   \u001b[39m | \u001b[39m0.3678   \u001b[39m | \u001b[39m559.5    \u001b[39m | \u001b[39m8.019e-06\u001b[39m | \u001b[39m0.009794 \u001b[39m | \u001b[39m1.618    \u001b[39m | \u001b[39m1.788    \u001b[39m | \u001b[39m215.7    \u001b[39m | \u001b[39m217.2    \u001b[39m | \u001b[39m0.03578  \u001b[39m | \u001b[39m0.4388   \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=4, hid_dim=249, act='sigmoid', dropout=np.float64(0.23833306290319062), use_bn=True, l2=np.float64(0.00019737377517000648), w_initial='he', optim='ADAM', lr=np.float64(0.007536152799090162), epoch=10, train_batch_size=163, test_batch_size=128, exp_name='exp10')\n",
            "Epoch 0, Acc(train/val): 31.40/35.43, Loss(train/val) 1.91/1.81. Took 6.84 sec\n",
            "Epoch 1, Acc(train/val): 37.37/38.72, Loss(train/val) 1.75/1.71. Took 6.55 sec\n",
            "Epoch 2, Acc(train/val): 38.35/36.82, Loss(train/val) 1.72/1.76. Took 6.97 sec\n",
            "Epoch 3, Acc(train/val): 38.67/37.72, Loss(train/val) 1.72/1.74. Took 6.64 sec\n",
            "Epoch 4, Acc(train/val): 38.76/38.88, Loss(train/val) 1.72/1.71. Took 5.87 sec\n",
            "Epoch 5, Acc(train/val): 38.95/36.94, Loss(train/val) 1.71/1.74. Took 6.74 sec\n",
            "Epoch 6, Acc(train/val): 38.57/38.04, Loss(train/val) 1.72/1.72. Took 6.37 sec\n",
            "Epoch 7, Acc(train/val): 38.02/38.42, Loss(train/val) 1.73/1.74. Took 6.97 sec\n",
            "Epoch 8, Acc(train/val): 38.09/36.89, Loss(train/val) 1.73/1.76. Took 7.08 sec\n",
            "Epoch 9, Acc(train/val): 38.07/38.25, Loss(train/val) 1.73/1.73. Took 6.81 sec\n",
            "| \u001b[39m12       \u001b[39m | \u001b[39m-1.726   \u001b[39m | \u001b[39m0.9465   \u001b[39m | \u001b[39m0.2383   \u001b[39m | \u001b[39m249.6    \u001b[39m | \u001b[39m0.0001974\u001b[39m | \u001b[39m0.007536 \u001b[39m | \u001b[39m4.122    \u001b[39m | \u001b[39m2.789    \u001b[39m | \u001b[39m128.5    \u001b[39m | \u001b[39m163.8    \u001b[39m | \u001b[39m0.3094   \u001b[39m | \u001b[39m0.01092  \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=4, hid_dim=1790, act='tanh', dropout=np.float64(0.3589092880992576), use_bn=False, l2=np.float64(0.00029173109666795965), w_initial='xavier', optim='SGD', lr=np.float64(0.006204277141663351), epoch=10, train_batch_size=113, test_batch_size=51, exp_name='exp11')\n",
            "Epoch 0, Acc(train/val): 30.27/35.97, Loss(train/val) 1.97/1.85. Took 7.24 sec\n",
            "Epoch 1, Acc(train/val): 36.39/37.99, Loss(train/val) 1.82/1.80. Took 7.02 sec\n",
            "Epoch 2, Acc(train/val): 38.09/38.77, Loss(train/val) 1.78/1.77. Took 7.05 sec\n",
            "Epoch 3, Acc(train/val): 39.19/39.34, Loss(train/val) 1.75/1.75. Took 6.94 sec\n",
            "Epoch 4, Acc(train/val): 39.88/40.00, Loss(train/val) 1.73/1.74. Took 6.48 sec\n",
            "Epoch 5, Acc(train/val): 40.41/40.33, Loss(train/val) 1.72/1.73. Took 7.19 sec\n",
            "Epoch 6, Acc(train/val): 40.88/40.61, Loss(train/val) 1.70/1.73. Took 6.96 sec\n",
            "Epoch 7, Acc(train/val): 41.37/40.66, Loss(train/val) 1.69/1.72. Took 7.04 sec\n",
            "Epoch 8, Acc(train/val): 41.49/40.89, Loss(train/val) 1.69/1.72. Took 6.64 sec\n",
            "Epoch 9, Acc(train/val): 41.92/41.24, Loss(train/val) 1.67/1.71. Took 7.43 sec\n",
            "| \u001b[39m13       \u001b[39m | \u001b[39m-1.71    \u001b[39m | \u001b[39m0.6701   \u001b[39m | \u001b[39m0.3589   \u001b[39m | \u001b[39m1.791e+03\u001b[39m | \u001b[39m0.0002917\u001b[39m | \u001b[39m0.006204 \u001b[39m | \u001b[39m4.945    \u001b[39m | \u001b[39m0.06357  \u001b[39m | \u001b[39m51.36    \u001b[39m | \u001b[39m113.8    \u001b[39m | \u001b[39m0.8818   \u001b[39m | \u001b[39m0.8754   \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=3, hid_dim=1468, act='tanh', dropout=np.float64(0.16493625567850817), use_bn=True, l2=np.float64(0.0001038738688904137), w_initial='xavier', optim='RMSprop', lr=np.float64(0.000988378135800286), epoch=10, train_batch_size=178, test_batch_size=198, exp_name='exp12')\n",
            "Epoch 0, Acc(train/val): 25.07/30.49, Loss(train/val) 2.30/1.97. Took 7.29 sec\n",
            "Epoch 1, Acc(train/val): 34.80/32.83, Loss(train/val) 1.85/1.96. Took 6.55 sec\n",
            "Epoch 2, Acc(train/val): 39.67/39.75, Loss(train/val) 1.71/1.73. Took 6.57 sec\n",
            "Epoch 3, Acc(train/val): 42.68/40.31, Loss(train/val) 1.63/1.70. Took 5.83 sec\n",
            "Epoch 4, Acc(train/val): 44.87/41.79, Loss(train/val) 1.55/1.66. Took 6.38 sec\n",
            "Epoch 5, Acc(train/val): 47.19/44.97, Loss(train/val) 1.49/1.58. Took 6.54 sec\n",
            "Epoch 6, Acc(train/val): 49.17/43.92, Loss(train/val) 1.44/1.61. Took 6.29 sec\n",
            "Epoch 7, Acc(train/val): 50.60/46.90, Loss(train/val) 1.39/1.54. Took 7.14 sec\n",
            "Epoch 8, Acc(train/val): 52.14/45.19, Loss(train/val) 1.35/1.59. Took 6.55 sec\n",
            "Epoch 9, Acc(train/val): 53.61/46.38, Loss(train/val) 1.31/1.54. Took 6.47 sec\n",
            "| \u001b[35m14       \u001b[39m | \u001b[35m-1.542   \u001b[39m | \u001b[35m0.6868   \u001b[39m | \u001b[35m0.1649   \u001b[39m | \u001b[35m1.468e+03\u001b[39m | \u001b[35m0.0001039\u001b[39m | \u001b[35m0.0009884\u001b[39m | \u001b[35m3.033    \u001b[39m | \u001b[35m1.165    \u001b[39m | \u001b[35m198.7    \u001b[39m | \u001b[35m178.4    \u001b[39m | \u001b[35m0.2362   \u001b[39m | \u001b[35m0.7918   \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=2, hid_dim=2076, act='leakyrelu', dropout=np.float64(0.42360051952988265), use_bn=False, l2=np.float64(0.0008505843750767311), w_initial='xavier', optim='SGD', lr=np.float64(0.009873040361421104), epoch=10, train_batch_size=123, test_batch_size=125, exp_name='exp13')\n",
            "Epoch 0, Acc(train/val): 28.86/34.38, Loss(train/val) 2.04/1.89. Took 7.27 sec\n",
            "Epoch 1, Acc(train/val): 36.67/38.35, Loss(train/val) 1.81/1.77. Took 6.63 sec\n",
            "Epoch 2, Acc(train/val): 40.24/40.95, Loss(train/val) 1.71/1.69. Took 6.45 sec\n",
            "Epoch 3, Acc(train/val): 42.63/42.31, Loss(train/val) 1.64/1.64. Took 7.57 sec\n",
            "Epoch 4, Acc(train/val): 44.29/42.95, Loss(train/val) 1.59/1.62. Took 6.39 sec\n",
            "Epoch 5, Acc(train/val): 45.85/44.41, Loss(train/val) 1.55/1.58. Took 7.19 sec\n",
            "Epoch 6, Acc(train/val): 47.11/45.37, Loss(train/val) 1.52/1.56. Took 6.62 sec\n",
            "Epoch 7, Acc(train/val): 48.28/46.00, Loss(train/val) 1.49/1.53. Took 6.50 sec\n",
            "Epoch 8, Acc(train/val): 49.31/46.97, Loss(train/val) 1.46/1.52. Took 6.75 sec\n",
            "Epoch 9, Acc(train/val): 50.26/47.21, Loss(train/val) 1.43/1.51. Took 6.72 sec\n",
            "| \u001b[35m15       \u001b[39m | \u001b[35m-1.515   \u001b[39m | \u001b[35m0.3535   \u001b[39m | \u001b[35m0.4236   \u001b[39m | \u001b[35m2.076e+03\u001b[39m | \u001b[35m0.0008506\u001b[39m | \u001b[35m0.009873 \u001b[39m | \u001b[35m2.004    \u001b[39m | \u001b[35m0.7011   \u001b[39m | \u001b[35m125.2    \u001b[39m | \u001b[35m123.9    \u001b[39m | \u001b[35m0.6159   \u001b[39m | \u001b[35m0.7713   \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=1, hid_dim=1229, act='tanh', dropout=np.float64(6.702164047522885e-05), use_bn=True, l2=np.float64(0.0005124511164380324), w_initial='he', optim='RMSprop', lr=np.float64(0.009412528945053612), epoch=10, train_batch_size=234, test_batch_size=130, exp_name='exp14')\n",
            "Epoch 0, Acc(train/val): 21.94/24.26, Loss(train/val) 8.93/5.10. Took 6.19 sec\n",
            "Epoch 1, Acc(train/val): 23.36/23.18, Loss(train/val) 4.66/4.20. Took 6.28 sec\n",
            "Epoch 2, Acc(train/val): 22.67/24.28, Loss(train/val) 4.24/4.15. Took 6.82 sec\n",
            "Epoch 3, Acc(train/val): 21.77/16.05, Loss(train/val) 4.26/5.25. Took 6.57 sec\n",
            "Epoch 4, Acc(train/val): 21.71/22.09, Loss(train/val) 4.32/4.38. Took 6.95 sec\n",
            "Epoch 5, Acc(train/val): 21.61/18.72, Loss(train/val) 4.30/6.31. Took 6.23 sec\n",
            "Epoch 6, Acc(train/val): 21.87/21.32, Loss(train/val) 4.30/4.88. Took 6.16 sec\n",
            "Epoch 7, Acc(train/val): 22.18/23.22, Loss(train/val) 4.28/3.98. Took 6.69 sec\n",
            "Epoch 8, Acc(train/val): 22.02/21.73, Loss(train/val) 4.30/4.03. Took 6.48 sec\n",
            "Epoch 9, Acc(train/val): 22.30/23.02, Loss(train/val) 4.29/3.71. Took 6.57 sec\n",
            "| \u001b[39m16       \u001b[39m | \u001b[39m-3.705   \u001b[39m | \u001b[39m0.6239   \u001b[39m | \u001b[39m6.702e-05\u001b[39m | \u001b[39m1.229e+03\u001b[39m | \u001b[39m0.0005125\u001b[39m | \u001b[39m0.009413 \u001b[39m | \u001b[39m1.611    \u001b[39m | \u001b[39m1.315    \u001b[39m | \u001b[39m130.7    \u001b[39m | \u001b[39m234.6    \u001b[39m | \u001b[39m0.2394   \u001b[39m | \u001b[39m0.3721   \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=4, hid_dim=1007, act='leakyrelu', dropout=np.float64(0.49631604372013566), use_bn=False, l2=np.float64(0.0001769215172947871), w_initial='he', optim='RMSprop', lr=np.float64(0.008947608790417212), epoch=10, train_batch_size=29, test_batch_size=158, exp_name='exp15')\n",
            "Epoch 0, Acc(train/val): 11.73/9.65, Loss(train/val) 3607.23/270.34. Took 10.11 sec\n",
            "Epoch 1, Acc(train/val): 11.12/10.30, Loss(train/val) 448.50/39.44. Took 9.77 sec\n",
            "Epoch 2, Acc(train/val): 10.72/10.11, Loss(train/val) 415.31/523.23. Took 9.57 sec\n",
            "Epoch 3, Acc(train/val): 10.49/9.61, Loss(train/val) 565.04/339.67. Took 8.50 sec\n",
            "Epoch 4, Acc(train/val): 10.62/9.12, Loss(train/val) 506.74/246.21. Took 9.64 sec\n",
            "Epoch 5, Acc(train/val): 10.51/9.90, Loss(train/val) 585.38/214.57. Took 9.56 sec\n",
            "Epoch 6, Acc(train/val): 10.59/11.31, Loss(train/val) 823.49/275.79. Took 9.32 sec\n",
            "Epoch 7, Acc(train/val): 10.69/11.01, Loss(train/val) 694.54/773.17. Took 9.56 sec\n",
            "Epoch 8, Acc(train/val): 10.57/10.03, Loss(train/val) 884.04/310.89. Took 9.65 sec\n",
            "Epoch 9, Acc(train/val): 10.46/15.97, Loss(train/val) 937.56/216.38. Took 8.94 sec\n",
            "| \u001b[39m17       \u001b[39m | \u001b[39m-216.4   \u001b[39m | \u001b[39m0.3223   \u001b[39m | \u001b[39m0.4963   \u001b[39m | \u001b[39m1.007e+03\u001b[39m | \u001b[39m0.0001769\u001b[39m | \u001b[39m0.008948 \u001b[39m | \u001b[39m4.747    \u001b[39m | \u001b[39m1.433    \u001b[39m | \u001b[39m158.2    \u001b[39m | \u001b[39m29.98    \u001b[39m | \u001b[39m0.7248   \u001b[39m | \u001b[39m0.4546   \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=4, hid_dim=2195, act='tanh', dropout=np.float64(0.08090931135872848), use_bn=False, l2=np.float64(0.0008799917735722677), w_initial='he', optim='SGD', lr=np.float64(0.0006073940058848975), epoch=10, train_batch_size=206, test_batch_size=149, exp_name='exp16')\n",
            "Epoch 0, Acc(train/val): 22.74/28.88, Loss(train/val) 2.14/2.02. Took 6.45 sec\n",
            "Epoch 1, Acc(train/val): 30.07/32.45, Loss(train/val) 1.99/1.95. Took 6.36 sec\n",
            "Epoch 2, Acc(train/val): 32.59/34.02, Loss(train/val) 1.93/1.91. Took 6.48 sec\n",
            "Epoch 3, Acc(train/val): 34.05/35.18, Loss(train/val) 1.90/1.88. Took 6.74 sec\n",
            "Epoch 4, Acc(train/val): 35.21/36.15, Loss(train/val) 1.87/1.86. Took 6.71 sec\n",
            "Epoch 5, Acc(train/val): 36.10/36.72, Loss(train/val) 1.85/1.85. Took 6.71 sec\n",
            "Epoch 6, Acc(train/val): 36.76/36.84, Loss(train/val) 1.84/1.83. Took 6.57 sec\n",
            "Epoch 7, Acc(train/val): 37.11/37.28, Loss(train/val) 1.82/1.82. Took 6.25 sec\n",
            "Epoch 8, Acc(train/val): 37.51/37.45, Loss(train/val) 1.81/1.82. Took 6.70 sec\n",
            "Epoch 9, Acc(train/val): 38.03/37.85, Loss(train/val) 1.80/1.81. Took 6.66 sec\n",
            "| \u001b[39m18       \u001b[39m | \u001b[39m-1.808   \u001b[39m | \u001b[39m0.5502   \u001b[39m | \u001b[39m0.08091  \u001b[39m | \u001b[39m2.196e+03\u001b[39m | \u001b[39m0.00088  \u001b[39m | \u001b[39m0.0006074\u001b[39m | \u001b[39m4.928    \u001b[39m | \u001b[39m0.83     \u001b[39m | \u001b[39m149.1    \u001b[39m | \u001b[39m206.1    \u001b[39m | \u001b[39m0.9669   \u001b[39m | \u001b[39m0.1233   \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=1, hid_dim=2779, act='sigmoid', dropout=np.float64(0.3422615498888152), use_bn=False, l2=np.float64(0.00014595856543039165), w_initial='he', optim='RMSprop', lr=np.float64(0.0019050284625736482), epoch=10, train_batch_size=177, test_batch_size=109, exp_name='exp17')\n",
            "Epoch 0, Acc(train/val): 30.96/33.59, Loss(train/val) 2.85/2.15. Took 6.68 sec\n",
            "Epoch 1, Acc(train/val): 35.39/30.62, Loss(train/val) 2.05/2.26. Took 6.46 sec\n",
            "Epoch 2, Acc(train/val): 37.68/37.69, Loss(train/val) 1.98/1.94. Took 6.87 sec\n",
            "Epoch 3, Acc(train/val): 38.67/41.78, Loss(train/val) 1.96/1.74. Took 6.65 sec\n",
            "Epoch 4, Acc(train/val): 40.08/38.49, Loss(train/val) 1.90/1.90. Took 6.87 sec\n",
            "Epoch 5, Acc(train/val): 41.13/40.43, Loss(train/val) 1.87/1.76. Took 6.66 sec\n",
            "Epoch 6, Acc(train/val): 42.36/36.77, Loss(train/val) 1.84/2.57. Took 7.28 sec\n",
            "Epoch 7, Acc(train/val): 43.49/43.93, Loss(train/val) 1.83/1.61. Took 7.50 sec\n",
            "Epoch 8, Acc(train/val): 44.09/41.86, Loss(train/val) 1.78/1.79. Took 6.77 sec\n",
            "Epoch 9, Acc(train/val): 44.98/41.10, Loss(train/val) 1.76/1.81. Took 6.62 sec\n",
            "| \u001b[39m19       \u001b[39m | \u001b[39m-1.814   \u001b[39m | \u001b[39m0.8896   \u001b[39m | \u001b[39m0.3423   \u001b[39m | \u001b[39m2.78e+03 \u001b[39m | \u001b[39m0.000146 \u001b[39m | \u001b[39m0.001905 \u001b[39m | \u001b[39m1.83     \u001b[39m | \u001b[39m1.096    \u001b[39m | \u001b[39m109.1    \u001b[39m | \u001b[39m177.2    \u001b[39m | \u001b[39m0.682    \u001b[39m | \u001b[39m0.3578   \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=2, hid_dim=2482, act='leakyrelu', dropout=np.float64(0.4295598957383391), use_bn=True, l2=np.float64(0.00027064234493235967), w_initial='xavier', optim='RMSprop', lr=np.float64(0.0007941686889020745), epoch=10, train_batch_size=52, test_batch_size=235, exp_name='exp18')\n",
            "Epoch 0, Acc(train/val): 34.12/33.12, Loss(train/val) 2.10/2.16. Took 7.46 sec\n",
            "Epoch 1, Acc(train/val): 41.03/34.78, Loss(train/val) 1.75/2.24. Took 6.70 sec\n",
            "Epoch 2, Acc(train/val): 44.09/37.87, Loss(train/val) 1.63/1.89. Took 6.51 sec\n",
            "Epoch 3, Acc(train/val): 46.42/38.76, Loss(train/val) 1.55/1.94. Took 7.34 sec\n",
            "Epoch 4, Acc(train/val): 48.11/44.80, Loss(train/val) 1.49/1.57. Took 6.98 sec\n",
            "Epoch 5, Acc(train/val): 49.70/44.96, Loss(train/val) 1.43/1.62. Took 7.31 sec\n",
            "Epoch 6, Acc(train/val): 50.68/43.66, Loss(train/val) 1.40/1.75. Took 6.40 sec\n",
            "Epoch 7, Acc(train/val): 51.76/40.82, Loss(train/val) 1.37/1.75. Took 7.46 sec\n",
            "Epoch 8, Acc(train/val): 52.85/43.22, Loss(train/val) 1.33/1.74. Took 7.02 sec\n",
            "Epoch 9, Acc(train/val): 53.50/44.63, Loss(train/val) 1.31/1.60. Took 7.41 sec\n",
            "| \u001b[39m20       \u001b[39m | \u001b[39m-1.598   \u001b[39m | \u001b[39m0.3306   \u001b[39m | \u001b[39m0.4296   \u001b[39m | \u001b[39m2.483e+03\u001b[39m | \u001b[39m0.0002706\u001b[39m | \u001b[39m0.0007942\u001b[39m | \u001b[39m2.952    \u001b[39m | \u001b[39m1.842    \u001b[39m | \u001b[39m235.3    \u001b[39m | \u001b[39m52.74    \u001b[39m | \u001b[39m0.4508   \u001b[39m | \u001b[39m0.6579   \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=1, hid_dim=1083, act='relu', dropout=np.float64(0.04186935068071185), use_bn=False, l2=np.float64(5.0523262386714295e-05), w_initial='xavier', optim='ADAM', lr=np.float64(0.001593371973760551), epoch=10, train_batch_size=230, test_batch_size=89, exp_name='exp19')\n",
            "Epoch 0, Acc(train/val): 40.60/44.52, Loss(train/val) 1.72/1.61. Took 5.86 sec\n",
            "Epoch 1, Acc(train/val): 48.31/48.03, Loss(train/val) 1.49/1.51. Took 6.46 sec\n",
            "Epoch 2, Acc(train/val): 51.23/48.91, Loss(train/val) 1.40/1.52. Took 6.91 sec\n",
            "Epoch 3, Acc(train/val): 54.38/48.07, Loss(train/val) 1.34/1.56. Took 6.64 sec\n",
            "Epoch 4, Acc(train/val): 56.41/48.74, Loss(train/val) 1.27/1.57. Took 6.17 sec\n",
            "Epoch 5, Acc(train/val): 58.39/50.29, Loss(train/val) 1.22/1.56. Took 6.73 sec\n",
            "Epoch 6, Acc(train/val): 60.88/49.74, Loss(train/val) 1.15/1.56. Took 6.45 sec\n",
            "Epoch 7, Acc(train/val): 62.97/50.84, Loss(train/val) 1.08/1.53. Took 6.51 sec\n",
            "Epoch 8, Acc(train/val): 64.26/51.61, Loss(train/val) 1.05/1.62. Took 6.78 sec\n",
            "Epoch 9, Acc(train/val): 65.86/50.51, Loss(train/val) 1.01/1.65. Took 6.15 sec\n",
            "| \u001b[39m21       \u001b[39m | \u001b[39m-1.646   \u001b[39m | \u001b[39m0.1703   \u001b[39m | \u001b[39m0.04187  \u001b[39m | \u001b[39m1.084e+03\u001b[39m | \u001b[39m5.052e-05\u001b[39m | \u001b[39m0.001593 \u001b[39m | \u001b[39m1.811    \u001b[39m | \u001b[39m2.619    \u001b[39m | \u001b[39m89.22    \u001b[39m | \u001b[39m230.4    \u001b[39m | \u001b[39m0.5949   \u001b[39m | \u001b[39m0.8654   \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=4, hid_dim=847, act='relu', dropout=np.float64(0.29986879238154923), use_bn=True, l2=np.float64(0.000215315905528923), w_initial='he', optim='ADAM', lr=np.float64(0.0066325060153535195), epoch=10, train_batch_size=45, test_batch_size=137, exp_name='exp20')\n",
            "Epoch 0, Acc(train/val): 28.44/33.36, Loss(train/val) 2.11/1.84. Took 7.73 sec\n",
            "Epoch 1, Acc(train/val): 34.90/34.70, Loss(train/val) 1.80/1.81. Took 7.64 sec\n",
            "Epoch 2, Acc(train/val): 35.41/35.75, Loss(train/val) 1.79/1.80. Took 7.70 sec\n",
            "Epoch 3, Acc(train/val): 35.06/35.13, Loss(train/val) 1.79/2.15. Took 7.62 sec\n",
            "Epoch 4, Acc(train/val): 35.69/34.70, Loss(train/val) 1.78/1.82. Took 7.64 sec\n",
            "Epoch 5, Acc(train/val): 35.04/34.33, Loss(train/val) 1.78/4.51. Took 7.33 sec\n",
            "Epoch 6, Acc(train/val): 36.01/36.55, Loss(train/val) 1.77/1.78. Took 7.04 sec\n",
            "Epoch 7, Acc(train/val): 31.14/29.30, Loss(train/val) 1.89/3.22. Took 7.95 sec\n",
            "Epoch 8, Acc(train/val): 33.80/36.15, Loss(train/val) 1.82/1.77. Took 7.93 sec\n",
            "Epoch 9, Acc(train/val): 35.62/37.05, Loss(train/val) 1.78/1.75. Took 7.97 sec\n",
            "| \u001b[39m22       \u001b[39m | \u001b[39m-1.75    \u001b[39m | \u001b[39m0.05238  \u001b[39m | \u001b[39m0.2999   \u001b[39m | \u001b[39m847.3    \u001b[39m | \u001b[39m0.0002153\u001b[39m | \u001b[39m0.006633 \u001b[39m | \u001b[39m4.293    \u001b[39m | \u001b[39m2.383    \u001b[39m | \u001b[39m137.4    \u001b[39m | \u001b[39m45.05    \u001b[39m | \u001b[39m0.3815   \u001b[39m | \u001b[39m0.03644  \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=5, hid_dim=813, act='leakyrelu', dropout=np.float64(0.15845072591936382), use_bn=False, l2=np.float64(0.0005747992814946335), w_initial='xavier', optim='SGD', lr=np.float64(0.006165998919763965), epoch=10, train_batch_size=188, test_batch_size=125, exp_name='exp21')\n",
            "Epoch 0, Acc(train/val): 14.79/22.07, Loss(train/val) 2.29/2.28. Took 8.12 sec\n",
            "Epoch 1, Acc(train/val): 21.24/23.60, Loss(train/val) 2.26/2.22. Took 7.67 sec\n",
            "Epoch 2, Acc(train/val): 22.97/23.59, Loss(train/val) 2.19/2.12. Took 8.21 sec\n",
            "Epoch 3, Acc(train/val): 25.04/26.12, Loss(train/val) 2.09/2.03. Took 8.27 sec\n",
            "Epoch 4, Acc(train/val): 27.67/28.52, Loss(train/val) 2.00/1.96. Took 7.31 sec\n",
            "Epoch 5, Acc(train/val): 30.15/30.70, Loss(train/val) 1.94/1.91. Took 7.60 sec\n",
            "Epoch 6, Acc(train/val): 31.91/32.34, Loss(train/val) 1.89/1.86. Took 7.99 sec\n",
            "Epoch 7, Acc(train/val): 33.63/35.26, Loss(train/val) 1.84/1.81. Took 8.45 sec\n",
            "Epoch 8, Acc(train/val): 35.34/36.04, Loss(train/val) 1.80/1.77. Took 7.68 sec\n",
            "Epoch 9, Acc(train/val): 36.77/37.69, Loss(train/val) 1.76/1.74. Took 7.74 sec\n",
            "| \u001b[39m23       \u001b[39m | \u001b[39m-1.744   \u001b[39m | \u001b[39m0.2805   \u001b[39m | \u001b[39m0.1585   \u001b[39m | \u001b[39m813.7    \u001b[39m | \u001b[39m0.0005748\u001b[39m | \u001b[39m0.006166 \u001b[39m | \u001b[39m5.658    \u001b[39m | \u001b[39m0.803    \u001b[39m | \u001b[39m125.5    \u001b[39m | \u001b[39m189.0    \u001b[39m | \u001b[39m0.9612   \u001b[39m | \u001b[39m0.6422   \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=5, hid_dim=1323, act='relu', dropout=np.float64(0.2773193408325318), use_bn=False, l2=np.float64(0.0008317227073573044), w_initial='he', optim='RMSprop', lr=np.float64(0.0011279432922320465), epoch=10, train_batch_size=95, test_batch_size=87, exp_name='exp22')\n",
            "Epoch 0, Acc(train/val): 28.57/28.63, Loss(train/val) 7.35/2.08. Took 8.17 sec\n",
            "Epoch 1, Acc(train/val): 36.72/31.30, Loss(train/val) 1.78/2.13. Took 7.72 sec\n",
            "Epoch 2, Acc(train/val): 38.33/27.73, Loss(train/val) 1.74/2.63. Took 7.57 sec\n",
            "Epoch 3, Acc(train/val): 40.57/31.80, Loss(train/val) 1.68/2.29. Took 7.74 sec\n",
            "Epoch 4, Acc(train/val): 41.99/28.06, Loss(train/val) 1.65/2.29. Took 7.68 sec\n",
            "Epoch 5, Acc(train/val): 42.98/32.88, Loss(train/val) 1.62/1.90. Took 7.30 sec\n",
            "Epoch 6, Acc(train/val): 44.27/32.23, Loss(train/val) 1.58/2.18. Took 7.73 sec\n",
            "Epoch 7, Acc(train/val): 45.13/32.80, Loss(train/val) 1.56/2.76. Took 8.03 sec\n",
            "Epoch 8, Acc(train/val): 45.49/36.80, Loss(train/val) 1.56/1.93. Took 7.79 sec\n",
            "Epoch 9, Acc(train/val): 46.49/28.31, Loss(train/val) 1.54/5.17. Took 7.01 sec\n",
            "| \u001b[39m24       \u001b[39m | \u001b[39m-5.169   \u001b[39m | \u001b[39m0.1121   \u001b[39m | \u001b[39m0.2773   \u001b[39m | \u001b[39m1.323e+03\u001b[39m | \u001b[39m0.0008317\u001b[39m | \u001b[39m0.001128 \u001b[39m | \u001b[39m5.088    \u001b[39m | \u001b[39m1.164    \u001b[39m | \u001b[39m87.96    \u001b[39m | \u001b[39m95.11    \u001b[39m | \u001b[39m0.6985   \u001b[39m | \u001b[39m0.2005   \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=1, hid_dim=2471, act='tanh', dropout=np.float64(0.3969028498519192), use_bn=False, l2=np.float64(0.000842351039630607), w_initial='xavier', optim='SGD', lr=np.float64(0.0035767966583980294), epoch=10, train_batch_size=229, test_batch_size=198, exp_name='exp23')\n",
            "Epoch 0, Acc(train/val): 27.68/32.32, Loss(train/val) 2.07/1.97. Took 8.10 sec\n",
            "Epoch 1, Acc(train/val): 34.15/34.44, Loss(train/val) 1.93/1.91. Took 8.03 sec\n",
            "Epoch 2, Acc(train/val): 36.12/35.88, Loss(train/val) 1.88/1.87. Took 8.26 sec\n",
            "Epoch 3, Acc(train/val): 37.03/36.98, Loss(train/val) 1.85/1.85. Took 8.33 sec\n",
            "Epoch 4, Acc(train/val): 37.93/37.21, Loss(train/val) 1.83/1.84. Took 8.20 sec\n",
            "Epoch 5, Acc(train/val): 38.39/37.76, Loss(train/val) 1.81/1.83. Took 8.29 sec\n",
            "Epoch 6, Acc(train/val): 38.91/37.93, Loss(train/val) 1.80/1.82. Took 8.24 sec\n",
            "Epoch 7, Acc(train/val): 39.15/38.05, Loss(train/val) 1.79/1.81. Took 7.86 sec\n",
            "Epoch 8, Acc(train/val): 39.53/38.29, Loss(train/val) 1.78/1.80. Took 7.23 sec\n",
            "Epoch 9, Acc(train/val): 39.78/38.33, Loss(train/val) 1.77/1.80. Took 7.32 sec\n",
            "| \u001b[39m25       \u001b[39m | \u001b[39m-1.797   \u001b[39m | \u001b[39m0.5213   \u001b[39m | \u001b[39m0.3969   \u001b[39m | \u001b[39m2.471e+03\u001b[39m | \u001b[39m0.0008424\u001b[39m | \u001b[39m0.003577 \u001b[39m | \u001b[39m1.671    \u001b[39m | \u001b[39m0.4547   \u001b[39m | \u001b[39m198.3    \u001b[39m | \u001b[39m229.2    \u001b[39m | \u001b[39m0.6695   \u001b[39m | \u001b[39m0.9833   \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=2, hid_dim=1448, act='tanh', dropout=np.float64(0.09821081041825858), use_bn=True, l2=np.float64(0.0005951818156317674), w_initial='xavier', optim='SGD', lr=np.float64(0.007604712270193616), epoch=10, train_batch_size=255, test_batch_size=102, exp_name='exp24')\n",
            "Epoch 0, Acc(train/val): 34.90/37.16, Loss(train/val) 1.86/1.80. Took 8.30 sec\n",
            "Epoch 1, Acc(train/val): 39.27/38.20, Loss(train/val) 1.76/1.77. Took 8.25 sec\n",
            "Epoch 2, Acc(train/val): 40.70/39.55, Loss(train/val) 1.72/1.76. Took 8.35 sec\n",
            "Epoch 3, Acc(train/val): 41.66/40.74, Loss(train/val) 1.70/1.73. Took 8.44 sec\n",
            "Epoch 4, Acc(train/val): 42.16/40.70, Loss(train/val) 1.68/1.73. Took 8.27 sec\n",
            "Epoch 5, Acc(train/val): 42.65/40.04, Loss(train/val) 1.67/1.73. Took 8.21 sec\n",
            "Epoch 6, Acc(train/val): 43.30/41.46, Loss(train/val) 1.66/1.71. Took 8.20 sec\n",
            "Epoch 7, Acc(train/val): 43.95/41.36, Loss(train/val) 1.64/1.71. Took 8.18 sec\n",
            "Epoch 8, Acc(train/val): 43.98/41.81, Loss(train/val) 1.63/1.69. Took 8.34 sec\n",
            "Epoch 9, Acc(train/val): 44.49/41.89, Loss(train/val) 1.62/1.70. Took 8.28 sec\n",
            "| \u001b[39m26       \u001b[39m | \u001b[39m-1.696   \u001b[39m | \u001b[39m0.653    \u001b[39m | \u001b[39m0.09821  \u001b[39m | \u001b[39m1.448e+03\u001b[39m | \u001b[39m0.0005952\u001b[39m | \u001b[39m0.007605 \u001b[39m | \u001b[39m2.69     \u001b[39m | \u001b[39m0.5981   \u001b[39m | \u001b[39m102.8    \u001b[39m | \u001b[39m255.8    \u001b[39m | \u001b[39m0.3011   \u001b[39m | \u001b[39m0.8618   \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=3, hid_dim=2465, act='sigmoid', dropout=np.float64(0.4222537409967852), use_bn=True, l2=np.float64(0.00025196820611322695), w_initial='xavier', optim='RMSprop', lr=np.float64(0.0002085166837167916), epoch=10, train_batch_size=59, test_batch_size=101, exp_name='exp25')\n",
            "Epoch 0, Acc(train/val): 32.83/39.74, Loss(train/val) 1.95/1.70. Took 7.77 sec\n",
            "Epoch 1, Acc(train/val): 40.93/42.53, Loss(train/val) 1.67/1.62. Took 7.62 sec\n",
            "Epoch 2, Acc(train/val): 44.83/44.72, Loss(train/val) 1.56/1.54. Took 7.60 sec\n",
            "Epoch 3, Acc(train/val): 46.63/48.23, Loss(train/val) 1.50/1.47. Took 7.42 sec\n",
            "Epoch 4, Acc(train/val): 48.33/47.86, Loss(train/val) 1.45/1.48. Took 7.99 sec\n",
            "Epoch 5, Acc(train/val): 49.80/48.57, Loss(train/val) 1.41/1.45. Took 7.53 sec\n",
            "Epoch 6, Acc(train/val): 51.07/49.41, Loss(train/val) 1.37/1.43. Took 8.36 sec\n",
            "Epoch 7, Acc(train/val): 52.20/49.66, Loss(train/val) 1.34/1.42. Took 7.66 sec\n",
            "Epoch 8, Acc(train/val): 53.08/51.39, Loss(train/val) 1.31/1.39. Took 7.39 sec\n",
            "Epoch 9, Acc(train/val): 54.11/49.90, Loss(train/val) 1.28/1.41. Took 7.48 sec\n",
            "| \u001b[35m27       \u001b[39m | \u001b[35m-1.414   \u001b[39m | \u001b[35m0.8394   \u001b[39m | \u001b[35m0.4223   \u001b[39m | \u001b[35m2.465e+03\u001b[39m | \u001b[35m0.000252 \u001b[39m | \u001b[35m0.0002085\u001b[39m | \u001b[35m3.997    \u001b[39m | \u001b[35m1.444    \u001b[39m | \u001b[35m101.8    \u001b[39m | \u001b[35m59.19    \u001b[39m | \u001b[35m0.2273   \u001b[39m | \u001b[35m0.8819   \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=4, hid_dim=2895, act='relu', dropout=np.float64(0.16562319198583852), use_bn=True, l2=np.float64(0.0007038258897158337), w_initial='he', optim='ADAM', lr=np.float64(0.0034789255903140076), epoch=10, train_batch_size=125, test_batch_size=89, exp_name='exp26')\n",
            "Epoch 0, Acc(train/val): 27.94/32.01, Loss(train/val) 2.26/2.07. Took 7.19 sec\n",
            "Epoch 1, Acc(train/val): 33.34/25.16, Loss(train/val) 1.90/2.10. Took 7.17 sec\n",
            "Epoch 2, Acc(train/val): 35.36/36.65, Loss(train/val) 1.77/1.78. Took 7.26 sec\n",
            "Epoch 3, Acc(train/val): 38.33/37.78, Loss(train/val) 1.70/1.69. Took 7.74 sec\n",
            "Epoch 4, Acc(train/val): 39.59/39.96, Loss(train/val) 1.67/1.66. Took 7.69 sec\n",
            "Epoch 5, Acc(train/val): 39.89/37.95, Loss(train/val) 1.67/1.71. Took 7.30 sec\n",
            "Epoch 6, Acc(train/val): 38.74/22.84, Loss(train/val) 1.70/3.06. Took 7.57 sec\n",
            "Epoch 7, Acc(train/val): 40.77/41.52, Loss(train/val) 1.64/1.62. Took 8.41 sec\n",
            "Epoch 8, Acc(train/val): 40.51/40.38, Loss(train/val) 1.64/1.63. Took 7.40 sec\n",
            "Epoch 9, Acc(train/val): 42.01/41.93, Loss(train/val) 1.61/1.61. Took 8.15 sec\n",
            "| \u001b[39m28       \u001b[39m | \u001b[39m-1.605   \u001b[39m | \u001b[39m0.1075   \u001b[39m | \u001b[39m0.1656   \u001b[39m | \u001b[39m2.895e+03\u001b[39m | \u001b[39m0.0007038\u001b[39m | \u001b[39m0.003479 \u001b[39m | \u001b[39m4.05     \u001b[39m | \u001b[39m2.015    \u001b[39m | \u001b[39m89.98    \u001b[39m | \u001b[39m125.4    \u001b[39m | \u001b[39m0.458    \u001b[39m | \u001b[39m0.1462   \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=1, hid_dim=2321, act='relu', dropout=np.float64(0.30575456150870095), use_bn=True, l2=np.float64(7.254797247452703e-05), w_initial='xavier', optim='ADAM', lr=np.float64(0.003913108676928642), epoch=10, train_batch_size=5, test_batch_size=65, exp_name='exp27')\n",
            "Epoch 0, Acc(train/val): 24.23/27.66, Loss(train/val) 34.85/29.06. Took 47.08 sec\n",
            "Epoch 1, Acc(train/val): 27.39/24.56, Loss(train/val) 32.13/45.63. Took 42.37 sec\n",
            "Epoch 2, Acc(train/val): 28.95/29.28, Loss(train/val) 30.63/35.83. Took 37.36 sec\n",
            "Epoch 3, Acc(train/val): 29.44/28.90, Loss(train/val) 28.83/27.66. Took 38.81 sec\n",
            "Epoch 4, Acc(train/val): 30.07/28.26, Loss(train/val) 28.68/31.06. Took 42.76 sec\n",
            "Epoch 5, Acc(train/val): 30.58/29.78, Loss(train/val) 29.00/26.39. Took 39.27 sec\n",
            "Epoch 6, Acc(train/val): 30.74/28.46, Loss(train/val) 27.98/35.62. Took 40.88 sec\n",
            "Epoch 7, Acc(train/val): 30.71/33.11, Loss(train/val) 28.18/22.22. Took 40.76 sec\n",
            "Epoch 8, Acc(train/val): 31.63/30.46, Loss(train/val) 28.15/26.72. Took 39.04 sec\n",
            "Epoch 9, Acc(train/val): 31.76/28.38, Loss(train/val) 28.49/34.31. Took 41.59 sec\n",
            "| \u001b[39m29       \u001b[39m | \u001b[39m-34.31   \u001b[39m | \u001b[39m0.1816   \u001b[39m | \u001b[39m0.3058   \u001b[39m | \u001b[39m2.321e+03\u001b[39m | \u001b[39m7.255e-05\u001b[39m | \u001b[39m0.003913 \u001b[39m | \u001b[39m1.39     \u001b[39m | \u001b[39m2.032    \u001b[39m | \u001b[39m65.42    \u001b[39m | \u001b[39m5.329    \u001b[39m | \u001b[39m0.4898   \u001b[39m | \u001b[39m0.856    \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=4, hid_dim=2824, act='relu', dropout=np.float64(0.028566076336998203), use_bn=False, l2=np.float64(0.0006349007797134496), w_initial='he', optim='SGD', lr=np.float64(0.004349838812414581), epoch=10, train_batch_size=150, test_batch_size=138, exp_name='exp28')\n",
            "Epoch 0, Acc(train/val): 26.12/30.84, Loss(train/val) 2.12/1.98. Took 7.13 sec\n",
            "Epoch 1, Acc(train/val): 33.87/35.09, Loss(train/val) 1.89/1.83. Took 7.10 sec\n",
            "Epoch 2, Acc(train/val): 38.16/38.63, Loss(train/val) 1.77/1.75. Took 6.70 sec\n",
            "Epoch 3, Acc(train/val): 40.88/40.70, Loss(train/val) 1.69/1.69. Took 7.49 sec\n",
            "Epoch 4, Acc(train/val): 43.27/42.66, Loss(train/val) 1.63/1.64. Took 7.08 sec\n",
            "Epoch 5, Acc(train/val): 45.07/43.67, Loss(train/val) 1.58/1.61. Took 7.44 sec\n",
            "Epoch 6, Acc(train/val): 46.54/44.51, Loss(train/val) 1.53/1.58. Took 6.72 sec\n",
            "Epoch 7, Acc(train/val): 48.04/45.77, Loss(train/val) 1.50/1.56. Took 7.14 sec\n",
            "Epoch 8, Acc(train/val): 49.12/46.55, Loss(train/val) 1.46/1.53. Took 7.38 sec\n",
            "Epoch 9, Acc(train/val): 50.36/47.30, Loss(train/val) 1.42/1.51. Took 7.23 sec\n",
            "| \u001b[39m30       \u001b[39m | \u001b[39m-1.511   \u001b[39m | \u001b[39m0.1305   \u001b[39m | \u001b[39m0.02857  \u001b[39m | \u001b[39m2.825e+03\u001b[39m | \u001b[39m0.0006349\u001b[39m | \u001b[39m0.00435  \u001b[39m | \u001b[39m4.577    \u001b[39m | \u001b[39m0.003301 \u001b[39m | \u001b[39m138.2    \u001b[39m | \u001b[39m150.1    \u001b[39m | \u001b[39m0.8641   \u001b[39m | \u001b[39m0.1501   \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=4, hid_dim=955, act='relu', dropout=np.float64(0.2688167723879525), use_bn=False, l2=np.float64(0.0006444168693023437), w_initial='he', optim='SGD', lr=np.float64(0.005386774411494756), epoch=10, train_batch_size=27, test_batch_size=131, exp_name='exp29')\n",
            "Epoch 0, Acc(train/val): 31.45/39.22, Loss(train/val) 1.91/1.69. Took 9.92 sec\n",
            "Epoch 1, Acc(train/val): 40.69/43.76, Loss(train/val) 1.66/1.59. Took 9.09 sec\n",
            "Epoch 2, Acc(train/val): 44.96/46.25, Loss(train/val) 1.55/1.52. Took 9.45 sec\n",
            "Epoch 3, Acc(train/val): 47.66/47.81, Loss(train/val) 1.48/1.48. Took 9.81 sec\n",
            "Epoch 4, Acc(train/val): 50.09/48.85, Loss(train/val) 1.41/1.46. Took 9.83 sec\n",
            "Epoch 5, Acc(train/val): 51.88/49.87, Loss(train/val) 1.36/1.43. Took 9.90 sec\n",
            "Epoch 6, Acc(train/val): 53.89/50.27, Loss(train/val) 1.31/1.42. Took 8.70 sec\n",
            "Epoch 7, Acc(train/val): 55.35/50.73, Loss(train/val) 1.26/1.40. Took 9.09 sec\n",
            "Epoch 8, Acc(train/val): 56.80/52.42, Loss(train/val) 1.21/1.37. Took 8.78 sec\n",
            "Epoch 9, Acc(train/val): 58.41/52.89, Loss(train/val) 1.17/1.38. Took 10.33 sec\n",
            "| \u001b[35m31       \u001b[39m | \u001b[35m-1.379   \u001b[39m | \u001b[35m0.2236   \u001b[39m | \u001b[35m0.2688   \u001b[39m | \u001b[35m955.9    \u001b[39m | \u001b[35m0.0006444\u001b[39m | \u001b[35m0.005387 \u001b[39m | \u001b[35m4.229    \u001b[39m | \u001b[35m0.3243   \u001b[39m | \u001b[35m131.3    \u001b[39m | \u001b[35m27.7     \u001b[39m | \u001b[35m0.8675   \u001b[39m | \u001b[35m0.1655   \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=4, hid_dim=2570, act='leakyrelu', dropout=np.float64(0.14065848860220748), use_bn=False, l2=np.float64(0.00014981741146729752), w_initial='he', optim='RMSprop', lr=np.float64(0.00980240290591749), epoch=10, train_batch_size=113, test_batch_size=85, exp_name='exp30')\n",
            "Epoch 0, Acc(train/val): 12.20/17.20, Loss(train/val) 784153.28/530.22. Took 7.38 sec\n",
            "Epoch 1, Acc(train/val): 12.52/13.71, Loss(train/val) 12343507.74/38230.06. Took 6.93 sec\n",
            "Epoch 2, Acc(train/val): 12.24/11.63, Loss(train/val) 26431.24/12310.73. Took 7.07 sec\n",
            "Epoch 3, Acc(train/val): 12.57/9.87, Loss(train/val) 16062.79/16341.98. Took 7.09 sec\n",
            "Epoch 4, Acc(train/val): 12.76/10.39, Loss(train/val) 52445.03/10185.33. Took 7.94 sec\n",
            "Epoch 5, Acc(train/val): 12.72/13.28, Loss(train/val) 256418.92/5905.44. Took 6.94 sec\n",
            "Epoch 6, Acc(train/val): 12.78/10.26, Loss(train/val) 200960.28/27113.30. Took 7.82 sec\n",
            "Epoch 7, Acc(train/val): 13.03/12.70, Loss(train/val) 16843.52/14165.15. Took 7.79 sec\n",
            "Epoch 8, Acc(train/val): 12.64/12.89, Loss(train/val) 1208908.41/73097.79. Took 7.54 sec\n",
            "Epoch 9, Acc(train/val): 12.22/11.19, Loss(train/val) 50410.59/12009.29. Took 7.02 sec\n",
            "| \u001b[39m32       \u001b[39m | \u001b[39m-1.201e+0\u001b[39m | \u001b[39m0.4777   \u001b[39m | \u001b[39m0.1407   \u001b[39m | \u001b[39m2.57e+03 \u001b[39m | \u001b[39m0.0001498\u001b[39m | \u001b[39m0.009802 \u001b[39m | \u001b[39m4.152    \u001b[39m | \u001b[39m1.876    \u001b[39m | \u001b[39m85.62    \u001b[39m | \u001b[39m113.8    \u001b[39m | \u001b[39m0.7366   \u001b[39m | \u001b[39m0.439    \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=5, hid_dim=971, act='tanh', dropout=np.float64(0.15812363887101993), use_bn=False, l2=np.float64(0.0002988868203119712), w_initial='xavier', optim='ADAM', lr=np.float64(0.003588198583760901), epoch=10, train_batch_size=114, test_batch_size=224, exp_name='exp31')\n",
            "Epoch 0, Acc(train/val): 17.25/20.88, Loss(train/val) 2.32/2.22. Took 7.67 sec\n",
            "Epoch 1, Acc(train/val): 23.54/24.28, Loss(train/val) 2.13/2.09. Took 7.28 sec\n",
            "Epoch 2, Acc(train/val): 22.85/22.97, Loss(train/val) 2.16/2.21. Took 7.47 sec\n",
            "Epoch 3, Acc(train/val): 23.31/24.52, Loss(train/val) 2.14/2.03. Took 7.08 sec\n",
            "Epoch 4, Acc(train/val): 24.17/19.48, Loss(train/val) 2.13/2.26. Took 7.04 sec\n",
            "Epoch 5, Acc(train/val): 23.86/22.72, Loss(train/val) 2.14/2.29. Took 7.30 sec\n",
            "Epoch 6, Acc(train/val): 22.68/24.77, Loss(train/val) 2.16/2.10. Took 6.72 sec\n",
            "Epoch 7, Acc(train/val): 22.93/22.21, Loss(train/val) 2.15/2.11. Took 7.36 sec\n",
            "Epoch 8, Acc(train/val): 24.42/24.28, Loss(train/val) 2.13/2.14. Took 7.04 sec\n",
            "Epoch 9, Acc(train/val): 23.30/24.50, Loss(train/val) 2.14/2.17. Took 7.32 sec\n",
            "| \u001b[39m33       \u001b[39m | \u001b[39m-2.169   \u001b[39m | \u001b[39m0.72     \u001b[39m | \u001b[39m0.1581   \u001b[39m | \u001b[39m971.5    \u001b[39m | \u001b[39m0.0002989\u001b[39m | \u001b[39m0.003588 \u001b[39m | \u001b[39m5.884    \u001b[39m | \u001b[39m2.904    \u001b[39m | \u001b[39m224.3    \u001b[39m | \u001b[39m114.8    \u001b[39m | \u001b[39m0.8049   \u001b[39m | \u001b[39m0.5922   \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=3, hid_dim=2399, act='relu', dropout=np.float64(0.2182264690994256), use_bn=True, l2=np.float64(0.0008397994072472151), w_initial='xavier', optim='SGD', lr=np.float64(0.0012728491239111456), epoch=10, train_batch_size=12, test_batch_size=162, exp_name='exp32')\n",
            "Epoch 0, Acc(train/val): 37.08/42.52, Loss(train/val) 1.78/1.63. Took 21.65 sec\n",
            "Epoch 1, Acc(train/val): 43.76/45.13, Loss(train/val) 1.60/1.57. Took 21.90 sec\n",
            "Epoch 2, Acc(train/val): 46.25/48.08, Loss(train/val) 1.52/1.50. Took 20.79 sec\n",
            "Epoch 3, Acc(train/val): 48.78/49.05, Loss(train/val) 1.45/1.47. Took 20.36 sec\n",
            "Epoch 4, Acc(train/val): 50.86/48.18, Loss(train/val) 1.40/1.48. Took 21.65 sec\n",
            "Epoch 5, Acc(train/val): 51.92/49.14, Loss(train/val) 1.36/1.47. Took 21.92 sec\n",
            "Epoch 6, Acc(train/val): 53.54/50.37, Loss(train/val) 1.32/1.43. Took 21.84 sec\n",
            "Epoch 7, Acc(train/val): 55.20/50.99, Loss(train/val) 1.27/1.42. Took 20.07 sec\n",
            "Epoch 8, Acc(train/val): 56.73/51.02, Loss(train/val) 1.23/1.45. Took 21.60 sec\n",
            "Epoch 9, Acc(train/val): 57.80/51.05, Loss(train/val) 1.20/1.44. Took 21.68 sec\n",
            "| \u001b[39m34       \u001b[39m | \u001b[39m-1.444   \u001b[39m | \u001b[39m0.02501  \u001b[39m | \u001b[39m0.2182   \u001b[39m | \u001b[39m2.4e+03  \u001b[39m | \u001b[39m0.0008398\u001b[39m | \u001b[39m0.001273 \u001b[39m | \u001b[39m3.047    \u001b[39m | \u001b[39m0.5844   \u001b[39m | \u001b[39m162.3    \u001b[39m | \u001b[39m12.36    \u001b[39m | \u001b[39m0.3456   \u001b[39m | \u001b[39m0.829    \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=3, hid_dim=2475, act='relu', dropout=np.float64(0.2015952057508787), use_bn=True, l2=np.float64(0.0007812824199267544), w_initial='he', optim='ADAM', lr=np.float64(0.0045504844192953255), epoch=10, train_batch_size=227, test_batch_size=194, exp_name='exp33')\n",
            "Epoch 0, Acc(train/val): 29.64/30.88, Loss(train/val) 2.19/2.00. Took 6.80 sec\n",
            "Epoch 1, Acc(train/val): 37.33/36.97, Loss(train/val) 1.78/1.76. Took 7.51 sec\n",
            "Epoch 2, Acc(train/val): 39.33/37.48, Loss(train/val) 1.70/1.74. Took 6.72 sec\n",
            "Epoch 3, Acc(train/val): 40.75/40.03, Loss(train/val) 1.64/1.67. Took 6.58 sec\n",
            "Epoch 4, Acc(train/val): 41.83/40.19, Loss(train/val) 1.62/1.66. Took 6.66 sec\n",
            "Epoch 5, Acc(train/val): 42.50/39.68, Loss(train/val) 1.60/1.67. Took 7.01 sec\n",
            "Epoch 6, Acc(train/val): 43.03/41.22, Loss(train/val) 1.58/1.63. Took 7.30 sec\n",
            "Epoch 7, Acc(train/val): 43.65/37.87, Loss(train/val) 1.57/1.73. Took 6.89 sec\n",
            "Epoch 8, Acc(train/val): 43.53/41.51, Loss(train/val) 1.57/1.61. Took 7.14 sec\n",
            "Epoch 9, Acc(train/val): 43.84/42.28, Loss(train/val) 1.57/1.60. Took 7.48 sec\n",
            "| \u001b[39m35       \u001b[39m | \u001b[39m-1.605   \u001b[39m | \u001b[39m0.000786 \u001b[39m | \u001b[39m0.2016   \u001b[39m | \u001b[39m2.475e+03\u001b[39m | \u001b[39m0.0007813\u001b[39m | \u001b[39m0.00455  \u001b[39m | \u001b[39m3.36     \u001b[39m | \u001b[39m2.902    \u001b[39m | \u001b[39m194.7    \u001b[39m | \u001b[39m227.2    \u001b[39m | \u001b[39m0.1039   \u001b[39m | \u001b[39m0.3245   \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=5, hid_dim=2338, act='tanh', dropout=np.float64(0.4140018936337093), use_bn=False, l2=np.float64(0.0003924258410945197), w_initial='he', optim='SGD', lr=np.float64(0.0033342886068416905), epoch=10, train_batch_size=133, test_batch_size=115, exp_name='exp34')\n",
            "Epoch 0, Acc(train/val): 24.99/34.76, Loss(train/val) 2.07/1.92. Took 7.44 sec\n",
            "Epoch 1, Acc(train/val): 31.47/36.60, Loss(train/val) 1.92/1.90. Took 6.94 sec\n",
            "Epoch 2, Acc(train/val): 33.34/37.96, Loss(train/val) 1.87/1.88. Took 7.36 sec\n",
            "Epoch 3, Acc(train/val): 34.84/38.33, Loss(train/val) 1.84/1.88. Took 6.84 sec\n",
            "Epoch 4, Acc(train/val): 36.16/39.24, Loss(train/val) 1.81/1.85. Took 7.68 sec\n",
            "Epoch 5, Acc(train/val): 37.12/39.29, Loss(train/val) 1.79/1.85. Took 7.39 sec\n",
            "Epoch 6, Acc(train/val): 37.33/39.88, Loss(train/val) 1.78/1.83. Took 7.73 sec\n",
            "Epoch 7, Acc(train/val): 37.91/39.71, Loss(train/val) 1.76/1.84. Took 7.05 sec\n",
            "Epoch 8, Acc(train/val): 38.38/40.55, Loss(train/val) 1.75/1.83. Took 6.76 sec\n",
            "Epoch 9, Acc(train/val): 38.89/40.62, Loss(train/val) 1.75/1.82. Took 7.02 sec\n",
            "| \u001b[39m36       \u001b[39m | \u001b[39m-1.821   \u001b[39m | \u001b[39m0.7104   \u001b[39m | \u001b[39m0.414    \u001b[39m | \u001b[39m2.338e+03\u001b[39m | \u001b[39m0.0003924\u001b[39m | \u001b[39m0.003334 \u001b[39m | \u001b[39m5.923    \u001b[39m | \u001b[39m0.3549   \u001b[39m | \u001b[39m115.8    \u001b[39m | \u001b[39m133.8    \u001b[39m | \u001b[39m0.9741   \u001b[39m | \u001b[39m0.44     \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=2, hid_dim=924, act='tanh', dropout=np.float64(0.3410328065042108), use_bn=False, l2=np.float64(0.0005776783784038366), w_initial='xavier', optim='SGD', lr=np.float64(0.0015438364280506356), epoch=10, train_batch_size=7, test_batch_size=241, exp_name='exp35')\n",
            "Epoch 0, Acc(train/val): 35.11/38.23, Loss(train/val) 1.86/1.79. Took 26.88 sec\n",
            "Epoch 1, Acc(train/val): 39.75/39.83, Loss(train/val) 1.74/1.75. Took 26.76 sec\n",
            "Epoch 2, Acc(train/val): 41.10/40.66, Loss(train/val) 1.70/1.72. Took 26.65 sec\n",
            "Epoch 3, Acc(train/val): 42.19/41.49, Loss(train/val) 1.67/1.70. Took 28.55 sec\n",
            "Epoch 4, Acc(train/val): 43.05/41.45, Loss(train/val) 1.64/1.69. Took 26.34 sec\n",
            "Epoch 5, Acc(train/val): 43.78/42.63, Loss(train/val) 1.62/1.68. Took 26.20 sec\n",
            "Epoch 6, Acc(train/val): 44.93/43.27, Loss(train/val) 1.60/1.66. Took 26.35 sec\n",
            "Epoch 7, Acc(train/val): 45.63/44.52, Loss(train/val) 1.58/1.64. Took 24.79 sec\n",
            "Epoch 8, Acc(train/val): 46.30/44.63, Loss(train/val) 1.55/1.63. Took 28.29 sec\n",
            "Epoch 9, Acc(train/val): 47.34/44.44, Loss(train/val) 1.53/1.61. Took 29.47 sec\n",
            "| \u001b[39m37       \u001b[39m | \u001b[39m-1.611   \u001b[39m | \u001b[39m0.6957   \u001b[39m | \u001b[39m0.341    \u001b[39m | \u001b[39m924.6    \u001b[39m | \u001b[39m0.0005777\u001b[39m | \u001b[39m0.001544 \u001b[39m | \u001b[39m2.837    \u001b[39m | \u001b[39m0.7188   \u001b[39m | \u001b[39m241.1    \u001b[39m | \u001b[39m7.699    \u001b[39m | \u001b[39m0.6439   \u001b[39m | \u001b[39m0.8195   \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=2, hid_dim=2696, act='tanh', dropout=np.float64(0.3310306675198064), use_bn=True, l2=np.float64(0.0003413667038014207), w_initial='he', optim='RMSprop', lr=np.float64(0.005134823130626914), epoch=10, train_batch_size=5, test_batch_size=255, exp_name='exp36')\n",
            "Epoch 0, Acc(train/val): 15.52/16.31, Loss(train/val) 3.68/2.35. Took 43.15 sec\n",
            "Epoch 1, Acc(train/val): 15.72/15.01, Loss(train/val) 2.38/2.28. Took 44.78 sec\n",
            "Epoch 2, Acc(train/val): 15.71/13.02, Loss(train/val) 2.38/2.35. Took 43.44 sec\n",
            "Epoch 3, Acc(train/val): 16.00/19.50, Loss(train/val) 2.38/2.18. Took 45.19 sec\n",
            "Epoch 4, Acc(train/val): 15.69/16.60, Loss(train/val) 2.38/2.49. Took 46.03 sec\n",
            "Epoch 5, Acc(train/val): 15.70/19.64, Loss(train/val) 2.38/2.21. Took 44.40 sec\n",
            "Epoch 6, Acc(train/val): 15.76/17.36, Loss(train/val) 2.38/2.34. Took 45.74 sec\n",
            "Epoch 7, Acc(train/val): 15.95/14.28, Loss(train/val) 2.38/2.72. Took 47.08 sec\n",
            "Epoch 8, Acc(train/val): 15.82/13.37, Loss(train/val) 2.38/2.41. Took 48.31 sec\n",
            "Epoch 9, Acc(train/val): 15.71/18.04, Loss(train/val) 2.39/2.51. Took 46.78 sec\n",
            "| \u001b[39m38       \u001b[39m | \u001b[39m-2.514   \u001b[39m | \u001b[39m0.6999   \u001b[39m | \u001b[39m0.331    \u001b[39m | \u001b[39m2.697e+03\u001b[39m | \u001b[39m0.0003414\u001b[39m | \u001b[39m0.005135 \u001b[39m | \u001b[39m2.799    \u001b[39m | \u001b[39m1.523    \u001b[39m | \u001b[39m255.7    \u001b[39m | \u001b[39m5.042    \u001b[39m | \u001b[39m0.1244   \u001b[39m | \u001b[39m0.3896   \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=5, hid_dim=2827, act='leakyrelu', dropout=np.float64(0.4848062015750515), use_bn=False, l2=np.float64(0.00029381042380507695), w_initial='he', optim='RMSprop', lr=np.float64(0.00784730090028311), epoch=10, train_batch_size=146, test_batch_size=141, exp_name='exp37')\n",
            "Epoch 0, Acc(train/val): 10.07/10.06, Loss(train/val) 8097490.66/4093.55. Took 6.55 sec\n",
            "Epoch 1, Acc(train/val): 10.23/10.45, Loss(train/val) 5999.59/1165.97. Took 7.53 sec\n",
            "Epoch 2, Acc(train/val): 10.09/10.11, Loss(train/val) 3494.54/1579.13. Took 6.82 sec\n",
            "Epoch 3, Acc(train/val): 10.38/11.72, Loss(train/val) 5123.49/228.62. Took 7.29 sec\n",
            "Epoch 4, Acc(train/val): 10.35/10.33, Loss(train/val) 185570373.36/188041.22. Took 6.88 sec\n",
            "Epoch 5, Acc(train/val): 10.45/8.56, Loss(train/val) 337995.89/142982.52. Took 6.81 sec\n",
            "Epoch 6, Acc(train/val): 10.21/13.02, Loss(train/val) 105784.47/94706.19. Took 6.92 sec\n",
            "Epoch 7, Acc(train/val): 10.40/10.08, Loss(train/val) 106218.56/30622.89. Took 6.96 sec\n",
            "Epoch 8, Acc(train/val): 10.29/10.41, Loss(train/val) 53622.79/12391.36. Took 7.19 sec\n",
            "Epoch 9, Acc(train/val): 10.23/10.92, Loss(train/val) 102366.48/14332.51. Took 7.33 sec\n",
            "| \u001b[39m39       \u001b[39m | \u001b[39m-1.433e+0\u001b[39m | \u001b[39m0.2594   \u001b[39m | \u001b[39m0.4848   \u001b[39m | \u001b[39m2.827e+03\u001b[39m | \u001b[39m0.0002938\u001b[39m | \u001b[39m0.007847 \u001b[39m | \u001b[39m5.477    \u001b[39m | \u001b[39m1.015    \u001b[39m | \u001b[39m141.5    \u001b[39m | \u001b[39m146.2    \u001b[39m | \u001b[39m0.5351   \u001b[39m | \u001b[39m0.4768   \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=1, hid_dim=2924, act='leakyrelu', dropout=np.float64(0.3974518721398055), use_bn=True, l2=np.float64(0.0003611617471380902), w_initial='xavier', optim='SGD', lr=np.float64(0.0019843308508906065), epoch=10, train_batch_size=222, test_batch_size=20, exp_name='exp38')\n",
            "Epoch 0, Acc(train/val): 21.96/27.40, Loss(train/val) 2.20/2.12. Took 7.88 sec\n",
            "Epoch 1, Acc(train/val): 28.97/29.87, Loss(train/val) 2.07/2.03. Took 7.62 sec\n",
            "Epoch 2, Acc(train/val): 31.55/31.91, Loss(train/val) 2.00/1.98. Took 7.79 sec\n",
            "Epoch 3, Acc(train/val): 33.25/33.02, Loss(train/val) 1.95/1.94. Took 7.82 sec\n",
            "Epoch 4, Acc(train/val): 34.45/34.25, Loss(train/val) 1.91/1.90. Took 7.99 sec\n",
            "Epoch 5, Acc(train/val): 35.56/35.46, Loss(train/val) 1.88/1.88. Took 7.40 sec\n",
            "Epoch 6, Acc(train/val): 36.47/36.10, Loss(train/val) 1.85/1.85. Took 8.13 sec\n",
            "Epoch 7, Acc(train/val): 37.12/36.82, Loss(train/val) 1.83/1.83. Took 8.43 sec\n",
            "Epoch 8, Acc(train/val): 37.78/37.29, Loss(train/val) 1.81/1.82. Took 7.82 sec\n",
            "Epoch 9, Acc(train/val): 38.43/37.63, Loss(train/val) 1.79/1.80. Took 8.08 sec\n",
            "| \u001b[39m40       \u001b[39m | \u001b[39m-1.802   \u001b[39m | \u001b[39m0.4689   \u001b[39m | \u001b[39m0.3975   \u001b[39m | \u001b[39m2.924e+03\u001b[39m | \u001b[39m0.0003612\u001b[39m | \u001b[39m0.001984 \u001b[39m | \u001b[39m1.093    \u001b[39m | \u001b[39m0.9854   \u001b[39m | \u001b[39m20.07    \u001b[39m | \u001b[39m222.8    \u001b[39m | \u001b[39m0.1928   \u001b[39m | \u001b[39m0.8816   \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=4, hid_dim=1449, act='leakyrelu', dropout=np.float64(0.26094966030885164), use_bn=True, l2=np.float64(0.0008524526009065634), w_initial='he', optim='RMSprop', lr=np.float64(0.0021999162027682042), epoch=10, train_batch_size=205, test_batch_size=178, exp_name='exp39')\n",
            "Epoch 0, Acc(train/val): 28.72/24.42, Loss(train/val) 2.49/3.15. Took 7.14 sec\n",
            "Epoch 1, Acc(train/val): 34.35/11.49, Loss(train/val) 1.94/124.98. Took 7.47 sec\n",
            "Epoch 2, Acc(train/val): 33.83/24.97, Loss(train/val) 1.92/3.56. Took 7.18 sec\n",
            "Epoch 3, Acc(train/val): 37.04/22.57, Loss(train/val) 1.77/3.66. Took 6.75 sec\n",
            "Epoch 4, Acc(train/val): 40.05/26.93, Loss(train/val) 1.68/2.11. Took 7.01 sec\n",
            "Epoch 5, Acc(train/val): 41.77/30.58, Loss(train/val) 1.62/2.07. Took 6.89 sec\n",
            "Epoch 6, Acc(train/val): 43.06/23.97, Loss(train/val) 1.59/2.28. Took 7.08 sec\n",
            "Epoch 7, Acc(train/val): 43.80/26.73, Loss(train/val) 1.56/2.23. Took 7.17 sec\n",
            "Epoch 8, Acc(train/val): 44.80/27.20, Loss(train/val) 1.55/2.18. Took 7.22 sec\n",
            "Epoch 9, Acc(train/val): 45.26/27.74, Loss(train/val) 1.53/2.21. Took 6.83 sec\n",
            "| \u001b[39m41       \u001b[39m | \u001b[39m-2.207   \u001b[39m | \u001b[39m0.2949   \u001b[39m | \u001b[39m0.2609   \u001b[39m | \u001b[39m1.449e+03\u001b[39m | \u001b[39m0.0008525\u001b[39m | \u001b[39m0.0022   \u001b[39m | \u001b[39m4.011    \u001b[39m | \u001b[39m1.274    \u001b[39m | \u001b[39m178.7    \u001b[39m | \u001b[39m206.0    \u001b[39m | \u001b[39m0.003797 \u001b[39m | \u001b[39m0.3158   \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=4, hid_dim=1320, act='relu', dropout=np.float64(0.28143710206576567), use_bn=False, l2=np.float64(0.0009703873384166691), w_initial='he', optim='SGD', lr=np.float64(0.0016556940507453828), epoch=10, train_batch_size=217, test_batch_size=97, exp_name='exp40')\n",
            "Epoch 0, Acc(train/val): 13.46/23.78, Loss(train/val) 2.29/2.23. Took 6.89 sec\n",
            "Epoch 1, Acc(train/val): 20.13/26.79, Loss(train/val) 2.22/2.16. Took 5.93 sec\n",
            "Epoch 2, Acc(train/val): 23.60/28.74, Loss(train/val) 2.15/2.10. Took 6.87 sec\n",
            "Epoch 3, Acc(train/val): 25.69/29.50, Loss(train/val) 2.10/2.04. Took 7.06 sec\n",
            "Epoch 4, Acc(train/val): 27.49/30.54, Loss(train/val) 2.05/2.00. Took 7.29 sec\n",
            "Epoch 5, Acc(train/val): 28.84/31.41, Loss(train/val) 2.01/1.96. Took 7.06 sec\n",
            "Epoch 6, Acc(train/val): 29.78/32.22, Loss(train/val) 1.97/1.92. Took 7.43 sec\n",
            "Epoch 7, Acc(train/val): 31.18/33.52, Loss(train/val) 1.94/1.89. Took 6.86 sec\n",
            "Epoch 8, Acc(train/val): 32.23/34.24, Loss(train/val) 1.91/1.87. Took 7.06 sec\n",
            "Epoch 9, Acc(train/val): 33.47/35.01, Loss(train/val) 1.89/1.85. Took 7.05 sec\n",
            "| \u001b[39m42       \u001b[39m | \u001b[39m-1.847   \u001b[39m | \u001b[39m0.0124   \u001b[39m | \u001b[39m0.2814   \u001b[39m | \u001b[39m1.32e+03 \u001b[39m | \u001b[39m0.0009704\u001b[39m | \u001b[39m0.001656 \u001b[39m | \u001b[39m4.913    \u001b[39m | \u001b[39m0.5332   \u001b[39m | \u001b[39m97.93    \u001b[39m | \u001b[39m217.7    \u001b[39m | \u001b[39m0.6682   \u001b[39m | \u001b[39m0.09921  \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=2, hid_dim=2247, act='relu', dropout=np.float64(0.2922038950094229), use_bn=True, l2=np.float64(0.00011739549416058032), w_initial='xavier', optim='ADAM', lr=np.float64(0.0008623046682770091), epoch=10, train_batch_size=47, test_batch_size=134, exp_name='exp41')\n",
            "Epoch 0, Acc(train/val): 36.67/40.46, Loss(train/val) 1.82/2.02. Took 8.97 sec\n",
            "Epoch 1, Acc(train/val): 44.20/41.68, Loss(train/val) 1.58/1.73. Took 8.22 sec\n",
            "Epoch 2, Acc(train/val): 46.28/46.40, Loss(train/val) 1.52/1.56. Took 8.24 sec\n",
            "Epoch 3, Acc(train/val): 46.59/46.72, Loss(train/val) 1.50/1.59. Took 7.70 sec\n",
            "Epoch 4, Acc(train/val): 48.14/48.07, Loss(train/val) 1.46/1.53. Took 7.97 sec\n",
            "Epoch 5, Acc(train/val): 50.02/46.90, Loss(train/val) 1.41/1.52. Took 8.16 sec\n",
            "Epoch 6, Acc(train/val): 51.05/46.46, Loss(train/val) 1.38/2.23. Took 7.54 sec\n",
            "Epoch 7, Acc(train/val): 50.11/48.25, Loss(train/val) 1.40/1.53. Took 7.51 sec\n",
            "Epoch 8, Acc(train/val): 51.74/33.91, Loss(train/val) 1.36/5.57. Took 7.80 sec\n",
            "Epoch 9, Acc(train/val): 53.32/49.71, Loss(train/val) 1.31/1.47. Took 6.70 sec\n",
            "| \u001b[39m43       \u001b[39m | \u001b[39m-1.47    \u001b[39m | \u001b[39m0.04141  \u001b[39m | \u001b[39m0.2922   \u001b[39m | \u001b[39m2.248e+03\u001b[39m | \u001b[39m0.0001174\u001b[39m | \u001b[39m0.0008623\u001b[39m | \u001b[39m2.543    \u001b[39m | \u001b[39m2.764    \u001b[39m | \u001b[39m134.5    \u001b[39m | \u001b[39m47.08    \u001b[39m | \u001b[39m0.07398  \u001b[39m | \u001b[39m0.5105   \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=1, hid_dim=645, act='tanh', dropout=np.float64(0.14036522006191443), use_bn=False, l2=np.float64(0.0003122334714313893), w_initial='xavier', optim='ADAM', lr=np.float64(0.0047967127070993746), epoch=10, train_batch_size=129, test_batch_size=152, exp_name='exp42')\n",
            "Epoch 0, Acc(train/val): 31.97/28.90, Loss(train/val) 1.96/1.99. Took 7.12 sec\n",
            "Epoch 1, Acc(train/val): 33.34/30.43, Loss(train/val) 1.91/1.98. Took 7.16 sec\n",
            "Epoch 2, Acc(train/val): 33.48/30.30, Loss(train/val) 1.91/2.05. Took 7.40 sec\n",
            "Epoch 3, Acc(train/val): 33.35/31.56, Loss(train/val) 1.91/2.00. Took 6.70 sec\n",
            "Epoch 4, Acc(train/val): 33.44/28.29, Loss(train/val) 1.91/2.05. Took 7.51 sec\n",
            "Epoch 5, Acc(train/val): 33.83/30.65, Loss(train/val) 1.91/2.02. Took 6.98 sec\n",
            "Epoch 6, Acc(train/val): 33.41/33.55, Loss(train/val) 1.91/1.93. Took 6.89 sec\n",
            "Epoch 7, Acc(train/val): 33.55/30.70, Loss(train/val) 1.91/1.97. Took 6.98 sec\n",
            "Epoch 8, Acc(train/val): 33.89/30.42, Loss(train/val) 1.91/1.98. Took 7.34 sec\n",
            "Epoch 9, Acc(train/val): 33.35/31.78, Loss(train/val) 1.91/2.01. Took 7.02 sec\n",
            "| \u001b[39m44       \u001b[39m | \u001b[39m-2.013   \u001b[39m | \u001b[39m0.5102   \u001b[39m | \u001b[39m0.1404   \u001b[39m | \u001b[39m645.0    \u001b[39m | \u001b[39m0.0003122\u001b[39m | \u001b[39m0.004797 \u001b[39m | \u001b[39m1.531    \u001b[39m | \u001b[39m2.159    \u001b[39m | \u001b[39m152.2    \u001b[39m | \u001b[39m129.2    \u001b[39m | \u001b[39m0.6312   \u001b[39m | \u001b[39m0.8868   \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=4, hid_dim=2373, act='tanh', dropout=np.float64(0.008082842793691869), use_bn=False, l2=np.float64(0.0001989762096561987), w_initial='xavier', optim='SGD', lr=np.float64(0.003614920785463726), epoch=10, train_batch_size=182, test_batch_size=89, exp_name='exp43')\n",
            "Epoch 0, Acc(train/val): 29.32/33.73, Loss(train/val) 2.03/1.92. Took 6.29 sec\n",
            "Epoch 1, Acc(train/val): 35.38/36.12, Loss(train/val) 1.87/1.86. Took 6.86 sec\n",
            "Epoch 2, Acc(train/val): 37.41/37.78, Loss(train/val) 1.82/1.82. Took 6.85 sec\n",
            "Epoch 3, Acc(train/val): 38.63/37.79, Loss(train/val) 1.79/1.80. Took 7.11 sec\n",
            "Epoch 4, Acc(train/val): 39.18/38.39, Loss(train/val) 1.77/1.79. Took 7.27 sec\n",
            "Epoch 5, Acc(train/val): 39.95/39.04, Loss(train/val) 1.75/1.77. Took 7.38 sec\n",
            "Epoch 6, Acc(train/val): 40.38/39.05, Loss(train/val) 1.74/1.76. Took 7.23 sec\n",
            "Epoch 7, Acc(train/val): 40.88/39.67, Loss(train/val) 1.72/1.75. Took 6.95 sec\n",
            "Epoch 8, Acc(train/val): 41.34/39.92, Loss(train/val) 1.71/1.75. Took 7.34 sec\n",
            "Epoch 9, Acc(train/val): 41.52/40.23, Loss(train/val) 1.70/1.74. Took 6.85 sec\n",
            "| \u001b[39m45       \u001b[39m | \u001b[39m-1.739   \u001b[39m | \u001b[39m0.6705   \u001b[39m | \u001b[39m0.008083 \u001b[39m | \u001b[39m2.373e+03\u001b[39m | \u001b[39m0.000199 \u001b[39m | \u001b[39m0.003615 \u001b[39m | \u001b[39m4.091    \u001b[39m | \u001b[39m0.7318   \u001b[39m | \u001b[39m89.78    \u001b[39m | \u001b[39m182.3    \u001b[39m | \u001b[39m0.7387   \u001b[39m | \u001b[39m0.9129   \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=3, hid_dim=1271, act='relu', dropout=np.float64(0.27752150367019435), use_bn=True, l2=np.float64(0.00013430899569960198), w_initial='he', optim='SGD', lr=np.float64(0.0009887900226157101), epoch=10, train_batch_size=6, test_batch_size=86, exp_name='exp44')\n",
            "Epoch 0, Acc(train/val): 29.51/38.25, Loss(train/val) 1.98/1.76. Took 35.12 sec\n",
            "Epoch 1, Acc(train/val): 33.91/40.40, Loss(train/val) 1.86/1.71. Took 37.38 sec\n",
            "Epoch 2, Acc(train/val): 36.30/40.65, Loss(train/val) 1.80/1.70. Took 36.00 sec\n",
            "Epoch 3, Acc(train/val): 37.54/42.25, Loss(train/val) 1.77/1.69. Took 37.85 sec\n",
            "Epoch 4, Acc(train/val): 38.64/41.55, Loss(train/val) 1.74/1.71. Took 35.23 sec\n",
            "Epoch 5, Acc(train/val): 39.07/42.33, Loss(train/val) 1.72/1.72. Took 37.02 sec\n",
            "Epoch 6, Acc(train/val): 40.42/43.92, Loss(train/val) 1.69/1.63. Took 36.08 sec\n",
            "Epoch 7, Acc(train/val): 41.11/43.99, Loss(train/val) 1.67/1.65. Took 33.83 sec\n",
            "Epoch 8, Acc(train/val): 41.60/45.18, Loss(train/val) 1.65/1.58. Took 35.61 sec\n",
            "Epoch 9, Acc(train/val): 42.65/45.18, Loss(train/val) 1.62/1.61. Took 35.98 sec\n",
            "| \u001b[39m46       \u001b[39m | \u001b[39m-1.612   \u001b[39m | \u001b[39m0.175    \u001b[39m | \u001b[39m0.2775   \u001b[39m | \u001b[39m1.271e+03\u001b[39m | \u001b[39m0.0001343\u001b[39m | \u001b[39m0.0009888\u001b[39m | \u001b[39m3.653    \u001b[39m | \u001b[39m0.5104   \u001b[39m | \u001b[39m86.19    \u001b[39m | \u001b[39m6.451    \u001b[39m | \u001b[39m0.02865  \u001b[39m | \u001b[39m0.1988   \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=5, hid_dim=924, act='leakyrelu', dropout=np.float64(0.16152103570341764), use_bn=True, l2=np.float64(0.00018333975809052834), w_initial='he', optim='ADAM', lr=np.float64(7.611147933185156e-05), epoch=10, train_batch_size=10, test_batch_size=239, exp_name='exp45')\n",
            "Epoch 0, Acc(train/val): 31.07/39.14, Loss(train/val) 1.93/1.72. Took 29.73 sec\n",
            "Epoch 1, Acc(train/val): 37.12/43.20, Loss(train/val) 1.76/1.61. Took 30.17 sec\n",
            "Epoch 2, Acc(train/val): 40.88/44.64, Loss(train/val) 1.66/1.58. Took 30.53 sec\n",
            "Epoch 3, Acc(train/val): 43.39/47.42, Loss(train/val) 1.59/1.51. Took 29.90 sec\n",
            "Epoch 4, Acc(train/val): 45.91/47.82, Loss(train/val) 1.52/1.50. Took 27.36 sec\n",
            "Epoch 5, Acc(train/val): 47.97/49.03, Loss(train/val) 1.46/1.45. Took 29.87 sec\n",
            "Epoch 6, Acc(train/val): 49.84/49.68, Loss(train/val) 1.42/1.44. Took 28.36 sec\n",
            "Epoch 7, Acc(train/val): 51.19/49.69, Loss(train/val) 1.37/1.42. Took 29.40 sec\n",
            "Epoch 8, Acc(train/val): 53.02/51.52, Loss(train/val) 1.33/1.39. Took 29.65 sec\n",
            "Epoch 9, Acc(train/val): 54.15/51.84, Loss(train/val) 1.29/1.38. Took 29.39 sec\n",
            "| \u001b[35m47       \u001b[39m | \u001b[35m-1.378   \u001b[39m | \u001b[35m0.4985   \u001b[39m | \u001b[35m0.1615   \u001b[39m | \u001b[35m924.6    \u001b[39m | \u001b[35m0.0001833\u001b[39m | \u001b[35m7.611e-05\u001b[39m | \u001b[35m5.798    \u001b[39m | \u001b[35m2.226    \u001b[39m | \u001b[35m239.8    \u001b[39m | \u001b[35m10.15    \u001b[39m | \u001b[35m0.2222   \u001b[39m | \u001b[35m0.371    \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=3, hid_dim=2250, act='leakyrelu', dropout=np.float64(0.046197503965171194), use_bn=False, l2=np.float64(0.00025826752493685387), w_initial='he', optim='RMSprop', lr=np.float64(0.003128187752346074), epoch=10, train_batch_size=45, test_batch_size=136, exp_name='exp46')\n",
            "Epoch 0, Acc(train/val): 24.12/20.73, Loss(train/val) 185.41/12.70. Took 8.24 sec\n",
            "Epoch 1, Acc(train/val): 27.83/32.42, Loss(train/val) 4.41/2.38. Took 8.23 sec\n",
            "Epoch 2, Acc(train/val): 24.86/27.95, Loss(train/val) 170.29/4.74. Took 8.40 sec\n",
            "Epoch 3, Acc(train/val): 23.81/23.40, Loss(train/val) 121.82/8.01. Took 7.62 sec\n",
            "Epoch 4, Acc(train/val): 21.77/22.09, Loss(train/val) 20.13/5.62. Took 8.06 sec\n",
            "Epoch 5, Acc(train/val): 26.33/33.20, Loss(train/val) 11.80/2.04. Took 8.10 sec\n",
            "Epoch 6, Acc(train/val): 19.90/24.47, Loss(train/val) 87.25/2.85. Took 8.47 sec\n",
            "Epoch 7, Acc(train/val): 22.99/20.88, Loss(train/val) 36.78/4.31. Took 8.09 sec\n",
            "Epoch 8, Acc(train/val): 22.68/18.13, Loss(train/val) 34.25/5.43. Took 8.28 sec\n",
            "Epoch 9, Acc(train/val): 21.83/18.92, Loss(train/val) 71.65/6.36. Took 7.51 sec\n",
            "| \u001b[39m48       \u001b[39m | \u001b[39m-6.361   \u001b[39m | \u001b[39m0.2932   \u001b[39m | \u001b[39m0.0462   \u001b[39m | \u001b[39m2.25e+03 \u001b[39m | \u001b[39m0.0002583\u001b[39m | \u001b[39m0.003128 \u001b[39m | \u001b[39m3.521    \u001b[39m | \u001b[39m1.955    \u001b[39m | \u001b[39m136.2    \u001b[39m | \u001b[39m45.46    \u001b[39m | \u001b[39m0.7634   \u001b[39m | \u001b[39m0.3588   \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=5, hid_dim=973, act='relu', dropout=np.float64(0.21886305307733323), use_bn=True, l2=np.float64(0.0006014605261906434), w_initial='he', optim='SGD', lr=np.float64(0.0050709300205495494), epoch=10, train_batch_size=120, test_batch_size=221, exp_name='exp47')\n",
            "Epoch 0, Acc(train/val): 29.75/37.79, Loss(train/val) 1.96/1.74. Took 7.00 sec\n",
            "Epoch 1, Acc(train/val): 38.84/41.98, Loss(train/val) 1.72/1.64. Took 6.92 sec\n",
            "Epoch 2, Acc(train/val): 42.34/43.96, Loss(train/val) 1.62/1.58. Took 6.42 sec\n",
            "Epoch 3, Acc(train/val): 44.55/45.29, Loss(train/val) 1.55/1.55. Took 6.90 sec\n",
            "Epoch 4, Acc(train/val): 46.86/46.15, Loss(train/val) 1.50/1.51. Took 6.73 sec\n",
            "Epoch 5, Acc(train/val): 48.56/46.94, Loss(train/val) 1.45/1.49. Took 6.91 sec\n",
            "Epoch 6, Acc(train/val): 49.52/47.72, Loss(train/val) 1.42/1.47. Took 6.74 sec\n",
            "Epoch 7, Acc(train/val): 50.76/48.81, Loss(train/val) 1.38/1.45. Took 7.32 sec\n",
            "Epoch 8, Acc(train/val): 51.73/48.90, Loss(train/val) 1.35/1.45. Took 7.11 sec\n",
            "Epoch 9, Acc(train/val): 53.19/49.29, Loss(train/val) 1.31/1.45. Took 6.87 sec\n",
            "| \u001b[39m49       \u001b[39m | \u001b[39m-1.446   \u001b[39m | \u001b[39m0.2354   \u001b[39m | \u001b[39m0.2189   \u001b[39m | \u001b[39m973.9    \u001b[39m | \u001b[39m0.0006015\u001b[39m | \u001b[39m0.005071 \u001b[39m | \u001b[39m5.764    \u001b[39m | \u001b[39m0.4917   \u001b[39m | \u001b[39m221.7    \u001b[39m | \u001b[39m120.3    \u001b[39m | \u001b[39m0.0318   \u001b[39m | \u001b[39m0.4106   \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=1, hid_dim=2466, act='relu', dropout=np.float64(0.2064403715778429), use_bn=True, l2=np.float64(0.0005321995125490102), w_initial='xavier', optim='SGD', lr=np.float64(0.007123276363184822), epoch=10, train_batch_size=66, test_batch_size=99, exp_name='exp48')\n",
            "Epoch 0, Acc(train/val): 33.61/37.44, Loss(train/val) 1.92/1.79. Took 7.84 sec\n",
            "Epoch 1, Acc(train/val): 41.01/41.07, Loss(train/val) 1.71/1.69. Took 8.36 sec\n",
            "Epoch 2, Acc(train/val): 43.79/42.88, Loss(train/val) 1.63/1.64. Took 7.65 sec\n",
            "Epoch 3, Acc(train/val): 46.01/43.50, Loss(train/val) 1.57/1.60. Took 8.19 sec\n",
            "Epoch 4, Acc(train/val): 47.38/45.87, Loss(train/val) 1.53/1.56. Took 7.96 sec\n",
            "Epoch 5, Acc(train/val): 48.71/46.12, Loss(train/val) 1.49/1.54. Took 8.12 sec\n",
            "Epoch 6, Acc(train/val): 49.92/46.97, Loss(train/val) 1.46/1.53. Took 7.47 sec\n",
            "Epoch 7, Acc(train/val): 51.05/47.69, Loss(train/val) 1.43/1.51. Took 7.77 sec\n",
            "Epoch 8, Acc(train/val): 52.19/47.81, Loss(train/val) 1.40/1.50. Took 7.63 sec\n",
            "Epoch 9, Acc(train/val): 53.06/48.11, Loss(train/val) 1.37/1.49. Took 7.74 sec\n",
            "| \u001b[39m50       \u001b[39m | \u001b[39m-1.495   \u001b[39m | \u001b[39m0.1264   \u001b[39m | \u001b[39m0.2064   \u001b[39m | \u001b[39m2.467e+03\u001b[39m | \u001b[39m0.0005322\u001b[39m | \u001b[39m0.007123 \u001b[39m | \u001b[39m1.711    \u001b[39m | \u001b[39m0.3418   \u001b[39m | \u001b[39m99.27    \u001b[39m | \u001b[39m66.0     \u001b[39m | \u001b[39m0.06249  \u001b[39m | \u001b[39m0.5578   \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=3, hid_dim=2246, act='sigmoid', dropout=np.float64(0.31460718890125283), use_bn=False, l2=np.float64(0.0008852020871806775), w_initial='he', optim='ADAM', lr=np.float64(0.004957837098991406), epoch=10, train_batch_size=43, test_batch_size=140, exp_name='exp49')\n",
            "Epoch 0, Acc(train/val): 10.09/10.41, Loss(train/val) 2.73/2.53. Took 8.56 sec\n",
            "Epoch 1, Acc(train/val): 10.09/9.61, Loss(train/val) 2.78/2.90. Took 8.55 sec\n",
            "Epoch 2, Acc(train/val): 9.88/10.08, Loss(train/val) 2.72/3.03. Took 8.54 sec\n",
            "Epoch 3, Acc(train/val): 10.21/10.08, Loss(train/val) 2.71/2.63. Took 8.73 sec\n",
            "Epoch 4, Acc(train/val): 9.86/10.06, Loss(train/val) 2.74/3.09. Took 8.06 sec\n",
            "Epoch 5, Acc(train/val): 10.24/10.27, Loss(train/val) 2.73/2.55. Took 8.02 sec\n",
            "Epoch 6, Acc(train/val): 9.96/9.61, Loss(train/val) 2.71/2.75. Took 8.73 sec\n",
            "Epoch 7, Acc(train/val): 10.08/9.12, Loss(train/val) 2.75/2.82. Took 8.11 sec\n",
            "Epoch 8, Acc(train/val): 9.94/9.61, Loss(train/val) 2.76/2.49. Took 8.31 sec\n",
            "Epoch 9, Acc(train/val): 9.89/10.08, Loss(train/val) 2.70/2.71. Took 9.14 sec\n",
            "| \u001b[39m51       \u001b[39m | \u001b[39m-2.706   \u001b[39m | \u001b[39m0.9221   \u001b[39m | \u001b[39m0.3146   \u001b[39m | \u001b[39m2.246e+03\u001b[39m | \u001b[39m0.0008852\u001b[39m | \u001b[39m0.004958 \u001b[39m | \u001b[39m3.372    \u001b[39m | \u001b[39m2.084    \u001b[39m | \u001b[39m140.6    \u001b[39m | \u001b[39m43.52    \u001b[39m | \u001b[39m0.5409   \u001b[39m | \u001b[39m0.377    \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=3, hid_dim=2463, act='relu', dropout=np.float64(0.04138478060831435), use_bn=True, l2=np.float64(0.0005899201621022556), w_initial='he', optim='RMSprop', lr=np.float64(0.00724182744545095), epoch=10, train_batch_size=65, test_batch_size=101, exp_name='exp50')\n",
            "Epoch 0, Acc(train/val): 21.30/10.21, Loss(train/val) 4.10/25.89. Took 7.41 sec\n",
            "Epoch 1, Acc(train/val): 27.11/26.61, Loss(train/val) 1.98/2.00. Took 7.38 sec\n",
            "Epoch 2, Acc(train/val): 29.78/27.13, Loss(train/val) 1.91/2.02. Took 7.34 sec\n",
            "Epoch 3, Acc(train/val): 29.52/23.63, Loss(train/val) 1.91/2.10. Took 7.34 sec\n",
            "Epoch 4, Acc(train/val): 31.05/22.17, Loss(train/val) 1.88/2.28. Took 6.39 sec\n",
            "Epoch 5, Acc(train/val): 32.07/26.03, Loss(train/val) 1.86/2.04. Took 7.19 sec\n",
            "Epoch 6, Acc(train/val): 32.63/25.27, Loss(train/val) 1.84/2.19. Took 7.02 sec\n",
            "Epoch 7, Acc(train/val): 32.61/21.56, Loss(train/val) 1.84/2.07. Took 7.42 sec\n",
            "Epoch 8, Acc(train/val): 31.51/24.59, Loss(train/val) 1.85/2.15. Took 6.91 sec\n",
            "Epoch 9, Acc(train/val): 32.54/30.81, Loss(train/val) 1.84/1.96. Took 7.82 sec\n",
            "| \u001b[39m52       \u001b[39m | \u001b[39m-1.958   \u001b[39m | \u001b[39m0.186    \u001b[39m | \u001b[39m0.04138  \u001b[39m | \u001b[39m2.464e+03\u001b[39m | \u001b[39m0.0005899\u001b[39m | \u001b[39m0.007242 \u001b[39m | \u001b[39m3.726    \u001b[39m | \u001b[39m1.71     \u001b[39m | \u001b[39m101.7    \u001b[39m | \u001b[39m65.01    \u001b[39m | \u001b[39m0.43     \u001b[39m | \u001b[39m0.2735   \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=5, hid_dim=2374, act='relu', dropout=np.float64(0.016987721428459557), use_bn=True, l2=np.float64(0.00036223251024715706), w_initial='he', optim='RMSprop', lr=np.float64(0.004276740759217237), epoch=10, train_batch_size=180, test_batch_size=83, exp_name='exp51')\n",
            "Epoch 0, Acc(train/val): 19.71/17.53, Loss(train/val) 4.86/3.57. Took 7.13 sec\n",
            "Epoch 1, Acc(train/val): 22.87/13.37, Loss(train/val) 2.70/472.69. Took 7.20 sec\n",
            "Epoch 2, Acc(train/val): 25.88/11.99, Loss(train/val) 2.12/67.17. Took 7.45 sec\n",
            "Epoch 3, Acc(train/val): 29.05/10.06, Loss(train/val) 1.97/140.57. Took 7.29 sec\n",
            "Epoch 4, Acc(train/val): 33.43/26.13, Loss(train/val) 1.84/2.07. Took 7.05 sec\n",
            "Epoch 5, Acc(train/val): 36.73/31.64, Loss(train/val) 1.76/1.91. Took 7.20 sec\n",
            "Epoch 6, Acc(train/val): 38.77/33.09, Loss(train/val) 1.69/1.93. Took 7.23 sec\n",
            "Epoch 7, Acc(train/val): 40.33/22.16, Loss(train/val) 1.65/4.73. Took 7.12 sec\n",
            "Epoch 8, Acc(train/val): 41.21/33.96, Loss(train/val) 1.63/1.85. Took 7.13 sec\n",
            "Epoch 9, Acc(train/val): 42.13/29.17, Loss(train/val) 1.60/2.00. Took 7.15 sec\n",
            "| \u001b[39m53       \u001b[39m | \u001b[39m-2.0     \u001b[39m | \u001b[39m0.1188   \u001b[39m | \u001b[39m0.01699  \u001b[39m | \u001b[39m2.374e+03\u001b[39m | \u001b[39m0.0003622\u001b[39m | \u001b[39m0.004277 \u001b[39m | \u001b[39m5.231    \u001b[39m | \u001b[39m1.215    \u001b[39m | \u001b[39m83.5     \u001b[39m | \u001b[39m180.7    \u001b[39m | \u001b[39m0.04119  \u001b[39m | \u001b[39m0.1682   \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=3, hid_dim=2233, act='sigmoid', dropout=np.float64(0.3119518082542955), use_bn=True, l2=np.float64(0.0008025600021001461), w_initial='xavier', optim='SGD', lr=np.float64(0.008650977765094827), epoch=10, train_batch_size=53, test_batch_size=194, exp_name='exp52')\n",
            "Epoch 0, Acc(train/val): 33.27/35.88, Loss(train/val) 1.93/1.84. Took 8.24 sec\n",
            "Epoch 1, Acc(train/val): 36.36/37.62, Loss(train/val) 1.83/1.80. Took 7.65 sec\n",
            "Epoch 2, Acc(train/val): 38.02/38.90, Loss(train/val) 1.79/1.78. Took 7.44 sec\n",
            "Epoch 3, Acc(train/val): 38.96/39.82, Loss(train/val) 1.77/1.76. Took 7.37 sec\n",
            "Epoch 4, Acc(train/val): 39.55/38.84, Loss(train/val) 1.75/1.77. Took 6.80 sec\n",
            "Epoch 5, Acc(train/val): 40.01/41.28, Loss(train/val) 1.73/1.71. Took 7.03 sec\n",
            "Epoch 6, Acc(train/val): 41.06/38.82, Loss(train/val) 1.71/1.77. Took 8.03 sec\n",
            "Epoch 7, Acc(train/val): 41.38/41.07, Loss(train/val) 1.70/1.72. Took 7.33 sec\n",
            "Epoch 8, Acc(train/val): 41.84/41.10, Loss(train/val) 1.68/1.70. Took 7.22 sec\n",
            "Epoch 9, Acc(train/val): 42.52/42.78, Loss(train/val) 1.66/1.68. Took 7.03 sec\n",
            "| \u001b[39m54       \u001b[39m | \u001b[39m-1.681   \u001b[39m | \u001b[39m0.8564   \u001b[39m | \u001b[39m0.312    \u001b[39m | \u001b[39m2.233e+03\u001b[39m | \u001b[39m0.0008026\u001b[39m | \u001b[39m0.008651 \u001b[39m | \u001b[39m3.928    \u001b[39m | \u001b[39m0.2404   \u001b[39m | \u001b[39m194.8    \u001b[39m | \u001b[39m53.69    \u001b[39m | \u001b[39m0.0464   \u001b[39m | \u001b[39m0.7892   \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=3, hid_dim=1004, act='leakyrelu', dropout=np.float64(0.4982944443533789), use_bn=True, l2=np.float64(0.0006772885880189224), w_initial='xavier', optim='RMSprop', lr=np.float64(0.0003571574456621472), epoch=10, train_batch_size=33, test_batch_size=159, exp_name='exp53')\n",
            "Epoch 0, Acc(train/val): 35.45/38.31, Loss(train/val) 1.83/1.74. Took 9.39 sec\n",
            "Epoch 1, Acc(train/val): 43.25/42.89, Loss(train/val) 1.59/1.61. Took 8.70 sec\n",
            "Epoch 2, Acc(train/val): 46.55/46.42, Loss(train/val) 1.50/1.51. Took 9.69 sec\n",
            "Epoch 3, Acc(train/val): 48.26/46.40, Loss(train/val) 1.44/1.50. Took 8.86 sec\n",
            "Epoch 4, Acc(train/val): 50.20/41.96, Loss(train/val) 1.40/1.67. Took 9.89 sec\n",
            "Epoch 5, Acc(train/val): 51.24/45.40, Loss(train/val) 1.36/1.57. Took 8.87 sec\n",
            "Epoch 6, Acc(train/val): 52.19/49.50, Loss(train/val) 1.33/1.46. Took 9.64 sec\n",
            "Epoch 7, Acc(train/val): 53.55/46.34, Loss(train/val) 1.30/1.51. Took 9.27 sec\n",
            "Epoch 8, Acc(train/val): 54.26/49.00, Loss(train/val) 1.28/1.47. Took 9.30 sec\n",
            "Epoch 9, Acc(train/val): 55.39/45.67, Loss(train/val) 1.26/1.64. Took 8.68 sec\n",
            "| \u001b[39m55       \u001b[39m | \u001b[39m-1.642   \u001b[39m | \u001b[39m0.3936   \u001b[39m | \u001b[39m0.4983   \u001b[39m | \u001b[39m1.005e+03\u001b[39m | \u001b[39m0.0006773\u001b[39m | \u001b[39m0.0003572\u001b[39m | \u001b[39m3.359    \u001b[39m | \u001b[39m1.901    \u001b[39m | \u001b[39m159.1    \u001b[39m | \u001b[39m33.34    \u001b[39m | \u001b[39m0.2334   \u001b[39m | \u001b[39m0.8808   \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=4, hid_dim=408, act='leakyrelu', dropout=np.float64(0.42275293185647383), use_bn=False, l2=np.float64(0.0005343679267952835), w_initial='he', optim='ADAM', lr=np.float64(0.0017859763251800564), epoch=10, train_batch_size=34, test_batch_size=43, exp_name='exp54')\n",
            "Epoch 0, Acc(train/val): 27.24/36.17, Loss(train/val) 2.03/1.80. Took 8.64 sec\n",
            "Epoch 1, Acc(train/val): 31.27/31.89, Loss(train/val) 1.92/1.84. Took 9.10 sec\n",
            "Epoch 2, Acc(train/val): 31.89/35.31, Loss(train/val) 1.89/1.81. Took 8.74 sec\n",
            "Epoch 3, Acc(train/val): 32.87/36.29, Loss(train/val) 1.87/1.80. Took 10.17 sec\n",
            "Epoch 4, Acc(train/val): 33.06/35.37, Loss(train/val) 1.86/1.79. Took 8.77 sec\n",
            "Epoch 5, Acc(train/val): 33.64/35.67, Loss(train/val) 1.84/1.78. Took 8.54 sec\n",
            "Epoch 6, Acc(train/val): 34.25/37.91, Loss(train/val) 1.83/1.76. Took 9.75 sec\n",
            "Epoch 7, Acc(train/val): 34.77/37.60, Loss(train/val) 1.82/1.75. Took 8.16 sec\n",
            "Epoch 8, Acc(train/val): 34.94/37.36, Loss(train/val) 1.81/1.77. Took 8.40 sec\n",
            "Epoch 9, Acc(train/val): 35.53/36.29, Loss(train/val) 1.80/1.77. Took 9.60 sec\n",
            "| \u001b[39m56       \u001b[39m | \u001b[39m-1.773   \u001b[39m | \u001b[39m0.4113   \u001b[39m | \u001b[39m0.4228   \u001b[39m | \u001b[39m408.5    \u001b[39m | \u001b[39m0.0005344\u001b[39m | \u001b[39m0.001786 \u001b[39m | \u001b[39m4.96     \u001b[39m | \u001b[39m2.923    \u001b[39m | \u001b[39m43.27    \u001b[39m | \u001b[39m34.58    \u001b[39m | \u001b[39m0.9538   \u001b[39m | \u001b[39m0.4423   \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=4, hid_dim=556, act='leakyrelu', dropout=np.float64(0.45400178152246123), use_bn=True, l2=np.float64(0.00032983436809717724), w_initial='he', optim='ADAM', lr=np.float64(0.007921479844793862), epoch=10, train_batch_size=214, test_batch_size=217, exp_name='exp55')\n",
            "Epoch 0, Acc(train/val): 28.23/35.61, Loss(train/val) 2.03/1.76. Took 6.63 sec\n",
            "Epoch 1, Acc(train/val): 37.83/39.42, Loss(train/val) 1.72/1.71. Took 6.79 sec\n",
            "Epoch 2, Acc(train/val): 41.03/40.46, Loss(train/val) 1.65/1.64. Took 6.31 sec\n",
            "Epoch 3, Acc(train/val): 41.47/41.81, Loss(train/val) 1.63/1.62. Took 5.76 sec\n",
            "Epoch 4, Acc(train/val): 42.27/41.67, Loss(train/val) 1.62/1.64. Took 7.15 sec\n",
            "Epoch 5, Acc(train/val): 42.19/41.30, Loss(train/val) 1.62/1.63. Took 7.37 sec\n",
            "Epoch 6, Acc(train/val): 42.41/41.37, Loss(train/val) 1.62/1.63. Took 7.47 sec\n",
            "Epoch 7, Acc(train/val): 42.16/41.75, Loss(train/val) 1.61/1.61. Took 6.75 sec\n",
            "Epoch 8, Acc(train/val): 42.54/41.10, Loss(train/val) 1.60/1.66. Took 6.79 sec\n",
            "Epoch 9, Acc(train/val): 42.33/42.50, Loss(train/val) 1.61/1.59. Took 7.19 sec\n",
            "| \u001b[39m57       \u001b[39m | \u001b[39m-1.588   \u001b[39m | \u001b[39m0.3231   \u001b[39m | \u001b[39m0.454    \u001b[39m | \u001b[39m556.8    \u001b[39m | \u001b[39m0.0003298\u001b[39m | \u001b[39m0.007921 \u001b[39m | \u001b[39m4.302    \u001b[39m | \u001b[39m2.48     \u001b[39m | \u001b[39m217.6    \u001b[39m | \u001b[39m214.4    \u001b[39m | \u001b[39m0.4967   \u001b[39m | \u001b[39m0.1684   \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=4, hid_dim=271, act='leakyrelu', dropout=np.float64(0.2520146694532102), use_bn=True, l2=np.float64(0.0006543515677114031), w_initial='xavier', optim='ADAM', lr=np.float64(0.0086709364982336), epoch=10, train_batch_size=135, test_batch_size=140, exp_name='exp56')\n",
            "Epoch 0, Acc(train/val): 31.89/36.30, Loss(train/val) 1.89/1.77. Took 6.85 sec\n",
            "Epoch 1, Acc(train/val): 37.87/37.17, Loss(train/val) 1.73/1.77. Took 6.60 sec\n",
            "Epoch 2, Acc(train/val): 38.08/37.12, Loss(train/val) 1.72/1.72. Took 6.98 sec\n",
            "Epoch 3, Acc(train/val): 38.40/36.93, Loss(train/val) 1.71/1.76. Took 7.81 sec\n",
            "Epoch 4, Acc(train/val): 38.42/35.97, Loss(train/val) 1.72/1.74. Took 7.45 sec\n",
            "Epoch 5, Acc(train/val): 38.51/37.73, Loss(train/val) 1.71/1.74. Took 6.83 sec\n",
            "Epoch 6, Acc(train/val): 38.89/39.03, Loss(train/val) 1.70/1.71. Took 6.69 sec\n",
            "Epoch 7, Acc(train/val): 38.23/37.61, Loss(train/val) 1.71/1.74. Took 7.45 sec\n",
            "Epoch 8, Acc(train/val): 37.91/37.24, Loss(train/val) 1.72/1.72. Took 6.89 sec\n",
            "Epoch 9, Acc(train/val): 38.30/35.74, Loss(train/val) 1.71/1.80. Took 7.59 sec\n",
            "| \u001b[39m58       \u001b[39m | \u001b[39m-1.801   \u001b[39m | \u001b[39m0.2908   \u001b[39m | \u001b[39m0.252    \u001b[39m | \u001b[39m271.0    \u001b[39m | \u001b[39m0.0006544\u001b[39m | \u001b[39m0.008671 \u001b[39m | \u001b[39m4.091    \u001b[39m | \u001b[39m2.595    \u001b[39m | \u001b[39m140.5    \u001b[39m | \u001b[39m135.3    \u001b[39m | \u001b[39m0.007256 \u001b[39m | \u001b[39m0.8768   \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=2, hid_dim=1011, act='relu', dropout=np.float64(0.19990845930887913), use_bn=False, l2=np.float64(0.0007392886685616315), w_initial='he', optim='SGD', lr=np.float64(0.009184792636454168), epoch=10, train_batch_size=27, test_batch_size=151, exp_name='exp57')\n",
            "Epoch 0, Acc(train/val): 37.91/42.60, Loss(train/val) 1.77/1.63. Took 9.32 sec\n",
            "Epoch 1, Acc(train/val): 46.16/46.09, Loss(train/val) 1.53/1.53. Took 9.90 sec\n",
            "Epoch 2, Acc(train/val): 50.08/48.05, Loss(train/val) 1.43/1.49. Took 10.50 sec\n",
            "Epoch 3, Acc(train/val): 52.75/50.46, Loss(train/val) 1.35/1.45. Took 9.33 sec\n",
            "Epoch 4, Acc(train/val): 55.55/50.95, Loss(train/val) 1.28/1.42. Took 11.53 sec\n",
            "Epoch 5, Acc(train/val): 57.76/50.35, Loss(train/val) 1.22/1.44. Took 10.89 sec\n",
            "Epoch 6, Acc(train/val): 59.49/52.72, Loss(train/val) 1.16/1.37. Took 10.48 sec\n",
            "Epoch 7, Acc(train/val): 61.46/52.02, Loss(train/val) 1.11/1.39. Took 9.37 sec\n",
            "Epoch 8, Acc(train/val): 63.42/52.03, Loss(train/val) 1.06/1.39. Took 9.90 sec\n",
            "Epoch 9, Acc(train/val): 65.08/51.24, Loss(train/val) 1.01/1.46. Took 10.83 sec\n",
            "| \u001b[39m59       \u001b[39m | \u001b[39m-1.456   \u001b[39m | \u001b[39m0.2224   \u001b[39m | \u001b[39m0.1999   \u001b[39m | \u001b[39m1.011e+03\u001b[39m | \u001b[39m0.0007393\u001b[39m | \u001b[39m0.009185 \u001b[39m | \u001b[39m2.069    \u001b[39m | \u001b[39m0.3725   \u001b[39m | \u001b[39m151.3    \u001b[39m | \u001b[39m27.26    \u001b[39m | \u001b[39m0.7931   \u001b[39m | \u001b[39m0.4428   \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=1, hid_dim=951, act='sigmoid', dropout=np.float64(0.325167674967986), use_bn=False, l2=np.float64(0.00011021183738117088), w_initial='xavier', optim='RMSprop', lr=np.float64(0.0005773674025220547), epoch=10, train_batch_size=26, test_batch_size=132, exp_name='exp58')\n",
            "Epoch 0, Acc(train/val): 38.31/40.36, Loss(train/val) 1.76/1.72. Took 9.25 sec\n",
            "Epoch 1, Acc(train/val): 44.20/37.77, Loss(train/val) 1.61/1.79. Took 9.57 sec\n",
            "Epoch 2, Acc(train/val): 46.82/43.73, Loss(train/val) 1.53/1.61. Took 9.34 sec\n",
            "Epoch 3, Acc(train/val): 49.10/42.68, Loss(train/val) 1.47/1.63. Took 8.57 sec\n",
            "Epoch 4, Acc(train/val): 51.30/45.92, Loss(train/val) 1.41/1.57. Took 8.86 sec\n",
            "Epoch 5, Acc(train/val): 53.45/45.08, Loss(train/val) 1.35/1.61. Took 9.92 sec\n",
            "Epoch 6, Acc(train/val): 55.00/46.40, Loss(train/val) 1.30/1.56. Took 10.15 sec\n",
            "Epoch 7, Acc(train/val): 57.11/46.73, Loss(train/val) 1.25/1.56. Took 9.42 sec\n",
            "Epoch 8, Acc(train/val): 58.55/44.73, Loss(train/val) 1.20/1.65. Took 9.26 sec\n",
            "Epoch 9, Acc(train/val): 60.47/47.64, Loss(train/val) 1.16/1.55. Took 10.39 sec\n",
            "| \u001b[39m60       \u001b[39m | \u001b[39m-1.553   \u001b[39m | \u001b[39m0.9243   \u001b[39m | \u001b[39m0.3252   \u001b[39m | \u001b[39m951.3    \u001b[39m | \u001b[39m0.0001102\u001b[39m | \u001b[39m0.0005774\u001b[39m | \u001b[39m1.003    \u001b[39m | \u001b[39m1.333    \u001b[39m | \u001b[39m132.9    \u001b[39m | \u001b[39m26.19    \u001b[39m | \u001b[39m0.5273   \u001b[39m | \u001b[39m0.9008   \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=5, hid_dim=1316, act='sigmoid', dropout=np.float64(0.02123678381247529), use_bn=False, l2=np.float64(0.0004311698278315433), w_initial='xavier', optim='SGD', lr=np.float64(0.0012263428253542485), epoch=10, train_batch_size=215, test_batch_size=93, exp_name='exp59')\n",
            "Epoch 0, Acc(train/val): 9.75/10.34, Loss(train/val) 2.31/2.30. Took 7.03 sec\n",
            "Epoch 1, Acc(train/val): 9.94/10.28, Loss(train/val) 2.30/2.30. Took 6.90 sec\n",
            "Epoch 2, Acc(train/val): 10.06/9.12, Loss(train/val) 2.30/2.30. Took 6.42 sec\n",
            "Epoch 3, Acc(train/val): 9.84/10.06, Loss(train/val) 2.30/2.30. Took 7.21 sec\n",
            "Epoch 4, Acc(train/val): 10.07/10.11, Loss(train/val) 2.30/2.30. Took 6.81 sec\n",
            "Epoch 5, Acc(train/val): 10.00/10.11, Loss(train/val) 2.30/2.30. Took 6.60 sec\n",
            "Epoch 6, Acc(train/val): 10.25/9.61, Loss(train/val) 2.30/2.30. Took 7.17 sec\n",
            "Epoch 7, Acc(train/val): 9.91/10.34, Loss(train/val) 2.30/2.30. Took 6.60 sec\n",
            "Epoch 8, Acc(train/val): 10.22/9.88, Loss(train/val) 2.30/2.30. Took 6.79 sec\n",
            "Epoch 9, Acc(train/val): 9.91/10.11, Loss(train/val) 2.30/2.30. Took 7.11 sec\n",
            "| \u001b[39m61       \u001b[39m | \u001b[39m-2.304   \u001b[39m | \u001b[39m0.9037   \u001b[39m | \u001b[39m0.02124  \u001b[39m | \u001b[39m1.316e+03\u001b[39m | \u001b[39m0.0004312\u001b[39m | \u001b[39m0.001226 \u001b[39m | \u001b[39m5.97     \u001b[39m | \u001b[39m0.1574   \u001b[39m | \u001b[39m93.17    \u001b[39m | \u001b[39m215.3    \u001b[39m | \u001b[39m0.9034   \u001b[39m | \u001b[39m0.7608   \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=4, hid_dim=2923, act='sigmoid', dropout=np.float64(0.023038505617227145), use_bn=False, l2=np.float64(0.0006153627227299175), w_initial='he', optim='RMSprop', lr=np.float64(0.005830059689095484), epoch=10, train_batch_size=217, test_batch_size=22, exp_name='exp60')\n",
            "Epoch 0, Acc(train/val): 9.95/10.34, Loss(train/val) 13.62/8.69. Took 7.93 sec\n",
            "Epoch 1, Acc(train/val): 9.57/9.61, Loss(train/val) 8.57/7.48. Took 7.28 sec\n",
            "Epoch 2, Acc(train/val): 9.92/10.34, Loss(train/val) 7.70/8.36. Took 7.35 sec\n",
            "Epoch 3, Acc(train/val): 10.14/10.34, Loss(train/val) 7.09/6.58. Took 7.30 sec\n",
            "Epoch 4, Acc(train/val): 9.81/10.28, Loss(train/val) 6.01/5.31. Took 7.10 sec\n",
            "Epoch 5, Acc(train/val): 9.82/9.12, Loss(train/val) 4.75/5.43. Took 7.09 sec\n",
            "Epoch 6, Acc(train/val): 9.98/9.12, Loss(train/val) 5.59/7.63. Took 7.56 sec\n",
            "Epoch 7, Acc(train/val): 9.99/10.28, Loss(train/val) 3.89/3.36. Took 7.05 sec\n",
            "Epoch 8, Acc(train/val): 10.06/10.11, Loss(train/val) 4.75/3.18. Took 7.71 sec\n",
            "Epoch 9, Acc(train/val): 10.09/9.88, Loss(train/val) 3.59/2.80. Took 7.09 sec\n",
            "| \u001b[39m62       \u001b[39m | \u001b[39m-2.802   \u001b[39m | \u001b[39m0.9988   \u001b[39m | \u001b[39m0.02304  \u001b[39m | \u001b[39m2.924e+03\u001b[39m | \u001b[39m0.0006154\u001b[39m | \u001b[39m0.00583  \u001b[39m | \u001b[39m4.28     \u001b[39m | \u001b[39m1.351    \u001b[39m | \u001b[39m22.71    \u001b[39m | \u001b[39m217.7    \u001b[39m | \u001b[39m0.7111   \u001b[39m | \u001b[39m0.2792   \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=5, hid_dim=2241, act='tanh', dropout=np.float64(0.2621203805753638), use_bn=True, l2=np.float64(0.0008287280215196882), w_initial='he', optim='ADAM', lr=np.float64(0.0016426626297616116), epoch=10, train_batch_size=40, test_batch_size=142, exp_name='exp61')\n",
            "Epoch 0, Acc(train/val): 18.77/23.92, Loss(train/val) 2.31/2.10. Took 11.19 sec\n",
            "Epoch 1, Acc(train/val): 23.52/24.72, Loss(train/val) 2.09/2.04. Took 11.48 sec\n",
            "Epoch 2, Acc(train/val): 26.54/28.16, Loss(train/val) 1.99/1.96. Took 11.70 sec\n",
            "Epoch 3, Acc(train/val): 28.44/30.30, Loss(train/val) 1.97/1.93. Took 11.54 sec\n",
            "Epoch 4, Acc(train/val): 28.39/29.34, Loss(train/val) 1.97/1.96. Took 11.01 sec\n",
            "Epoch 5, Acc(train/val): 28.68/30.94, Loss(train/val) 1.96/1.94. Took 10.83 sec\n",
            "Epoch 6, Acc(train/val): 28.73/27.75, Loss(train/val) 1.96/1.97. Took 11.24 sec\n",
            "Epoch 7, Acc(train/val): 29.00/30.12, Loss(train/val) 1.96/1.94. Took 10.84 sec\n",
            "Epoch 8, Acc(train/val): 28.88/30.80, Loss(train/val) 1.95/1.92. Took 11.01 sec\n",
            "Epoch 9, Acc(train/val): 28.84/29.35, Loss(train/val) 1.95/1.93. Took 11.31 sec\n",
            "| \u001b[39m63       \u001b[39m | \u001b[39m-1.932   \u001b[39m | \u001b[39m0.5718   \u001b[39m | \u001b[39m0.2621   \u001b[39m | \u001b[39m2.241e+03\u001b[39m | \u001b[39m0.0008287\u001b[39m | \u001b[39m0.001643 \u001b[39m | \u001b[39m5.852    \u001b[39m | \u001b[39m2.46     \u001b[39m | \u001b[39m142.3    \u001b[39m | \u001b[39m40.38    \u001b[39m | \u001b[39m0.01605  \u001b[39m | \u001b[39m0.00491  \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=1, hid_dim=2191, act='leakyrelu', dropout=np.float64(0.19199151192189795), use_bn=False, l2=np.float64(1.4760141883831203e-05), w_initial='xavier', optim='RMSprop', lr=np.float64(0.0031304625814710715), epoch=10, train_batch_size=38, test_batch_size=160, exp_name='exp62')\n",
            "Epoch 0, Acc(train/val): 30.52/31.02, Loss(train/val) 7.19/3.96. Took 8.55 sec\n",
            "Epoch 1, Acc(train/val): 35.56/30.44, Loss(train/val) 4.07/6.23. Took 9.02 sec\n",
            "Epoch 2, Acc(train/val): 37.56/29.39, Loss(train/val) 3.78/5.59. Took 8.49 sec\n",
            "Epoch 3, Acc(train/val): 39.66/37.28, Loss(train/val) 3.50/3.61. Took 8.96 sec\n",
            "Epoch 4, Acc(train/val): 40.77/32.20, Loss(train/val) 3.36/5.08. Took 7.85 sec\n",
            "Epoch 5, Acc(train/val): 41.01/38.90, Loss(train/val) 3.34/3.67. Took 9.02 sec\n",
            "Epoch 6, Acc(train/val): 42.41/37.50, Loss(train/val) 3.21/4.20. Took 7.97 sec\n",
            "Epoch 7, Acc(train/val): 42.84/39.21, Loss(train/val) 3.19/3.93. Took 7.95 sec\n",
            "Epoch 8, Acc(train/val): 43.77/35.48, Loss(train/val) 3.12/6.12. Took 8.17 sec\n",
            "Epoch 9, Acc(train/val): 44.00/34.21, Loss(train/val) 3.10/6.45. Took 7.70 sec\n",
            "| \u001b[39m64       \u001b[39m | \u001b[39m-6.448   \u001b[39m | \u001b[39m0.3846   \u001b[39m | \u001b[39m0.192    \u001b[39m | \u001b[39m2.191e+03\u001b[39m | \u001b[39m1.476e-05\u001b[39m | \u001b[39m0.00313  \u001b[39m | \u001b[39m1.276    \u001b[39m | \u001b[39m1.857    \u001b[39m | \u001b[39m160.7    \u001b[39m | \u001b[39m38.72    \u001b[39m | \u001b[39m0.712    \u001b[39m | \u001b[39m0.7051   \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=5, hid_dim=2375, act='tanh', dropout=np.float64(0.3347084540642088), use_bn=True, l2=np.float64(0.0004885798512976182), w_initial='he', optim='ADAM', lr=np.float64(0.007313333960560677), epoch=10, train_batch_size=176, test_batch_size=86, exp_name='exp63')\n",
            "Epoch 0, Acc(train/val): 16.25/19.58, Loss(train/val) 2.77/2.50. Took 7.59 sec\n",
            "Epoch 1, Acc(train/val): 21.34/24.70, Loss(train/val) 2.25/2.15. Took 7.26 sec\n",
            "Epoch 2, Acc(train/val): 25.31/26.68, Loss(train/val) 2.08/2.06. Took 7.42 sec\n",
            "Epoch 3, Acc(train/val): 28.54/29.67, Loss(train/val) 1.95/1.94. Took 6.98 sec\n",
            "Epoch 4, Acc(train/val): 29.78/26.58, Loss(train/val) 1.93/2.01. Took 7.11 sec\n",
            "Epoch 5, Acc(train/val): 29.31/25.11, Loss(train/val) 1.94/1.99. Took 6.87 sec\n",
            "Epoch 6, Acc(train/val): 29.07/27.13, Loss(train/val) 1.95/2.07. Took 6.88 sec\n",
            "Epoch 7, Acc(train/val): 28.62/26.14, Loss(train/val) 1.96/2.03. Took 7.08 sec\n",
            "Epoch 8, Acc(train/val): 28.12/29.74, Loss(train/val) 1.97/1.93. Took 6.58 sec\n",
            "Epoch 9, Acc(train/val): 28.10/27.76, Loss(train/val) 1.97/1.96. Took 6.76 sec\n",
            "| \u001b[39m65       \u001b[39m | \u001b[39m-1.964   \u001b[39m | \u001b[39m0.7362   \u001b[39m | \u001b[39m0.3347   \u001b[39m | \u001b[39m2.376e+03\u001b[39m | \u001b[39m0.0004886\u001b[39m | \u001b[39m0.007313 \u001b[39m | \u001b[39m5.72     \u001b[39m | \u001b[39m2.379    \u001b[39m | \u001b[39m86.48    \u001b[39m | \u001b[39m176.2    \u001b[39m | \u001b[39m0.09049  \u001b[39m | \u001b[39m0.2181   \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=4, hid_dim=967, act='sigmoid', dropout=np.float64(0.25401238299537887), use_bn=False, l2=np.float64(0.0005151742608734659), w_initial='he', optim='RMSprop', lr=np.float64(0.0032988022373249748), epoch=10, train_batch_size=119, test_batch_size=222, exp_name='exp64')\n",
            "Epoch 0, Acc(train/val): 9.90/9.88, Loss(train/val) 2.61/2.68. Took 7.04 sec\n",
            "Epoch 1, Acc(train/val): 10.07/9.12, Loss(train/val) 2.34/2.35. Took 7.31 sec\n",
            "Epoch 2, Acc(train/val): 9.96/9.88, Loss(train/val) 2.33/2.58. Took 6.61 sec\n",
            "Epoch 3, Acc(train/val): 10.15/10.08, Loss(train/val) 2.33/2.55. Took 7.21 sec\n",
            "Epoch 4, Acc(train/val): 10.10/10.06, Loss(train/val) 2.34/2.37. Took 7.51 sec\n",
            "Epoch 5, Acc(train/val): 9.82/9.88, Loss(train/val) 2.33/2.51. Took 6.57 sec\n",
            "Epoch 6, Acc(train/val): 10.11/10.06, Loss(train/val) 2.34/2.42. Took 6.60 sec\n",
            "Epoch 7, Acc(train/val): 9.93/10.06, Loss(train/val) 2.34/3.78. Took 7.10 sec\n",
            "Epoch 8, Acc(train/val): 9.98/10.28, Loss(train/val) 2.34/2.57. Took 7.23 sec\n",
            "Epoch 9, Acc(train/val): 10.11/10.06, Loss(train/val) 2.34/2.53. Took 6.71 sec\n",
            "| \u001b[39m66       \u001b[39m | \u001b[39m-2.53    \u001b[39m | \u001b[39m0.8585   \u001b[39m | \u001b[39m0.254    \u001b[39m | \u001b[39m967.8    \u001b[39m | \u001b[39m0.0005152\u001b[39m | \u001b[39m0.003299 \u001b[39m | \u001b[39m4.976    \u001b[39m | \u001b[39m1.221    \u001b[39m | \u001b[39m222.4    \u001b[39m | \u001b[39m119.9    \u001b[39m | \u001b[39m0.5186   \u001b[39m | \u001b[39m0.07764  \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=3, hid_dim=1324, act='tanh', dropout=np.float64(0.22601975335215757), use_bn=True, l2=np.float64(0.0003554903621051208), w_initial='xavier', optim='ADAM', lr=np.float64(0.007102504491542446), epoch=10, train_batch_size=217, test_batch_size=99, exp_name='exp65')\n",
            "Epoch 0, Acc(train/val): 21.92/28.74, Loss(train/val) 2.42/2.06. Took 7.40 sec\n",
            "Epoch 1, Acc(train/val): 29.57/32.93, Loss(train/val) 1.98/1.86. Took 7.20 sec\n",
            "Epoch 2, Acc(train/val): 32.83/33.60, Loss(train/val) 1.86/1.81. Took 6.93 sec\n",
            "Epoch 3, Acc(train/val): 34.54/32.79, Loss(train/val) 1.81/1.83. Took 7.30 sec\n",
            "Epoch 4, Acc(train/val): 34.73/32.86, Loss(train/val) 1.81/1.83. Took 6.73 sec\n",
            "Epoch 5, Acc(train/val): 34.83/33.09, Loss(train/val) 1.81/1.87. Took 6.78 sec\n",
            "Epoch 6, Acc(train/val): 34.48/33.67, Loss(train/val) 1.82/1.82. Took 7.23 sec\n",
            "Epoch 7, Acc(train/val): 34.02/32.44, Loss(train/val) 1.82/1.85. Took 6.86 sec\n",
            "Epoch 8, Acc(train/val): 34.00/31.36, Loss(train/val) 1.84/1.87. Took 6.94 sec\n",
            "Epoch 9, Acc(train/val): 33.98/33.48, Loss(train/val) 1.84/1.86. Took 7.29 sec\n",
            "| \u001b[39m67       \u001b[39m | \u001b[39m-1.856   \u001b[39m | \u001b[39m0.7085   \u001b[39m | \u001b[39m0.226    \u001b[39m | \u001b[39m1.325e+03\u001b[39m | \u001b[39m0.0003555\u001b[39m | \u001b[39m0.007103 \u001b[39m | \u001b[39m3.28     \u001b[39m | \u001b[39m2.878    \u001b[39m | \u001b[39m99.03    \u001b[39m | \u001b[39m217.3    \u001b[39m | \u001b[39m0.002636 \u001b[39m | \u001b[39m0.8947   \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=4, hid_dim=1313, act='relu', dropout=np.float64(0.0828942770100905), use_bn=False, l2=np.float64(0.0005562597260294501), w_initial='xavier', optim='SGD', lr=np.float64(0.002364956469385161), epoch=10, train_batch_size=210, test_batch_size=91, exp_name='exp66')\n",
            "Epoch 0, Acc(train/val): 14.25/21.21, Loss(train/val) 2.29/2.28. Took 7.20 sec\n",
            "Epoch 1, Acc(train/val): 21.61/24.22, Loss(train/val) 2.27/2.25. Took 7.08 sec\n",
            "Epoch 2, Acc(train/val): 23.71/24.51, Loss(train/val) 2.24/2.22. Took 6.66 sec\n",
            "Epoch 3, Acc(train/val): 24.51/24.67, Loss(train/val) 2.20/2.17. Took 7.21 sec\n",
            "Epoch 4, Acc(train/val): 25.07/25.00, Loss(train/val) 2.16/2.13. Took 7.17 sec\n",
            "Epoch 5, Acc(train/val): 25.70/25.94, Loss(train/val) 2.12/2.09. Took 6.70 sec\n",
            "Epoch 6, Acc(train/val): 26.77/26.84, Loss(train/val) 2.08/2.05. Took 6.53 sec\n",
            "Epoch 7, Acc(train/val): 28.16/27.98, Loss(train/val) 2.04/2.02. Took 6.33 sec\n",
            "Epoch 8, Acc(train/val): 29.10/29.07, Loss(train/val) 2.00/1.99. Took 7.21 sec\n",
            "Epoch 9, Acc(train/val): 30.11/30.11, Loss(train/val) 1.97/1.96. Took 6.84 sec\n",
            "| \u001b[39m68       \u001b[39m | \u001b[39m-1.957   \u001b[39m | \u001b[39m0.1538   \u001b[39m | \u001b[39m0.08289  \u001b[39m | \u001b[39m1.314e+03\u001b[39m | \u001b[39m0.0005563\u001b[39m | \u001b[39m0.002365 \u001b[39m | \u001b[39m4.665    \u001b[39m | \u001b[39m0.2507   \u001b[39m | \u001b[39m91.28    \u001b[39m | \u001b[39m210.7    \u001b[39m | \u001b[39m0.5816   \u001b[39m | \u001b[39m0.7852   \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=1, hid_dim=1315, act='leakyrelu', dropout=np.float64(0.3818259570067726), use_bn=True, l2=np.float64(0.0007954630028185858), w_initial='xavier', optim='ADAM', lr=np.float64(0.0022266732548095612), epoch=10, train_batch_size=215, test_batch_size=88, exp_name='exp67')\n",
            "Epoch 0, Acc(train/val): 39.12/40.91, Loss(train/val) 1.83/1.72. Took 7.28 sec\n",
            "Epoch 1, Acc(train/val): 44.08/41.29, Loss(train/val) 1.60/1.72. Took 7.11 sec\n",
            "Epoch 2, Acc(train/val): 46.41/41.52, Loss(train/val) 1.54/1.73. Took 6.93 sec\n",
            "Epoch 3, Acc(train/val): 47.82/44.69, Loss(train/val) 1.50/1.62. Took 7.07 sec\n",
            "Epoch 4, Acc(train/val): 48.93/40.84, Loss(train/val) 1.48/1.74. Took 7.27 sec\n",
            "Epoch 5, Acc(train/val): 48.90/43.04, Loss(train/val) 1.48/1.65. Took 7.13 sec\n",
            "Epoch 6, Acc(train/val): 50.41/46.00, Loss(train/val) 1.44/1.57. Took 7.14 sec\n",
            "Epoch 7, Acc(train/val): 50.50/41.75, Loss(train/val) 1.42/1.74. Took 7.32 sec\n",
            "Epoch 8, Acc(train/val): 51.00/43.73, Loss(train/val) 1.44/1.64. Took 6.87 sec\n",
            "Epoch 9, Acc(train/val): 51.64/47.13, Loss(train/val) 1.39/1.56. Took 7.18 sec\n",
            "| \u001b[39m69       \u001b[39m | \u001b[39m-1.56    \u001b[39m | \u001b[39m0.373    \u001b[39m | \u001b[39m0.3818   \u001b[39m | \u001b[39m1.316e+03\u001b[39m | \u001b[39m0.0007955\u001b[39m | \u001b[39m0.002227 \u001b[39m | \u001b[39m1.514    \u001b[39m | \u001b[39m2.365    \u001b[39m | \u001b[39m88.49    \u001b[39m | \u001b[39m215.7    \u001b[39m | \u001b[39m0.2459   \u001b[39m | \u001b[39m0.8264   \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=3, hid_dim=2620, act='tanh', dropout=np.float64(0.04481740764788705), use_bn=True, l2=np.float64(0.00031284879660373546), w_initial='xavier', optim='SGD', lr=np.float64(0.006651655398139105), epoch=10, train_batch_size=14, test_batch_size=175, exp_name='exp68')\n",
            "Epoch 0, Acc(train/val): 31.34/21.25, Loss(train/val) 2.09/2.29. Took 19.64 sec\n",
            "Epoch 1, Acc(train/val): 33.06/29.93, Loss(train/val) 2.03/2.16. Took 18.28 sec\n",
            "Epoch 2, Acc(train/val): 35.99/29.01, Loss(train/val) 1.92/2.28. Took 20.52 sec\n",
            "Epoch 3, Acc(train/val): 38.13/34.64, Loss(train/val) 1.86/1.94. Took 20.79 sec\n",
            "Epoch 4, Acc(train/val): 38.39/35.47, Loss(train/val) 1.85/1.94. Took 19.05 sec\n",
            "Epoch 5, Acc(train/val): 40.94/39.43, Loss(train/val) 1.77/1.82. Took 18.50 sec\n",
            "Epoch 6, Acc(train/val): 42.95/35.22, Loss(train/val) 1.70/2.00. Took 18.82 sec\n",
            "Epoch 7, Acc(train/val): 43.69/37.27, Loss(train/val) 1.69/1.95. Took 19.47 sec\n",
            "Epoch 8, Acc(train/val): 45.47/39.43, Loss(train/val) 1.62/1.80. Took 18.91 sec\n",
            "Epoch 9, Acc(train/val): 45.98/43.77, Loss(train/val) 1.60/1.72. Took 18.86 sec\n",
            "| \u001b[39m70       \u001b[39m | \u001b[39m-1.719   \u001b[39m | \u001b[39m0.5815   \u001b[39m | \u001b[39m0.04482  \u001b[39m | \u001b[39m2.621e+03\u001b[39m | \u001b[39m0.0003128\u001b[39m | \u001b[39m0.006652 \u001b[39m | \u001b[39m3.208    \u001b[39m | \u001b[39m0.05418  \u001b[39m | \u001b[39m175.8    \u001b[39m | \u001b[39m14.86    \u001b[39m | \u001b[39m0.1775   \u001b[39m | \u001b[39m0.9556   \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=1, hid_dim=817, act='leakyrelu', dropout=np.float64(0.22371079930829174), use_bn=False, l2=np.float64(0.00014148611159235352), w_initial='he', optim='RMSprop', lr=np.float64(0.0018405915130106293), epoch=10, train_batch_size=189, test_batch_size=128, exp_name='exp69')\n",
            "Epoch 0, Acc(train/val): 37.60/41.53, Loss(train/val) 3.17/1.80. Took 7.83 sec\n",
            "Epoch 1, Acc(train/val): 44.17/38.30, Loss(train/val) 1.65/1.97. Took 7.26 sec\n",
            "Epoch 2, Acc(train/val): 45.84/42.21, Loss(train/val) 1.61/1.82. Took 7.19 sec\n",
            "Epoch 3, Acc(train/val): 48.37/46.47, Loss(train/val) 1.55/1.54. Took 7.55 sec\n",
            "Epoch 4, Acc(train/val): 50.52/43.98, Loss(train/val) 1.48/1.64. Took 7.34 sec\n",
            "Epoch 5, Acc(train/val): 51.44/45.81, Loss(train/val) 1.47/1.65. Took 7.52 sec\n",
            "Epoch 6, Acc(train/val): 53.14/45.89, Loss(train/val) 1.47/1.62. Took 7.83 sec\n",
            "Epoch 7, Acc(train/val): 54.36/46.09, Loss(train/val) 1.36/1.62. Took 7.66 sec\n",
            "Epoch 8, Acc(train/val): 55.56/43.72, Loss(train/val) 1.37/1.80. Took 6.87 sec\n",
            "Epoch 9, Acc(train/val): 55.81/42.19, Loss(train/val) 1.35/1.88. Took 7.13 sec\n",
            "| \u001b[39m71       \u001b[39m | \u001b[39m-1.88    \u001b[39m | \u001b[39m0.3528   \u001b[39m | \u001b[39m0.2237   \u001b[39m | \u001b[39m817.7    \u001b[39m | \u001b[39m0.0001415\u001b[39m | \u001b[39m0.001841 \u001b[39m | \u001b[39m1.92     \u001b[39m | \u001b[39m1.536    \u001b[39m | \u001b[39m128.4    \u001b[39m | \u001b[39m189.5    \u001b[39m | \u001b[39m0.9691   \u001b[39m | \u001b[39m0.2809   \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=2, hid_dim=2189, act='tanh', dropout=np.float64(0.14419198890092522), use_bn=True, l2=np.float64(0.0005172452124637613), w_initial='he', optim='ADAM', lr=np.float64(0.008755488252640675), epoch=10, train_batch_size=29, test_batch_size=159, exp_name='exp70')\n",
            "Epoch 0, Acc(train/val): 22.12/22.28, Loss(train/val) 2.52/2.09. Took 12.46 sec\n",
            "Epoch 1, Acc(train/val): 22.50/21.68, Loss(train/val) 2.10/2.15. Took 11.06 sec\n",
            "Epoch 2, Acc(train/val): 21.35/20.63, Loss(train/val) 2.14/2.13. Took 10.54 sec\n",
            "Epoch 3, Acc(train/val): 20.36/19.26, Loss(train/val) 2.16/2.19. Took 11.20 sec\n",
            "Epoch 4, Acc(train/val): 20.15/21.01, Loss(train/val) 2.17/2.14. Took 9.59 sec\n",
            "Epoch 5, Acc(train/val): 20.28/19.51, Loss(train/val) 2.18/2.21. Took 9.73 sec\n",
            "Epoch 6, Acc(train/val): 20.11/21.96, Loss(train/val) 2.18/2.18. Took 11.22 sec\n",
            "Epoch 7, Acc(train/val): 19.60/20.07, Loss(train/val) 2.18/2.17. Took 10.93 sec\n",
            "Epoch 8, Acc(train/val): 19.98/18.67, Loss(train/val) 2.17/2.21. Took 11.68 sec\n",
            "Epoch 9, Acc(train/val): 20.36/20.91, Loss(train/val) 2.17/2.14. Took 11.44 sec\n",
            "| \u001b[39m72       \u001b[39m | \u001b[39m-2.143   \u001b[39m | \u001b[39m0.6022   \u001b[39m | \u001b[39m0.1442   \u001b[39m | \u001b[39m2.19e+03 \u001b[39m | \u001b[39m0.0005172\u001b[39m | \u001b[39m0.008755 \u001b[39m | \u001b[39m2.677    \u001b[39m | \u001b[39m2.433    \u001b[39m | \u001b[39m159.5    \u001b[39m | \u001b[39m29.4     \u001b[39m | \u001b[39m0.4638   \u001b[39m | \u001b[39m0.2779   \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=4, hid_dim=2472, act='relu', dropout=np.float64(0.37124652670353714), use_bn=True, l2=np.float64(0.0006911041399852395), w_initial='he', optim='SGD', lr=np.float64(0.002979335771887166), epoch=10, train_batch_size=69, test_batch_size=100, exp_name='exp71')\n",
            "Epoch 0, Acc(train/val): 31.53/39.35, Loss(train/val) 1.93/1.71. Took 8.12 sec\n",
            "Epoch 1, Acc(train/val): 39.79/41.88, Loss(train/val) 1.70/1.63. Took 7.85 sec\n",
            "Epoch 2, Acc(train/val): 43.08/44.75, Loss(train/val) 1.61/1.57. Took 6.97 sec\n",
            "Epoch 3, Acc(train/val): 45.01/45.94, Loss(train/val) 1.55/1.54. Took 7.24 sec\n",
            "Epoch 4, Acc(train/val): 47.15/46.09, Loss(train/val) 1.49/1.54. Took 7.45 sec\n",
            "Epoch 5, Acc(train/val): 48.88/47.21, Loss(train/val) 1.45/1.51. Took 7.10 sec\n",
            "Epoch 6, Acc(train/val): 49.73/47.93, Loss(train/val) 1.42/1.47. Took 7.20 sec\n",
            "Epoch 7, Acc(train/val): 51.14/48.26, Loss(train/val) 1.38/1.47. Took 7.44 sec\n",
            "Epoch 8, Acc(train/val): 52.19/49.43, Loss(train/val) 1.35/1.44. Took 6.99 sec\n",
            "Epoch 9, Acc(train/val): 53.16/49.66, Loss(train/val) 1.32/1.44. Took 7.03 sec\n",
            "| \u001b[39m73       \u001b[39m | \u001b[39m-1.437   \u001b[39m | \u001b[39m0.1129   \u001b[39m | \u001b[39m0.3712   \u001b[39m | \u001b[39m2.472e+03\u001b[39m | \u001b[39m0.0006911\u001b[39m | \u001b[39m0.002979 \u001b[39m | \u001b[39m4.039    \u001b[39m | \u001b[39m0.7781   \u001b[39m | \u001b[39m100.7    \u001b[39m | \u001b[39m69.15    \u001b[39m | \u001b[39m0.2705   \u001b[39m | \u001b[39m0.2223   \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=2, hid_dim=2470, act='leakyrelu', dropout=np.float64(0.11188826678849628), use_bn=True, l2=np.float64(0.0002594154985468388), w_initial='xavier', optim='SGD', lr=np.float64(0.0009143542013781575), epoch=10, train_batch_size=228, test_batch_size=191, exp_name='exp72')\n",
            "Epoch 0, Acc(train/val): 29.84/37.12, Loss(train/val) 1.97/1.80. Took 6.83 sec\n",
            "Epoch 1, Acc(train/val): 39.38/41.04, Loss(train/val) 1.73/1.70. Took 6.54 sec\n",
            "Epoch 2, Acc(train/val): 42.91/42.82, Loss(train/val) 1.64/1.65. Took 7.05 sec\n",
            "Epoch 3, Acc(train/val): 45.31/43.98, Loss(train/val) 1.58/1.62. Took 7.47 sec\n",
            "Epoch 4, Acc(train/val): 47.38/44.95, Loss(train/val) 1.53/1.60. Took 7.16 sec\n",
            "Epoch 5, Acc(train/val): 48.55/45.40, Loss(train/val) 1.50/1.57. Took 7.08 sec\n",
            "Epoch 6, Acc(train/val): 49.96/45.83, Loss(train/val) 1.46/1.56. Took 6.81 sec\n",
            "Epoch 7, Acc(train/val): 51.09/46.76, Loss(train/val) 1.43/1.54. Took 6.69 sec\n",
            "Epoch 8, Acc(train/val): 52.05/47.20, Loss(train/val) 1.40/1.53. Took 6.90 sec\n",
            "Epoch 9, Acc(train/val): 53.13/47.84, Loss(train/val) 1.38/1.52. Took 7.13 sec\n",
            "| \u001b[39m74       \u001b[39m | \u001b[39m-1.518   \u001b[39m | \u001b[39m0.3846   \u001b[39m | \u001b[39m0.1119   \u001b[39m | \u001b[39m2.47e+03 \u001b[39m | \u001b[39m0.0002594\u001b[39m | \u001b[39m0.0009144\u001b[39m | \u001b[39m2.724    \u001b[39m | \u001b[39m0.9608   \u001b[39m | \u001b[39m191.2    \u001b[39m | \u001b[39m228.6    \u001b[39m | \u001b[39m0.3673   \u001b[39m | \u001b[39m0.9179   \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=4, hid_dim=2779, act='tanh', dropout=np.float64(0.006118922601705323), use_bn=True, l2=np.float64(0.00024997854103956903), w_initial='xavier', optim='RMSprop', lr=np.float64(0.0018174533854016359), epoch=10, train_batch_size=171, test_batch_size=107, exp_name='exp73')\n",
            "Epoch 0, Acc(train/val): 15.30/18.14, Loss(train/val) 4.52/2.91. Took 7.40 sec\n",
            "Epoch 1, Acc(train/val): 19.09/21.43, Loss(train/val) 2.90/2.44. Took 7.81 sec\n",
            "Epoch 2, Acc(train/val): 21.22/24.94, Loss(train/val) 2.60/2.23. Took 7.91 sec\n",
            "Epoch 3, Acc(train/val): 23.09/19.36, Loss(train/val) 2.31/2.39. Took 6.97 sec\n",
            "Epoch 4, Acc(train/val): 24.42/20.52, Loss(train/val) 2.15/2.13. Took 6.79 sec\n",
            "Epoch 5, Acc(train/val): 26.52/28.85, Loss(train/val) 2.03/1.98. Took 6.98 sec\n",
            "Epoch 6, Acc(train/val): 30.89/30.98, Loss(train/val) 1.91/1.94. Took 7.49 sec\n",
            "Epoch 7, Acc(train/val): 34.48/34.03, Loss(train/val) 1.82/1.83. Took 7.28 sec\n",
            "Epoch 8, Acc(train/val): 36.27/27.61, Loss(train/val) 1.77/2.22. Took 7.02 sec\n",
            "Epoch 9, Acc(train/val): 37.49/33.86, Loss(train/val) 1.74/1.85. Took 6.82 sec\n",
            "| \u001b[39m75       \u001b[39m | \u001b[39m-1.848   \u001b[39m | \u001b[39m0.5841   \u001b[39m | \u001b[39m0.006119 \u001b[39m | \u001b[39m2.78e+03 \u001b[39m | \u001b[39m0.00025  \u001b[39m | \u001b[39m0.001817 \u001b[39m | \u001b[39m4.774    \u001b[39m | \u001b[39m1.214    \u001b[39m | \u001b[39m107.8    \u001b[39m | \u001b[39m171.5    \u001b[39m | \u001b[39m0.008505 \u001b[39m | \u001b[39m0.5779   \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=1, hid_dim=842, act='relu', dropout=np.float64(0.2689146866022078), use_bn=True, l2=np.float64(0.00019395912179492175), w_initial='xavier', optim='ADAM', lr=np.float64(0.003675010198727521), epoch=10, train_batch_size=46, test_batch_size=138, exp_name='exp74')\n",
            "Epoch 0, Acc(train/val): 33.30/33.81, Loss(train/val) 2.47/2.77. Took 7.42 sec\n",
            "Epoch 1, Acc(train/val): 37.24/38.54, Loss(train/val) 2.38/2.19. Took 7.54 sec\n",
            "Epoch 2, Acc(train/val): 39.23/38.91, Loss(train/val) 2.28/2.17. Took 7.92 sec\n",
            "Epoch 3, Acc(train/val): 40.21/37.74, Loss(train/val) 2.27/2.44. Took 7.83 sec\n",
            "Epoch 4, Acc(train/val): 41.43/39.84, Loss(train/val) 2.19/2.00. Took 7.83 sec\n",
            "Epoch 5, Acc(train/val): 41.16/36.72, Loss(train/val) 2.31/2.40. Took 7.80 sec\n",
            "Epoch 6, Acc(train/val): 41.92/37.86, Loss(train/val) 2.27/2.76. Took 7.44 sec\n",
            "Epoch 7, Acc(train/val): 42.91/38.36, Loss(train/val) 2.15/2.80. Took 8.21 sec\n",
            "Epoch 8, Acc(train/val): 42.70/41.62, Loss(train/val) 2.19/2.14. Took 7.84 sec\n",
            "Epoch 9, Acc(train/val): 43.30/38.99, Loss(train/val) 2.14/2.34. Took 8.52 sec\n",
            "| \u001b[39m76       \u001b[39m | \u001b[39m-2.336   \u001b[39m | \u001b[39m0.1856   \u001b[39m | \u001b[39m0.2689   \u001b[39m | \u001b[39m842.7    \u001b[39m | \u001b[39m0.000194 \u001b[39m | \u001b[39m0.003675 \u001b[39m | \u001b[39m1.627    \u001b[39m | \u001b[39m2.069    \u001b[39m | \u001b[39m138.4    \u001b[39m | \u001b[39m46.84    \u001b[39m | \u001b[39m0.3047   \u001b[39m | \u001b[39m0.5291   \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=3, hid_dim=2627, act='sigmoid', dropout=np.float64(0.31835711662478916), use_bn=False, l2=np.float64(0.0005919892679164256), w_initial='xavier', optim='RMSprop', lr=np.float64(0.000590522672026433), epoch=10, train_batch_size=13, test_batch_size=176, exp_name='exp75')\n",
            "Epoch 0, Acc(train/val): 16.38/20.20, Loss(train/val) 2.23/2.17. Took 22.65 sec\n",
            "Epoch 1, Acc(train/val): 22.95/25.60, Loss(train/val) 2.06/2.00. Took 21.19 sec\n",
            "Epoch 2, Acc(train/val): 24.32/20.59, Loss(train/val) 2.03/2.05. Took 20.71 sec\n",
            "Epoch 3, Acc(train/val): 25.12/23.94, Loss(train/val) 2.01/2.05. Took 21.47 sec\n",
            "Epoch 4, Acc(train/val): 25.79/23.61, Loss(train/val) 2.01/2.09. Took 20.36 sec\n",
            "Epoch 5, Acc(train/val): 26.64/24.94, Loss(train/val) 1.99/1.97. Took 21.26 sec\n",
            "Epoch 6, Acc(train/val): 26.83/27.95, Loss(train/val) 1.99/1.97. Took 21.00 sec\n",
            "Epoch 7, Acc(train/val): 27.11/28.70, Loss(train/val) 1.98/1.93. Took 20.91 sec\n",
            "Epoch 8, Acc(train/val): 26.94/28.72, Loss(train/val) 1.98/1.96. Took 20.79 sec\n",
            "Epoch 9, Acc(train/val): 27.07/27.96, Loss(train/val) 1.97/1.94. Took 21.84 sec\n",
            "| \u001b[39m77       \u001b[39m | \u001b[39m-1.941   \u001b[39m | \u001b[39m0.9449   \u001b[39m | \u001b[39m0.3184   \u001b[39m | \u001b[39m2.627e+03\u001b[39m | \u001b[39m0.000592 \u001b[39m | \u001b[39m0.0005905\u001b[39m | \u001b[39m3.839    \u001b[39m | \u001b[39m1.838    \u001b[39m | \u001b[39m176.1    \u001b[39m | \u001b[39m13.6     \u001b[39m | \u001b[39m0.8661   \u001b[39m | \u001b[39m0.9924   \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=5, hid_dim=2617, act='leakyrelu', dropout=np.float64(0.3111951778443311), use_bn=True, l2=np.float64(0.0007716389515841335), w_initial='xavier', optim='ADAM', lr=np.float64(0.0019884849763396237), epoch=10, train_batch_size=9, test_batch_size=172, exp_name='exp76')\n",
            "Epoch 0, Acc(train/val): 17.57/22.15, Loss(train/val) 2.62/2.10. Took 48.19 sec\n",
            "Epoch 1, Acc(train/val): 22.22/22.96, Loss(train/val) 2.07/2.18. Took 43.81 sec\n",
            "Epoch 2, Acc(train/val): 22.42/26.33, Loss(train/val) 2.06/2.21. Took 45.64 sec\n",
            "Epoch 3, Acc(train/val): 23.23/20.61, Loss(train/val) 2.05/2.89. Took 46.17 sec\n",
            "Epoch 4, Acc(train/val): 23.55/24.31, Loss(train/val) 2.05/2.30. Took 45.51 sec\n",
            "Epoch 5, Acc(train/val): 23.04/25.63, Loss(train/val) 2.05/2.69. Took 44.62 sec\n",
            "Epoch 6, Acc(train/val): 23.37/20.98, Loss(train/val) 2.04/2.77. Took 46.12 sec\n",
            "Epoch 7, Acc(train/val): 23.31/23.90, Loss(train/val) 2.04/2.04. Took 45.62 sec\n",
            "Epoch 8, Acc(train/val): 23.61/25.23, Loss(train/val) 2.04/2.03. Took 46.01 sec\n",
            "Epoch 9, Acc(train/val): 23.89/24.70, Loss(train/val) 2.03/2.67. Took 44.70 sec\n",
            "| \u001b[39m78       \u001b[39m | \u001b[39m-2.672   \u001b[39m | \u001b[39m0.3105   \u001b[39m | \u001b[39m0.3112   \u001b[39m | \u001b[39m2.617e+03\u001b[39m | \u001b[39m0.0007716\u001b[39m | \u001b[39m0.001988 \u001b[39m | \u001b[39m5.834    \u001b[39m | \u001b[39m2.977    \u001b[39m | \u001b[39m172.1    \u001b[39m | \u001b[39m9.572    \u001b[39m | \u001b[39m0.297    \u001b[39m | \u001b[39m0.8453   \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=3, hid_dim=2464, act='tanh', dropout=np.float64(0.06247865615343384), use_bn=False, l2=np.float64(0.00012687263752521837), w_initial='he', optim='RMSprop', lr=np.float64(0.005332683417166393), epoch=10, train_batch_size=72, test_batch_size=98, exp_name='exp77')\n",
            "Epoch 0, Acc(train/val): 16.70/14.43, Loss(train/val) 8.91/9.42. Took 7.13 sec\n",
            "Epoch 1, Acc(train/val): 18.84/16.73, Loss(train/val) 6.13/5.75. Took 7.43 sec\n",
            "Epoch 2, Acc(train/val): 19.25/18.51, Loss(train/val) 5.61/7.20. Took 7.25 sec\n",
            "Epoch 3, Acc(train/val): 19.56/17.30, Loss(train/val) 5.41/5.15. Took 7.42 sec\n",
            "Epoch 4, Acc(train/val): 20.04/18.22, Loss(train/val) 5.27/7.81. Took 6.95 sec\n",
            "Epoch 5, Acc(train/val): 20.39/18.89, Loss(train/val) 5.18/6.56. Took 7.09 sec\n",
            "Epoch 6, Acc(train/val): 20.01/22.77, Loss(train/val) 5.25/4.54. Took 7.40 sec\n",
            "Epoch 7, Acc(train/val): 20.27/16.57, Loss(train/val) 5.23/6.21. Took 7.41 sec\n",
            "Epoch 8, Acc(train/val): 20.16/19.77, Loss(train/val) 5.17/5.43. Took 6.58 sec\n",
            "Epoch 9, Acc(train/val): 20.41/21.38, Loss(train/val) 5.19/5.12. Took 6.39 sec\n",
            "| \u001b[39m79       \u001b[39m | \u001b[39m-5.115   \u001b[39m | \u001b[39m0.5296   \u001b[39m | \u001b[39m0.06248  \u001b[39m | \u001b[39m2.465e+03\u001b[39m | \u001b[39m0.0001269\u001b[39m | \u001b[39m0.005333 \u001b[39m | \u001b[39m3.299    \u001b[39m | \u001b[39m1.803    \u001b[39m | \u001b[39m98.63    \u001b[39m | \u001b[39m72.32    \u001b[39m | \u001b[39m0.6079   \u001b[39m | \u001b[39m0.373    \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=3, hid_dim=2248, act='tanh', dropout=np.float64(0.3493605586901266), use_bn=True, l2=np.float64(0.0008183261916642773), w_initial='xavier', optim='SGD', lr=np.float64(0.004313413651897785), epoch=10, train_batch_size=37, test_batch_size=140, exp_name='exp78')\n",
            "Epoch 0, Acc(train/val): 33.09/34.75, Loss(train/val) 1.92/1.92. Took 8.90 sec\n",
            "Epoch 1, Acc(train/val): 37.39/38.87, Loss(train/val) 1.81/1.79. Took 7.75 sec\n",
            "Epoch 2, Acc(train/val): 39.11/36.11, Loss(train/val) 1.75/1.83. Took 8.67 sec\n",
            "Epoch 3, Acc(train/val): 39.93/39.78, Loss(train/val) 1.73/1.75. Took 8.49 sec\n",
            "Epoch 4, Acc(train/val): 41.39/41.45, Loss(train/val) 1.70/1.73. Took 8.19 sec\n",
            "Epoch 5, Acc(train/val): 41.66/40.24, Loss(train/val) 1.68/1.72. Took 8.61 sec\n",
            "Epoch 6, Acc(train/val): 42.66/39.69, Loss(train/val) 1.66/1.78. Took 7.96 sec\n",
            "Epoch 7, Acc(train/val): 43.54/40.35, Loss(train/val) 1.63/1.75. Took 10.00 sec\n",
            "Epoch 8, Acc(train/val): 44.10/41.93, Loss(train/val) 1.61/1.71. Took 8.82 sec\n",
            "Epoch 9, Acc(train/val): 44.73/38.34, Loss(train/val) 1.60/1.81. Took 8.53 sec\n",
            "| \u001b[39m80       \u001b[39m | \u001b[39m-1.807   \u001b[39m | \u001b[39m0.716    \u001b[39m | \u001b[39m0.3494   \u001b[39m | \u001b[39m2.249e+03\u001b[39m | \u001b[39m0.0008183\u001b[39m | \u001b[39m0.004313 \u001b[39m | \u001b[39m3.257    \u001b[39m | \u001b[39m0.5283   \u001b[39m | \u001b[39m140.3    \u001b[39m | \u001b[39m37.45    \u001b[39m | \u001b[39m0.147    \u001b[39m | \u001b[39m0.5249   \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=4, hid_dim=958, act='relu', dropout=np.float64(0.43067314240298454), use_bn=True, l2=np.float64(0.0006630665643795503), w_initial='xavier', optim='SGD', lr=np.float64(0.0017214352465807926), epoch=10, train_batch_size=31, test_batch_size=136, exp_name='exp79')\n",
            "Epoch 0, Acc(train/val): 27.35/37.67, Loss(train/val) 2.03/1.75. Took 8.59 sec\n",
            "Epoch 1, Acc(train/val): 35.25/40.63, Loss(train/val) 1.80/1.67. Took 10.52 sec\n",
            "Epoch 2, Acc(train/val): 38.83/42.52, Loss(train/val) 1.71/1.62. Took 9.67 sec\n",
            "Epoch 3, Acc(train/val): 40.99/44.43, Loss(train/val) 1.66/1.56. Took 9.50 sec\n",
            "Epoch 4, Acc(train/val): 43.05/45.21, Loss(train/val) 1.60/1.55. Took 9.67 sec\n",
            "Epoch 5, Acc(train/val): 44.19/46.13, Loss(train/val) 1.57/1.52. Took 9.22 sec\n",
            "Epoch 6, Acc(train/val): 45.42/46.75, Loss(train/val) 1.53/1.50. Took 9.12 sec\n",
            "Epoch 7, Acc(train/val): 46.53/47.82, Loss(train/val) 1.50/1.47. Took 9.50 sec\n",
            "Epoch 8, Acc(train/val): 48.11/48.14, Loss(train/val) 1.47/1.46. Took 9.15 sec\n",
            "Epoch 9, Acc(train/val): 48.62/49.03, Loss(train/val) 1.44/1.44. Took 9.93 sec\n",
            "| \u001b[39m81       \u001b[39m | \u001b[39m-1.443   \u001b[39m | \u001b[39m0.09009  \u001b[39m | \u001b[39m0.4307   \u001b[39m | \u001b[39m958.1    \u001b[39m | \u001b[39m0.0006631\u001b[39m | \u001b[39m0.001721 \u001b[39m | \u001b[39m4.93     \u001b[39m | \u001b[39m0.5042   \u001b[39m | \u001b[39m136.5    \u001b[39m | \u001b[39m31.71    \u001b[39m | \u001b[39m0.2699   \u001b[39m | \u001b[39m0.8548   \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=2, hid_dim=1000, act='tanh', dropout=np.float64(0.32667775868275617), use_bn=True, l2=np.float64(0.0002343632970888828), w_initial='xavier', optim='RMSprop', lr=np.float64(0.004168131939849586), epoch=10, train_batch_size=36, test_batch_size=160, exp_name='exp80')\n",
            "Epoch 0, Acc(train/val): 23.24/20.60, Loss(train/val) 2.56/2.47. Took 7.98 sec\n",
            "Epoch 1, Acc(train/val): 28.18/20.92, Loss(train/val) 1.99/2.14. Took 8.83 sec\n",
            "Epoch 2, Acc(train/val): 27.63/23.62, Loss(train/val) 1.99/2.06. Took 8.27 sec\n",
            "Epoch 3, Acc(train/val): 27.38/24.64, Loss(train/val) 1.99/2.08. Took 8.60 sec\n",
            "Epoch 4, Acc(train/val): 28.20/22.18, Loss(train/val) 1.99/2.17. Took 8.80 sec\n",
            "Epoch 5, Acc(train/val): 27.43/19.20, Loss(train/val) 1.99/2.17. Took 9.24 sec\n",
            "Epoch 6, Acc(train/val): 27.80/19.55, Loss(train/val) 1.99/2.12. Took 8.28 sec\n",
            "Epoch 7, Acc(train/val): 27.80/25.44, Loss(train/val) 1.99/2.03. Took 9.28 sec\n",
            "Epoch 8, Acc(train/val): 27.64/24.63, Loss(train/val) 1.99/2.02. Took 8.48 sec\n",
            "Epoch 9, Acc(train/val): 27.86/24.14, Loss(train/val) 1.99/2.05. Took 8.67 sec\n",
            "| \u001b[39m82       \u001b[39m | \u001b[39m-2.051   \u001b[39m | \u001b[39m0.6003   \u001b[39m | \u001b[39m0.3267   \u001b[39m | \u001b[39m1.001e+03\u001b[39m | \u001b[39m0.0002344\u001b[39m | \u001b[39m0.004168 \u001b[39m | \u001b[39m2.609    \u001b[39m | \u001b[39m1.902    \u001b[39m | \u001b[39m160.7    \u001b[39m | \u001b[39m36.81    \u001b[39m | \u001b[39m0.3841   \u001b[39m | \u001b[39m0.7499   \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=3, hid_dim=2781, act='sigmoid', dropout=np.float64(0.30436046634160535), use_bn=True, l2=np.float64(0.0009680136772433526), w_initial='he', optim='SGD', lr=np.float64(0.004423812563062541), epoch=10, train_batch_size=174, test_batch_size=114, exp_name='exp81')\n",
            "Epoch 0, Acc(train/val): 31.66/38.22, Loss(train/val) 1.93/1.79. Took 7.60 sec\n",
            "Epoch 1, Acc(train/val): 36.80/39.69, Loss(train/val) 1.81/1.75. Took 7.00 sec\n",
            "Epoch 2, Acc(train/val): 38.40/40.44, Loss(train/val) 1.77/1.74. Took 7.08 sec\n",
            "Epoch 3, Acc(train/val): 39.67/40.88, Loss(train/val) 1.74/1.73. Took 7.43 sec\n",
            "Epoch 4, Acc(train/val): 40.60/41.60, Loss(train/val) 1.72/1.72. Took 7.29 sec\n",
            "Epoch 5, Acc(train/val): 40.98/41.86, Loss(train/val) 1.70/1.70. Took 7.68 sec\n",
            "Epoch 6, Acc(train/val): 41.96/41.92, Loss(train/val) 1.69/1.70. Took 7.16 sec\n",
            "Epoch 7, Acc(train/val): 42.08/41.94, Loss(train/val) 1.68/1.69. Took 7.45 sec\n",
            "Epoch 8, Acc(train/val): 42.48/42.82, Loss(train/val) 1.66/1.69. Took 7.59 sec\n",
            "Epoch 9, Acc(train/val): 42.79/42.03, Loss(train/val) 1.66/1.69. Took 7.25 sec\n",
            "| \u001b[39m83       \u001b[39m | \u001b[39m-1.69    \u001b[39m | \u001b[39m0.7938   \u001b[39m | \u001b[39m0.3044   \u001b[39m | \u001b[39m2.782e+03\u001b[39m | \u001b[39m0.000968 \u001b[39m | \u001b[39m0.004424 \u001b[39m | \u001b[39m3.207    \u001b[39m | \u001b[39m0.945    \u001b[39m | \u001b[39m114.1    \u001b[39m | \u001b[39m174.9    \u001b[39m | \u001b[39m0.268    \u001b[39m | \u001b[39m0.2072   \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=2, hid_dim=2784, act='tanh', dropout=np.float64(0.15269973625381672), use_bn=False, l2=np.float64(0.0008886764941452247), w_initial='xavier', optim='SGD', lr=np.float64(0.0076243894384873), epoch=10, train_batch_size=181, test_batch_size=113, exp_name='exp82')\n",
            "Epoch 0, Acc(train/val): 31.89/35.55, Loss(train/val) 1.96/1.87. Took 7.17 sec\n",
            "Epoch 1, Acc(train/val): 37.24/37.39, Loss(train/val) 1.83/1.82. Took 7.41 sec\n",
            "Epoch 2, Acc(train/val): 38.84/38.24, Loss(train/val) 1.79/1.79. Took 7.30 sec\n",
            "Epoch 3, Acc(train/val): 39.81/38.67, Loss(train/val) 1.76/1.78. Took 7.35 sec\n",
            "Epoch 4, Acc(train/val): 40.58/39.20, Loss(train/val) 1.74/1.77. Took 7.36 sec\n",
            "Epoch 5, Acc(train/val): 41.06/39.72, Loss(train/val) 1.72/1.76. Took 7.27 sec\n",
            "Epoch 6, Acc(train/val): 41.52/40.11, Loss(train/val) 1.71/1.75. Took 7.29 sec\n",
            "Epoch 7, Acc(train/val): 41.80/40.51, Loss(train/val) 1.70/1.74. Took 7.26 sec\n",
            "Epoch 8, Acc(train/val): 42.29/40.61, Loss(train/val) 1.69/1.73. Took 7.81 sec\n",
            "Epoch 9, Acc(train/val): 42.38/40.63, Loss(train/val) 1.68/1.73. Took 7.28 sec\n",
            "| \u001b[39m84       \u001b[39m | \u001b[39m-1.731   \u001b[39m | \u001b[39m0.5092   \u001b[39m | \u001b[39m0.1527   \u001b[39m | \u001b[39m2.784e+03\u001b[39m | \u001b[39m0.0008887\u001b[39m | \u001b[39m0.007624 \u001b[39m | \u001b[39m2.633    \u001b[39m | \u001b[39m0.898    \u001b[39m | \u001b[39m113.5    \u001b[39m | \u001b[39m181.6    \u001b[39m | \u001b[39m0.9567   \u001b[39m | \u001b[39m0.6842   \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=3, hid_dim=926, act='leakyrelu', dropout=np.float64(0.47699541357456704), use_bn=False, l2=np.float64(0.0006900414697450668), w_initial='xavier', optim='ADAM', lr=np.float64(0.00812946045280212), epoch=10, train_batch_size=6, test_batch_size=233, exp_name='exp83')\n",
            "Epoch 0, Acc(train/val): 12.21/14.50, Loss(train/val) 421.28/200.76. Took 34.42 sec\n",
            "Epoch 1, Acc(train/val): 12.84/11.01, Loss(train/val) 627.39/667.31. Took 34.95 sec\n",
            "Epoch 2, Acc(train/val): 13.22/16.00, Loss(train/val) 749.18/364.08. Took 34.34 sec\n",
            "Epoch 3, Acc(train/val): 13.13/16.10, Loss(train/val) 896.51/320.15. Took 33.75 sec\n",
            "Epoch 4, Acc(train/val): 13.37/19.65, Loss(train/val) 1023.34/395.59. Took 35.14 sec\n",
            "Epoch 5, Acc(train/val): 13.71/19.44, Loss(train/val) 1151.87/260.78. Took 35.39 sec\n",
            "Epoch 6, Acc(train/val): 13.31/19.86, Loss(train/val) 1178.31/498.07. Took 35.52 sec\n",
            "Epoch 7, Acc(train/val): 14.03/13.98, Loss(train/val) 1351.99/388.50. Took 34.27 sec\n",
            "Epoch 8, Acc(train/val): 13.66/13.48, Loss(train/val) 1373.41/1832.58. Took 35.84 sec\n",
            "Epoch 9, Acc(train/val): 13.86/18.11, Loss(train/val) 1414.66/585.11. Took 35.26 sec\n",
            "| \u001b[39m85       \u001b[39m | \u001b[39m-585.1   \u001b[39m | \u001b[39m0.3898   \u001b[39m | \u001b[39m0.477    \u001b[39m | \u001b[39m926.2    \u001b[39m | \u001b[39m0.00069  \u001b[39m | \u001b[39m0.008129 \u001b[39m | \u001b[39m3.521    \u001b[39m | \u001b[39m2.275    \u001b[39m | \u001b[39m233.4    \u001b[39m | \u001b[39m6.438    \u001b[39m | \u001b[39m0.6189   \u001b[39m | \u001b[39m0.92     \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=2, hid_dim=1265, act='tanh', dropout=np.float64(0.03987317722071687), use_bn=True, l2=np.float64(0.0006937999672041722), w_initial='xavier', optim='RMSprop', lr=np.float64(6.4660910829980185e-06), epoch=10, train_batch_size=4, test_batch_size=87, exp_name='exp84')\n",
            "Epoch 0, Acc(train/val): 30.95/39.07, Loss(train/val) 1.96/1.78. Took 54.50 sec\n",
            "Epoch 1, Acc(train/val): 35.12/40.80, Loss(train/val) 1.87/1.74. Took 52.05 sec\n",
            "Epoch 2, Acc(train/val): 36.57/40.75, Loss(train/val) 1.84/1.74. Took 52.27 sec\n",
            "Epoch 3, Acc(train/val): 38.09/42.61, Loss(train/val) 1.80/1.70. Took 51.43 sec\n",
            "Epoch 4, Acc(train/val): 39.47/42.01, Loss(train/val) 1.77/1.69. Took 56.63 sec\n",
            "Epoch 5, Acc(train/val): 40.11/42.57, Loss(train/val) 1.75/1.68. Took 48.81 sec\n",
            "Epoch 6, Acc(train/val): 40.86/43.37, Loss(train/val) 1.73/1.67. Took 53.79 sec\n",
            "Epoch 7, Acc(train/val): 42.03/43.05, Loss(train/val) 1.70/1.65. Took 52.91 sec\n",
            "Epoch 8, Acc(train/val): 42.93/42.89, Loss(train/val) 1.67/1.65. Took 55.50 sec\n",
            "Epoch 9, Acc(train/val): 43.89/44.17, Loss(train/val) 1.66/1.62. Took 55.47 sec\n",
            "| \u001b[39m86       \u001b[39m | \u001b[39m-1.622   \u001b[39m | \u001b[39m0.6566   \u001b[39m | \u001b[39m0.03987  \u001b[39m | \u001b[39m1.265e+03\u001b[39m | \u001b[39m0.0006938\u001b[39m | \u001b[39m6.466e-06\u001b[39m | \u001b[39m2.704    \u001b[39m | \u001b[39m1.931    \u001b[39m | \u001b[39m87.22    \u001b[39m | \u001b[39m4.816    \u001b[39m | \u001b[39m0.4934   \u001b[39m | \u001b[39m0.6776   \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=1, hid_dim=1014, act='leakyrelu', dropout=np.float64(0.19877864277144236), use_bn=False, l2=np.float64(0.0003770015934942675), w_initial='he', optim='ADAM', lr=np.float64(0.008011608035754732), epoch=10, train_batch_size=24, test_batch_size=152, exp_name='exp85')\n",
            "Epoch 0, Acc(train/val): 24.61/25.55, Loss(train/val) 12.53/11.90. Took 10.52 sec\n",
            "Epoch 1, Acc(train/val): 26.10/26.80, Loss(train/val) 12.38/14.90. Took 11.67 sec\n",
            "Epoch 2, Acc(train/val): 27.18/27.49, Loss(train/val) 11.01/13.12. Took 11.22 sec\n",
            "Epoch 3, Acc(train/val): 27.24/27.51, Loss(train/val) 11.93/12.14. Took 11.34 sec\n",
            "Epoch 4, Acc(train/val): 27.28/28.70, Loss(train/val) 12.46/8.87. Took 10.38 sec\n",
            "Epoch 5, Acc(train/val): 27.71/27.20, Loss(train/val) 11.58/10.51. Took 9.44 sec\n",
            "Epoch 6, Acc(train/val): 28.20/29.12, Loss(train/val) 12.32/8.91. Took 10.86 sec\n",
            "Epoch 7, Acc(train/val): 28.27/27.35, Loss(train/val) 11.36/15.93. Took 9.84 sec\n",
            "Epoch 8, Acc(train/val): 27.96/28.43, Loss(train/val) 12.34/9.94. Took 10.54 sec\n",
            "Epoch 9, Acc(train/val): 27.79/28.82, Loss(train/val) 12.04/12.36. Took 11.78 sec\n",
            "| \u001b[39m87       \u001b[39m | \u001b[39m-12.36   \u001b[39m | \u001b[39m0.3651   \u001b[39m | \u001b[39m0.1988   \u001b[39m | \u001b[39m1.015e+03\u001b[39m | \u001b[39m0.000377 \u001b[39m | \u001b[39m0.008012 \u001b[39m | \u001b[39m1.38     \u001b[39m | \u001b[39m2.687    \u001b[39m | \u001b[39m152.7    \u001b[39m | \u001b[39m24.1     \u001b[39m | \u001b[39m0.5677   \u001b[39m | \u001b[39m0.07846  \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=3, hid_dim=2787, act='sigmoid', dropout=np.float64(0.17310438201371575), use_bn=False, l2=np.float64(9.048477716617698e-05), w_initial='xavier', optim='RMSprop', lr=np.float64(0.0002806843234685615), epoch=10, train_batch_size=176, test_batch_size=112, exp_name='exp86')\n",
            "Epoch 0, Acc(train/val): 30.05/35.63, Loss(train/val) 1.98/1.78. Took 6.71 sec\n",
            "Epoch 1, Acc(train/val): 38.33/35.25, Loss(train/val) 1.71/1.81. Took 7.32 sec\n",
            "Epoch 2, Acc(train/val): 41.68/40.83, Loss(train/val) 1.62/1.65. Took 7.34 sec\n",
            "Epoch 3, Acc(train/val): 43.74/39.95, Loss(train/val) 1.57/1.67. Took 6.83 sec\n",
            "Epoch 4, Acc(train/val): 45.42/36.25, Loss(train/val) 1.52/1.85. Took 7.37 sec\n",
            "Epoch 5, Acc(train/val): 46.58/41.81, Loss(train/val) 1.49/1.60. Took 6.93 sec\n",
            "Epoch 6, Acc(train/val): 47.49/42.51, Loss(train/val) 1.46/1.63. Took 7.29 sec\n",
            "Epoch 7, Acc(train/val): 48.46/40.94, Loss(train/val) 1.43/1.75. Took 6.39 sec\n",
            "Epoch 8, Acc(train/val): 49.35/44.00, Loss(train/val) 1.41/1.55. Took 6.60 sec\n",
            "Epoch 9, Acc(train/val): 50.09/44.68, Loss(train/val) 1.39/1.57. Took 7.16 sec\n",
            "| \u001b[39m88       \u001b[39m | \u001b[39m-1.571   \u001b[39m | \u001b[39m0.8788   \u001b[39m | \u001b[39m0.1731   \u001b[39m | \u001b[39m2.787e+03\u001b[39m | \u001b[39m9.048e-05\u001b[39m | \u001b[39m0.0002807\u001b[39m | \u001b[39m3.05     \u001b[39m | \u001b[39m1.012    \u001b[39m | \u001b[39m112.1    \u001b[39m | \u001b[39m176.8    \u001b[39m | \u001b[39m0.5976   \u001b[39m | \u001b[39m0.7794   \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=5, hid_dim=1787, act='sigmoid', dropout=np.float64(0.25363666262637974), use_bn=False, l2=np.float64(0.000665111089708623), w_initial='xavier', optim='SGD', lr=np.float64(0.005486040301823537), epoch=10, train_batch_size=109, test_batch_size=53, exp_name='exp87')\n",
            "Epoch 0, Acc(train/val): 10.08/9.88, Loss(train/val) 2.32/2.31. Took 7.22 sec\n",
            "Epoch 1, Acc(train/val): 10.06/9.61, Loss(train/val) 2.32/2.31. Took 7.03 sec\n",
            "Epoch 2, Acc(train/val): 10.20/10.08, Loss(train/val) 2.32/2.31. Took 7.32 sec\n",
            "Epoch 3, Acc(train/val): 9.97/10.28, Loss(train/val) 2.32/2.31. Took 6.95 sec\n",
            "Epoch 4, Acc(train/val): 9.88/10.08, Loss(train/val) 2.32/2.32. Took 6.53 sec\n",
            "Epoch 5, Acc(train/val): 9.91/10.34, Loss(train/val) 2.32/2.31. Took 7.55 sec\n",
            "Epoch 6, Acc(train/val): 9.92/9.12, Loss(train/val) 2.32/2.31. Took 6.92 sec\n",
            "Epoch 7, Acc(train/val): 10.11/9.12, Loss(train/val) 2.32/2.31. Took 6.92 sec\n",
            "Epoch 8, Acc(train/val): 9.94/10.41, Loss(train/val) 2.32/2.31. Took 6.77 sec\n",
            "Epoch 9, Acc(train/val): 10.02/9.61, Loss(train/val) 2.32/2.31. Took 6.73 sec\n",
            "| \u001b[39m89       \u001b[39m | \u001b[39m-2.308   \u001b[39m | \u001b[39m0.8448   \u001b[39m | \u001b[39m0.2536   \u001b[39m | \u001b[39m1.787e+03\u001b[39m | \u001b[39m0.0006651\u001b[39m | \u001b[39m0.005486 \u001b[39m | \u001b[39m5.785    \u001b[39m | \u001b[39m0.2629   \u001b[39m | \u001b[39m53.04    \u001b[39m | \u001b[39m109.8    \u001b[39m | \u001b[39m0.8593   \u001b[39m | \u001b[39m0.6346   \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=4, hid_dim=925, act='relu', dropout=np.float64(0.2226055371677319), use_bn=False, l2=np.float64(6.094799013532213e-05), w_initial='xavier', optim='RMSprop', lr=np.float64(0.001882614668275281), epoch=10, train_batch_size=13, test_batch_size=245, exp_name='exp88')\n",
            "Epoch 0, Acc(train/val): 23.26/25.11, Loss(train/val) 3.36/1.98. Took 18.85 sec\n",
            "Epoch 1, Acc(train/val): 25.54/27.22, Loss(train/val) 2.07/1.96. Took 19.10 sec\n",
            "Epoch 2, Acc(train/val): 25.63/23.37, Loss(train/val) 2.06/2.31. Took 18.69 sec\n",
            "Epoch 3, Acc(train/val): 26.00/22.49, Loss(train/val) 2.05/2.03. Took 18.55 sec\n",
            "Epoch 4, Acc(train/val): 26.77/25.74, Loss(train/val) 2.03/2.07. Took 18.20 sec\n",
            "Epoch 5, Acc(train/val): 26.58/26.92, Loss(train/val) 2.02/1.95. Took 19.07 sec\n",
            "Epoch 6, Acc(train/val): 27.86/27.85, Loss(train/val) 2.01/1.92. Took 18.67 sec\n",
            "Epoch 7, Acc(train/val): 28.23/27.72, Loss(train/val) 2.00/1.93. Took 17.97 sec\n",
            "Epoch 8, Acc(train/val): 27.79/29.48, Loss(train/val) 1.99/1.88. Took 19.41 sec\n",
            "Epoch 9, Acc(train/val): 28.01/30.91, Loss(train/val) 1.99/1.88. Took 18.57 sec\n",
            "| \u001b[39m90       \u001b[39m | \u001b[39m-1.883   \u001b[39m | \u001b[39m0.1308   \u001b[39m | \u001b[39m0.2226   \u001b[39m | \u001b[39m925.6    \u001b[39m | \u001b[39m6.095e-05\u001b[39m | \u001b[39m0.001883 \u001b[39m | \u001b[39m4.538    \u001b[39m | \u001b[39m1.109    \u001b[39m | \u001b[39m245.9    \u001b[39m | \u001b[39m13.23    \u001b[39m | \u001b[39m0.9327   \u001b[39m | \u001b[39m0.6288   \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=4, hid_dim=813, act='relu', dropout=np.float64(0.33017747037913336), use_bn=False, l2=np.float64(0.0003346671809333208), w_initial='xavier', optim='SGD', lr=np.float64(0.00897358745830551), epoch=10, train_batch_size=195, test_batch_size=124, exp_name='exp89')\n",
            "Epoch 0, Acc(train/val): 15.07/24.53, Loss(train/val) 2.27/2.21. Took 6.51 sec\n",
            "Epoch 1, Acc(train/val): 23.11/25.43, Loss(train/val) 2.16/2.07. Took 7.38 sec\n",
            "Epoch 2, Acc(train/val): 26.89/30.38, Loss(train/val) 2.03/1.95. Took 7.32 sec\n",
            "Epoch 3, Acc(train/val): 30.12/32.66, Loss(train/val) 1.94/1.88. Took 7.24 sec\n",
            "Epoch 4, Acc(train/val): 32.56/34.01, Loss(train/val) 1.88/1.83. Took 6.97 sec\n",
            "Epoch 5, Acc(train/val): 34.50/35.86, Loss(train/val) 1.82/1.79. Took 7.29 sec\n",
            "Epoch 6, Acc(train/val): 36.23/38.03, Loss(train/val) 1.78/1.74. Took 7.10 sec\n",
            "Epoch 7, Acc(train/val): 37.55/39.23, Loss(train/val) 1.74/1.70. Took 7.17 sec\n",
            "Epoch 8, Acc(train/val): 38.94/40.59, Loss(train/val) 1.70/1.68. Took 7.17 sec\n",
            "Epoch 9, Acc(train/val): 39.95/41.57, Loss(train/val) 1.68/1.66. Took 6.68 sec\n",
            "| \u001b[39m91       \u001b[39m | \u001b[39m-1.655   \u001b[39m | \u001b[39m0.2301   \u001b[39m | \u001b[39m0.3302   \u001b[39m | \u001b[39m813.3    \u001b[39m | \u001b[39m0.0003347\u001b[39m | \u001b[39m0.008974 \u001b[39m | \u001b[39m4.27     \u001b[39m | \u001b[39m0.9584   \u001b[39m | \u001b[39m124.7    \u001b[39m | \u001b[39m195.3    \u001b[39m | \u001b[39m0.8233   \u001b[39m | \u001b[39m0.5499   \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=1, hid_dim=1785, act='leakyrelu', dropout=np.float64(0.25673916542769293), use_bn=True, l2=np.float64(0.00011617397324897028), w_initial='he', optim='SGD', lr=np.float64(0.002331331381374181), epoch=10, train_batch_size=116, test_batch_size=51, exp_name='exp90')\n",
            "Epoch 0, Acc(train/val): 25.40/30.00, Loss(train/val) 2.14/2.04. Took 7.25 sec\n",
            "Epoch 1, Acc(train/val): 32.38/33.23, Loss(train/val) 1.98/1.94. Took 6.97 sec\n",
            "Epoch 2, Acc(train/val): 35.11/35.36, Loss(train/val) 1.90/1.88. Took 6.96 sec\n",
            "Epoch 3, Acc(train/val): 36.85/36.71, Loss(train/val) 1.84/1.83. Took 7.08 sec\n",
            "Epoch 4, Acc(train/val): 38.10/37.72, Loss(train/val) 1.80/1.80. Took 7.10 sec\n",
            "Epoch 5, Acc(train/val): 39.23/38.39, Loss(train/val) 1.77/1.77. Took 7.21 sec\n",
            "Epoch 6, Acc(train/val): 39.98/39.44, Loss(train/val) 1.74/1.75. Took 7.31 sec\n",
            "Epoch 7, Acc(train/val): 40.82/40.16, Loss(train/val) 1.72/1.73. Took 7.06 sec\n",
            "Epoch 8, Acc(train/val): 41.43/40.72, Loss(train/val) 1.70/1.72. Took 7.78 sec\n",
            "Epoch 9, Acc(train/val): 42.18/41.08, Loss(train/val) 1.68/1.70. Took 7.05 sec\n",
            "| \u001b[39m92       \u001b[39m | \u001b[39m-1.701   \u001b[39m | \u001b[39m0.2598   \u001b[39m | \u001b[39m0.2567   \u001b[39m | \u001b[39m1.786e+03\u001b[39m | \u001b[39m0.0001162\u001b[39m | \u001b[39m0.002331 \u001b[39m | \u001b[39m1.366    \u001b[39m | \u001b[39m0.3564   \u001b[39m | \u001b[39m51.95    \u001b[39m | \u001b[39m116.9    \u001b[39m | \u001b[39m0.08124  \u001b[39m | \u001b[39m0.2544   \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=2, hid_dim=1448, act='tanh', dropout=np.float64(0.09445156468522886), use_bn=False, l2=np.float64(0.0002483758442659838), w_initial='he', optim='RMSprop', lr=np.float64(0.006707687502539066), epoch=10, train_batch_size=249, test_batch_size=103, exp_name='exp91')\n",
            "Epoch 0, Acc(train/val): 19.43/22.06, Loss(train/val) 8.19/4.56. Took 7.03 sec\n",
            "Epoch 1, Acc(train/val): 21.55/21.44, Loss(train/val) 4.53/4.20. Took 6.86 sec\n",
            "Epoch 2, Acc(train/val): 21.90/19.77, Loss(train/val) 3.88/3.68. Took 7.40 sec\n",
            "Epoch 3, Acc(train/val): 21.71/20.16, Loss(train/val) 3.75/4.16. Took 6.72 sec\n",
            "Epoch 4, Acc(train/val): 20.46/18.97, Loss(train/val) 3.91/3.45. Took 6.88 sec\n",
            "Epoch 5, Acc(train/val): 21.31/21.07, Loss(train/val) 3.81/3.49. Took 7.29 sec\n",
            "Epoch 6, Acc(train/val): 21.23/20.98, Loss(train/val) 3.79/3.72. Took 7.16 sec\n",
            "Epoch 7, Acc(train/val): 20.59/20.38, Loss(train/val) 3.73/3.64. Took 6.88 sec\n",
            "Epoch 8, Acc(train/val): 20.85/20.88, Loss(train/val) 3.86/4.61. Took 6.94 sec\n",
            "Epoch 9, Acc(train/val): 20.70/20.46, Loss(train/val) 3.91/4.41. Took 7.13 sec\n",
            "| \u001b[39m93       \u001b[39m | \u001b[39m-4.412   \u001b[39m | \u001b[39m0.5552   \u001b[39m | \u001b[39m0.09445  \u001b[39m | \u001b[39m1.449e+03\u001b[39m | \u001b[39m0.0002484\u001b[39m | \u001b[39m0.006708 \u001b[39m | \u001b[39m2.763    \u001b[39m | \u001b[39m1.907    \u001b[39m | \u001b[39m103.8    \u001b[39m | \u001b[39m249.6    \u001b[39m | \u001b[39m0.6452   \u001b[39m | \u001b[39m0.2498   \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=4, hid_dim=2617, act='sigmoid', dropout=np.float64(0.14517975068186895), use_bn=False, l2=np.float64(3.8403183260594516e-05), w_initial='he', optim='ADAM', lr=np.float64(0.00227822553345649), epoch=10, train_batch_size=17, test_batch_size=179, exp_name='exp92')\n",
            "Epoch 0, Acc(train/val): 9.89/10.08, Loss(train/val) 2.50/2.49. Took 22.04 sec\n",
            "Epoch 1, Acc(train/val): 10.38/9.12, Loss(train/val) 2.55/2.76. Took 21.42 sec\n",
            "Epoch 2, Acc(train/val): 10.25/10.28, Loss(train/val) 2.63/2.57. Took 21.37 sec\n",
            "Epoch 3, Acc(train/val): 10.07/10.11, Loss(train/val) 2.66/2.48. Took 22.47 sec\n",
            "Epoch 4, Acc(train/val): 9.84/9.88, Loss(train/val) 2.67/2.50. Took 20.82 sec\n",
            "Epoch 5, Acc(train/val): 10.21/10.34, Loss(train/val) 2.68/2.63. Took 21.39 sec\n",
            "Epoch 6, Acc(train/val): 10.02/10.28, Loss(train/val) 2.65/2.62. Took 21.56 sec\n",
            "Epoch 7, Acc(train/val): 10.17/10.34, Loss(train/val) 2.70/2.45. Took 21.82 sec\n",
            "Epoch 8, Acc(train/val): 10.24/10.11, Loss(train/val) 2.68/2.38. Took 20.28 sec\n",
            "Epoch 9, Acc(train/val): 10.24/10.41, Loss(train/val) 2.69/2.68. Took 19.98 sec\n",
            "| \u001b[39m94       \u001b[39m | \u001b[39m-2.684   \u001b[39m | \u001b[39m0.881    \u001b[39m | \u001b[39m0.1452   \u001b[39m | \u001b[39m2.617e+03\u001b[39m | \u001b[39m3.84e-05 \u001b[39m | \u001b[39m0.002278 \u001b[39m | \u001b[39m4.978    \u001b[39m | \u001b[39m2.049    \u001b[39m | \u001b[39m179.3    \u001b[39m | \u001b[39m17.76    \u001b[39m | \u001b[39m0.8835   \u001b[39m | \u001b[39m0.3037   \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=2, hid_dim=1011, act='sigmoid', dropout=np.float64(0.4169323736739267), use_bn=False, l2=np.float64(0.0005565673164856746), w_initial='xavier', optim='RMSprop', lr=np.float64(0.009576310768931673), epoch=10, train_batch_size=23, test_batch_size=157, exp_name='exp93')\n",
            "Epoch 0, Acc(train/val): 12.07/10.70, Loss(train/val) 4.15/5.38. Took 10.06 sec\n",
            "Epoch 1, Acc(train/val): 12.37/14.60, Loss(train/val) 3.75/6.21. Took 11.73 sec\n",
            "Epoch 2, Acc(train/val): 12.40/9.88, Loss(train/val) 3.83/7.91. Took 10.08 sec\n",
            "Epoch 3, Acc(train/val): 12.35/9.61, Loss(train/val) 3.68/4.61. Took 12.51 sec\n",
            "Epoch 4, Acc(train/val): 12.22/12.94, Loss(train/val) 3.75/3.73. Took 11.31 sec\n",
            "Epoch 5, Acc(train/val): 12.17/15.73, Loss(train/val) 3.74/4.98. Took 11.74 sec\n",
            "Epoch 6, Acc(train/val): 12.27/10.92, Loss(train/val) 3.77/5.22. Took 11.65 sec\n",
            "Epoch 7, Acc(train/val): 12.23/15.30, Loss(train/val) 3.74/4.76. Took 13.04 sec\n",
            "Epoch 8, Acc(train/val): 12.07/10.41, Loss(train/val) 3.74/5.19. Took 11.54 sec\n",
            "Epoch 9, Acc(train/val): 12.10/14.20, Loss(train/val) 3.77/4.87. Took 10.26 sec\n",
            "| \u001b[39m95       \u001b[39m | \u001b[39m-4.874   \u001b[39m | \u001b[39m0.9361   \u001b[39m | \u001b[39m0.4169   \u001b[39m | \u001b[39m1.011e+03\u001b[39m | \u001b[39m0.0005566\u001b[39m | \u001b[39m0.009576 \u001b[39m | \u001b[39m2.121    \u001b[39m | \u001b[39m1.183    \u001b[39m | \u001b[39m157.9    \u001b[39m | \u001b[39m23.61    \u001b[39m | \u001b[39m0.6083   \u001b[39m | \u001b[39m0.9883   \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=2, hid_dim=2790, act='relu', dropout=np.float64(0.43243719001350456), use_bn=True, l2=np.float64(0.00038414185390569616), w_initial='he', optim='ADAM', lr=np.float64(0.00548982985211813), epoch=10, train_batch_size=185, test_batch_size=114, exp_name='exp94')\n",
            "Epoch 0, Acc(train/val): 29.03/34.07, Loss(train/val) 2.23/1.99. Took 7.44 sec\n",
            "Epoch 1, Acc(train/val): 37.09/37.12, Loss(train/val) 1.83/1.73. Took 6.93 sec\n",
            "Epoch 2, Acc(train/val): 38.77/39.65, Loss(train/val) 1.71/1.70. Took 7.14 sec\n",
            "Epoch 3, Acc(train/val): 41.09/40.55, Loss(train/val) 1.65/1.67. Took 7.45 sec\n",
            "Epoch 4, Acc(train/val): 42.41/9.84, Loss(train/val) 1.61/16.18. Took 7.22 sec\n",
            "Epoch 5, Acc(train/val): 41.13/42.15, Loss(train/val) 1.64/1.63. Took 7.22 sec\n",
            "Epoch 6, Acc(train/val): 43.28/42.78, Loss(train/val) 1.59/1.62. Took 6.77 sec\n",
            "Epoch 7, Acc(train/val): 42.65/42.11, Loss(train/val) 1.60/1.63. Took 6.44 sec\n",
            "Epoch 8, Acc(train/val): 42.94/41.19, Loss(train/val) 1.59/1.63. Took 6.57 sec\n",
            "Epoch 9, Acc(train/val): 41.42/38.45, Loss(train/val) 1.63/1.69. Took 7.17 sec\n",
            "| \u001b[39m96       \u001b[39m | \u001b[39m-1.69    \u001b[39m | \u001b[39m0.1877   \u001b[39m | \u001b[39m0.4324   \u001b[39m | \u001b[39m2.79e+03 \u001b[39m | \u001b[39m0.0003841\u001b[39m | \u001b[39m0.00549  \u001b[39m | \u001b[39m2.913    \u001b[39m | \u001b[39m2.263    \u001b[39m | \u001b[39m114.1    \u001b[39m | \u001b[39m185.6    \u001b[39m | \u001b[39m0.004191 \u001b[39m | \u001b[39m0.3972   \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=2, hid_dim=1323, act='sigmoid', dropout=np.float64(0.35745173590942814), use_bn=False, l2=np.float64(0.0007197699933538517), w_initial='xavier', optim='ADAM', lr=np.float64(0.0026583250135565556), epoch=10, train_batch_size=216, test_batch_size=92, exp_name='exp95')\n",
            "Epoch 0, Acc(train/val): 29.55/31.96, Loss(train/val) 1.96/1.85. Took 6.51 sec\n",
            "Epoch 1, Acc(train/val): 34.18/32.69, Loss(train/val) 1.82/1.86. Took 7.31 sec\n",
            "Epoch 2, Acc(train/val): 34.97/35.03, Loss(train/val) 1.79/1.79. Took 7.12 sec\n",
            "Epoch 3, Acc(train/val): 35.41/37.04, Loss(train/val) 1.78/1.74. Took 7.37 sec\n",
            "Epoch 4, Acc(train/val): 35.70/35.97, Loss(train/val) 1.77/1.78. Took 6.99 sec\n",
            "Epoch 5, Acc(train/val): 36.41/35.69, Loss(train/val) 1.76/1.76. Took 6.84 sec\n",
            "Epoch 6, Acc(train/val): 36.45/34.39, Loss(train/val) 1.75/1.75. Took 7.05 sec\n",
            "Epoch 7, Acc(train/val): 36.49/33.53, Loss(train/val) 1.75/1.81. Took 6.78 sec\n",
            "Epoch 8, Acc(train/val): 37.26/36.67, Loss(train/val) 1.74/1.75. Took 7.36 sec\n",
            "Epoch 9, Acc(train/val): 36.97/35.99, Loss(train/val) 1.74/1.76. Took 7.10 sec\n",
            "| \u001b[39m97       \u001b[39m | \u001b[39m-1.759   \u001b[39m | \u001b[39m0.8061   \u001b[39m | \u001b[39m0.3575   \u001b[39m | \u001b[39m1.323e+03\u001b[39m | \u001b[39m0.0007198\u001b[39m | \u001b[39m0.002658 \u001b[39m | \u001b[39m2.847    \u001b[39m | \u001b[39m2.867    \u001b[39m | \u001b[39m92.82    \u001b[39m | \u001b[39m216.4    \u001b[39m | \u001b[39m0.9822   \u001b[39m | \u001b[39m0.5915   \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=1, hid_dim=1323, act='leakyrelu', dropout=np.float64(0.08945851533801191), use_bn=True, l2=np.float64(0.000956693812851234), w_initial='xavier', optim='RMSprop', lr=np.float64(0.0016856368655669776), epoch=10, train_batch_size=214, test_batch_size=86, exp_name='exp96')\n",
            "Epoch 0, Acc(train/val): 35.77/37.70, Loss(train/val) 4.17/1.97. Took 6.34 sec\n",
            "Epoch 1, Acc(train/val): 43.04/42.76, Loss(train/val) 1.69/1.63. Took 6.54 sec\n",
            "Epoch 2, Acc(train/val): 43.08/41.16, Loss(train/val) 1.77/1.69. Took 7.08 sec\n",
            "Epoch 3, Acc(train/val): 44.13/43.66, Loss(train/val) 1.93/1.65. Took 7.01 sec\n",
            "Epoch 4, Acc(train/val): 45.24/37.16, Loss(train/val) 1.88/2.55. Took 7.31 sec\n",
            "Epoch 5, Acc(train/val): 48.97/46.14, Loss(train/val) 1.50/1.54. Took 6.49 sec\n",
            "Epoch 6, Acc(train/val): 47.16/44.09, Loss(train/val) 1.87/1.61. Took 7.27 sec\n",
            "Epoch 7, Acc(train/val): 48.63/45.18, Loss(train/val) 1.68/1.59. Took 7.72 sec\n",
            "Epoch 8, Acc(train/val): 48.12/45.69, Loss(train/val) 2.07/1.67. Took 7.22 sec\n",
            "Epoch 9, Acc(train/val): 51.43/47.98, Loss(train/val) 1.42/1.52. Took 7.09 sec\n",
            "| \u001b[39m98       \u001b[39m | \u001b[39m-1.518   \u001b[39m | \u001b[39m0.46     \u001b[39m | \u001b[39m0.08946  \u001b[39m | \u001b[39m1.323e+03\u001b[39m | \u001b[39m0.0009567\u001b[39m | \u001b[39m0.001686 \u001b[39m | \u001b[39m1.153    \u001b[39m | \u001b[39m1.388    \u001b[39m | \u001b[39m86.99    \u001b[39m | \u001b[39m214.7    \u001b[39m | \u001b[39m0.4724   \u001b[39m | \u001b[39m0.6927   \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=2, hid_dim=920, act='relu', dropout=np.float64(0.4136529843676527), use_bn=True, l2=np.float64(0.00042652571939262417), w_initial='he', optim='SGD', lr=np.float64(0.00366282144169275), epoch=10, train_batch_size=12, test_batch_size=243, exp_name='exp97')\n",
            "Epoch 0, Acc(train/val): 35.69/41.94, Loss(train/val) 1.82/1.65. Took 16.79 sec\n",
            "Epoch 1, Acc(train/val): 41.33/43.72, Loss(train/val) 1.67/1.59. Took 17.13 sec\n",
            "Epoch 2, Acc(train/val): 43.82/45.81, Loss(train/val) 1.59/1.56. Took 16.96 sec\n",
            "Epoch 3, Acc(train/val): 45.69/48.07, Loss(train/val) 1.55/1.50. Took 15.62 sec\n",
            "Epoch 4, Acc(train/val): 47.46/47.71, Loss(train/val) 1.50/1.50. Took 18.97 sec\n",
            "Epoch 5, Acc(train/val): 48.66/48.93, Loss(train/val) 1.47/1.50. Took 16.85 sec\n",
            "Epoch 6, Acc(train/val): 49.52/48.54, Loss(train/val) 1.43/1.49. Took 17.43 sec\n",
            "Epoch 7, Acc(train/val): 50.73/50.27, Loss(train/val) 1.40/1.45. Took 18.24 sec\n",
            "Epoch 8, Acc(train/val): 51.49/50.99, Loss(train/val) 1.38/1.46. Took 17.13 sec\n",
            "Epoch 9, Acc(train/val): 52.48/50.60, Loss(train/val) 1.35/1.45. Took 15.55 sec\n",
            "| \u001b[39m99       \u001b[39m | \u001b[39m-1.452   \u001b[39m | \u001b[39m0.2208   \u001b[39m | \u001b[39m0.4137   \u001b[39m | \u001b[39m920.8    \u001b[39m | \u001b[39m0.0004265\u001b[39m | \u001b[39m0.003663 \u001b[39m | \u001b[39m2.149    \u001b[39m | \u001b[39m0.05143  \u001b[39m | \u001b[39m243.4    \u001b[39m | \u001b[39m12.04    \u001b[39m | \u001b[39m0.3113   \u001b[39m | \u001b[39m0.06428  \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=2, hid_dim=1327, act='relu', dropout=np.float64(0.15854500453715825), use_bn=False, l2=np.float64(0.0007210230731840136), w_initial='xavier', optim='ADAM', lr=np.float64(0.007126050772233336), epoch=10, train_batch_size=222, test_batch_size=96, exp_name='exp98')\n",
            "Epoch 0, Acc(train/val): 27.07/33.49, Loss(train/val) 2.89/1.83. Took 7.04 sec\n",
            "Epoch 1, Acc(train/val): 34.56/34.46, Loss(train/val) 1.81/1.81. Took 6.60 sec\n",
            "Epoch 2, Acc(train/val): 35.17/35.09, Loss(train/val) 1.81/1.91. Took 7.09 sec\n",
            "Epoch 3, Acc(train/val): 34.66/35.55, Loss(train/val) 1.81/1.81. Took 6.81 sec\n",
            "Epoch 4, Acc(train/val): 34.40/33.27, Loss(train/val) 1.82/1.83. Took 6.77 sec\n",
            "Epoch 5, Acc(train/val): 35.53/35.58, Loss(train/val) 1.79/1.76. Took 6.98 sec\n",
            "Epoch 6, Acc(train/val): 35.09/37.89, Loss(train/val) 1.79/1.76. Took 6.95 sec\n",
            "Epoch 7, Acc(train/val): 35.51/34.22, Loss(train/val) 1.80/1.84. Took 6.98 sec\n",
            "Epoch 8, Acc(train/val): 35.08/34.71, Loss(train/val) 1.81/1.80. Took 6.54 sec\n",
            "Epoch 9, Acc(train/val): 35.51/30.56, Loss(train/val) 1.80/2.31. Took 7.09 sec\n",
            "| \u001b[39m100      \u001b[39m | \u001b[39m-2.313   \u001b[39m | \u001b[39m0.1499   \u001b[39m | \u001b[39m0.1585   \u001b[39m | \u001b[39m1.328e+03\u001b[39m | \u001b[39m0.000721 \u001b[39m | \u001b[39m0.007126 \u001b[39m | \u001b[39m2.455    \u001b[39m | \u001b[39m2.902    \u001b[39m | \u001b[39m96.42    \u001b[39m | \u001b[39m222.1    \u001b[39m | \u001b[39m0.9358   \u001b[39m | \u001b[39m0.5941   \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=4, hid_dim=1318, act='leakyrelu', dropout=np.float64(0.32516154669982394), use_bn=False, l2=np.float64(0.00043842220498611304), w_initial='he', optim='SGD', lr=np.float64(0.004150396408987805), epoch=10, train_batch_size=220, test_batch_size=89, exp_name='exp99')\n",
            "Epoch 0, Acc(train/val): 16.84/26.44, Loss(train/val) 2.24/2.14. Took 7.64 sec\n",
            "Epoch 1, Acc(train/val): 24.46/29.60, Loss(train/val) 2.10/2.01. Took 7.11 sec\n",
            "Epoch 2, Acc(train/val): 28.45/32.31, Loss(train/val) 2.00/1.92. Took 6.77 sec\n",
            "Epoch 3, Acc(train/val): 31.30/34.00, Loss(train/val) 1.93/1.86. Took 6.45 sec\n",
            "Epoch 4, Acc(train/val): 32.99/35.83, Loss(train/val) 1.87/1.82. Took 6.03 sec\n",
            "Epoch 5, Acc(train/val): 34.69/36.93, Loss(train/val) 1.83/1.78. Took 7.80 sec\n",
            "Epoch 6, Acc(train/val): 36.02/38.00, Loss(train/val) 1.79/1.75. Took 6.87 sec\n",
            "Epoch 7, Acc(train/val): 37.06/38.98, Loss(train/val) 1.76/1.72. Took 6.94 sec\n",
            "Epoch 8, Acc(train/val): 38.20/39.96, Loss(train/val) 1.73/1.70. Took 6.53 sec\n",
            "Epoch 9, Acc(train/val): 38.88/40.55, Loss(train/val) 1.71/1.68. Took 6.95 sec\n",
            "| \u001b[39m101      \u001b[39m | \u001b[39m-1.678   \u001b[39m | \u001b[39m0.424    \u001b[39m | \u001b[39m0.3252   \u001b[39m | \u001b[39m1.319e+03\u001b[39m | \u001b[39m0.0004384\u001b[39m | \u001b[39m0.00415  \u001b[39m | \u001b[39m4.901    \u001b[39m | \u001b[39m0.6784   \u001b[39m | \u001b[39m89.16    \u001b[39m | \u001b[39m220.8    \u001b[39m | \u001b[39m0.7658   \u001b[39m | \u001b[39m0.1492   \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=3, hid_dim=971, act='tanh', dropout=np.float64(0.20303948592422005), use_bn=False, l2=np.float64(0.0002312361365996439), w_initial='xavier', optim='ADAM', lr=np.float64(0.007617159143071493), epoch=10, train_batch_size=119, test_batch_size=216, exp_name='exp100')\n",
            "Epoch 0, Acc(train/val): 21.21/22.78, Loss(train/val) 2.37/2.35. Took 7.36 sec\n",
            "Epoch 1, Acc(train/val): 23.81/26.51, Loss(train/val) 2.27/2.17. Took 7.02 sec\n",
            "Epoch 2, Acc(train/val): 23.66/24.26, Loss(train/val) 2.27/2.36. Took 6.83 sec\n",
            "Epoch 3, Acc(train/val): 23.06/23.08, Loss(train/val) 2.31/2.30. Took 7.34 sec\n",
            "Epoch 4, Acc(train/val): 23.00/22.80, Loss(train/val) 2.32/2.52. Took 7.10 sec\n",
            "Epoch 5, Acc(train/val): 23.68/25.60, Loss(train/val) 2.32/2.36. Took 7.52 sec\n",
            "Epoch 6, Acc(train/val): 23.61/23.18, Loss(train/val) 2.30/2.41. Took 7.39 sec\n",
            "Epoch 7, Acc(train/val): 23.02/21.61, Loss(train/val) 2.34/2.30. Took 7.46 sec\n",
            "Epoch 8, Acc(train/val): 23.15/24.11, Loss(train/val) 2.31/2.29. Took 7.05 sec\n",
            "Epoch 9, Acc(train/val): 23.29/20.99, Loss(train/val) 2.32/2.54. Took 7.63 sec\n",
            "| \u001b[39m102      \u001b[39m | \u001b[39m-2.542   \u001b[39m | \u001b[39m0.6463   \u001b[39m | \u001b[39m0.203    \u001b[39m | \u001b[39m971.0    \u001b[39m | \u001b[39m0.0002312\u001b[39m | \u001b[39m0.007617 \u001b[39m | \u001b[39m3.12     \u001b[39m | \u001b[39m2.416    \u001b[39m | \u001b[39m216.5    \u001b[39m | \u001b[39m119.3    \u001b[39m | \u001b[39m0.6353   \u001b[39m | \u001b[39m0.856    \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=5, hid_dim=2185, act='leakyrelu', dropout=np.float64(0.159314926852419), use_bn=True, l2=np.float64(0.00035234617928436943), w_initial='xavier', optim='RMSprop', lr=np.float64(0.004949944679278894), epoch=10, train_batch_size=40, test_batch_size=157, exp_name='exp101')\n",
            "Epoch 0, Acc(train/val): 19.32/22.05, Loss(train/val) 3.56/2.13. Took 9.78 sec\n",
            "Epoch 1, Acc(train/val): 24.50/26.50, Loss(train/val) 2.01/2.05. Took 10.02 sec\n",
            "Epoch 2, Acc(train/val): 27.59/28.49, Loss(train/val) 1.94/1.95. Took 10.74 sec\n",
            "Epoch 3, Acc(train/val): 29.72/32.99, Loss(train/val) 1.91/1.87. Took 9.43 sec\n",
            "Epoch 4, Acc(train/val): 31.11/25.44, Loss(train/val) 1.88/1.95. Took 10.27 sec\n",
            "Epoch 5, Acc(train/val): 31.59/33.54, Loss(train/val) 1.87/1.83. Took 10.77 sec\n",
            "Epoch 6, Acc(train/val): 31.40/28.96, Loss(train/val) 1.87/1.90. Took 10.10 sec\n",
            "Epoch 7, Acc(train/val): 31.39/30.96, Loss(train/val) 1.87/1.86. Took 10.29 sec\n",
            "Epoch 8, Acc(train/val): 31.75/30.24, Loss(train/val) 1.87/1.88. Took 10.35 sec\n",
            "Epoch 9, Acc(train/val): 31.70/23.98, Loss(train/val) 1.86/2.13. Took 10.78 sec\n",
            "| \u001b[39m103      \u001b[39m | \u001b[39m-2.128   \u001b[39m | \u001b[39m0.2683   \u001b[39m | \u001b[39m0.1593   \u001b[39m | \u001b[39m2.186e+03\u001b[39m | \u001b[39m0.0003523\u001b[39m | \u001b[39m0.00495  \u001b[39m | \u001b[39m5.851    \u001b[39m | \u001b[39m1.632    \u001b[39m | \u001b[39m157.0    \u001b[39m | \u001b[39m40.24    \u001b[39m | \u001b[39m0.4943   \u001b[39m | \u001b[39m0.8491   \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=4, hid_dim=2778, act='relu', dropout=np.float64(0.40912975957652337), use_bn=False, l2=np.float64(0.0006660338483618235), w_initial='xavier', optim='SGD', lr=np.float64(0.008444944408443457), epoch=10, train_batch_size=180, test_batch_size=116, exp_name='exp102')\n",
            "Epoch 0, Acc(train/val): 17.70/26.67, Loss(train/val) 2.25/2.15. Took 6.95 sec\n",
            "Epoch 1, Acc(train/val): 25.95/28.15, Loss(train/val) 2.10/2.00. Took 7.09 sec\n",
            "Epoch 2, Acc(train/val): 29.98/31.18, Loss(train/val) 1.97/1.90. Took 7.35 sec\n",
            "Epoch 3, Acc(train/val): 32.55/34.23, Loss(train/val) 1.88/1.83. Took 7.10 sec\n",
            "Epoch 4, Acc(train/val): 35.00/37.10, Loss(train/val) 1.81/1.77. Took 6.54 sec\n",
            "Epoch 5, Acc(train/val): 36.96/38.62, Loss(train/val) 1.76/1.73. Took 6.84 sec\n",
            "Epoch 6, Acc(train/val): 38.49/39.63, Loss(train/val) 1.72/1.70. Took 7.05 sec\n",
            "Epoch 7, Acc(train/val): 40.04/40.36, Loss(train/val) 1.68/1.67. Took 7.39 sec\n",
            "Epoch 8, Acc(train/val): 41.22/41.29, Loss(train/val) 1.65/1.65. Took 6.83 sec\n",
            "Epoch 9, Acc(train/val): 42.59/42.40, Loss(train/val) 1.61/1.61. Took 7.11 sec\n",
            "| \u001b[39m104      \u001b[39m | \u001b[39m-1.615   \u001b[39m | \u001b[39m0.1499   \u001b[39m | \u001b[39m0.4091   \u001b[39m | \u001b[39m2.779e+03\u001b[39m | \u001b[39m0.000666 \u001b[39m | \u001b[39m0.008445 \u001b[39m | \u001b[39m4.388    \u001b[39m | \u001b[39m0.3971   \u001b[39m | \u001b[39m116.9    \u001b[39m | \u001b[39m180.3    \u001b[39m | \u001b[39m0.6303   \u001b[39m | \u001b[39m0.7627   \u001b[39m |\n",
            "Namespace(in_dim=3072, out_dim=10, n_layer=2, hid_dim=1000, act='relu', dropout=np.float64(0.42122653894953827), use_bn=False, l2=np.float64(0.0004974761769904128), w_initial='he', optim='ADAM', lr=np.float64(0.009865908121583888), epoch=10, train_batch_size=38, test_batch_size=154, exp_name='exp103')\n",
            "Epoch 0, Acc(train/val): 12.80/9.26, Loss(train/val) 2.66/2.32. Took 8.72 sec\n",
            "Epoch 1, Acc(train/val): 10.28/9.09, Loss(train/val) 2.41/2.31. Took 8.73 sec\n",
            "Epoch 2, Acc(train/val): 9.98/10.09, Loss(train/val) 2.52/2.30. Took 8.73 sec\n",
            "Epoch 3, Acc(train/val): 10.28/9.12, Loss(train/val) 2.37/2.30. Took 8.25 sec\n",
            "Epoch 4, Acc(train/val): 10.23/10.39, Loss(train/val) 2.49/2.62. Took 8.39 sec\n",
            "Epoch 5, Acc(train/val): 10.07/9.12, Loss(train/val) 2.56/2.31. Took 8.34 sec\n",
            "Epoch 6, Acc(train/val): 10.12/9.61, Loss(train/val) 2.55/2.30. Took 8.85 sec\n",
            "Epoch 7, Acc(train/val): 10.11/10.11, Loss(train/val) 2.59/2.30. Took 7.45 sec\n",
            "Epoch 8, Acc(train/val): 10.16/10.55, Loss(train/val) 3.15/7.01. Took 7.88 sec\n",
            "Epoch 9, Acc(train/val): 10.00/10.08, Loss(train/val) 2.41/2.31. Took 7.59 sec\n",
            "| \u001b[39m105      \u001b[39m | \u001b[39m-2.305   \u001b[39m | \u001b[39m0.0933   \u001b[39m | \u001b[39m0.4212   \u001b[39m | \u001b[39m1e+03    \u001b[39m | \u001b[39m0.0004975\u001b[39m | \u001b[39m0.009866 \u001b[39m | \u001b[39m2.023    \u001b[39m | \u001b[39m2.219    \u001b[39m | \u001b[39m154.0    \u001b[39m | \u001b[39m38.39    \u001b[39m | \u001b[39m0.856    \u001b[39m | \u001b[39m0.02218  \u001b[39m |\n",
            "=============================================================================================================================================================\n"
          ]
        }
      ],
      "source": [
        "from bayes_opt import BayesianOptimization\n",
        "\n",
        "# ====== Random Seed Initialization ====== #\n",
        "seed = 123\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "args = parser.parse_args(\"\")\n",
        "\n",
        "# ====== Model Capacity ====== #\n",
        "args.in_dim = 3072\n",
        "args.out_dim = 10\n",
        "\n",
        "cnt = 0\n",
        "\n",
        "# ====== Experiment Variable ====== #\n",
        "def exp_set(n_layer, hid_dim, act, dropout, use_bn, l2, w_initial, optim, lr, train_batch_size, test_batch_size):\n",
        "    setattr(args, 'n_layer', int(n_layer))\n",
        "    setattr(args, 'hid_dim', int(hid_dim))\n",
        "    if act < 0.25:\n",
        "        act = 'relu'\n",
        "    elif act < 0.5:\n",
        "        act = 'leakyrelu'\n",
        "    elif act < 0.75:\n",
        "        act = 'tanh'\n",
        "    else:\n",
        "        act = 'sigmoid'\n",
        "    setattr(args, 'act', act)\n",
        "    setattr(args, 'dropout', dropout)\n",
        "    if use_bn < 0.5:\n",
        "        use_bn = True\n",
        "    else:\n",
        "        use_bn = False\n",
        "    setattr(args, 'use_bn', use_bn)\n",
        "    setattr(args, 'l2', l2)\n",
        "    if w_initial < 0.5:\n",
        "        w_initial = 'he'\n",
        "    else:\n",
        "        w_initial = 'xavier'\n",
        "    setattr(args, 'w_initial', w_initial)\n",
        "    if optim < 1:\n",
        "        optim = 'SGD'\n",
        "    elif optim < 2:\n",
        "        optim = 'RMSprop'\n",
        "    else:\n",
        "        optim = 'ADAM'\n",
        "    setattr(args, 'optim', optim)\n",
        "    setattr(args, 'lr', lr)\n",
        "    setattr(args, 'epoch', 10)\n",
        "    setattr(args, 'train_batch_size', int(train_batch_size))\n",
        "    setattr(args, 'test_batch_size', int(test_batch_size))\n",
        "    print(args)\n",
        "    global cnt\n",
        "    args.exp_name = f\"exp{cnt}\"\n",
        "    cnt += 1\n",
        "\n",
        "    setting, result = experiment(partition, deepcopy(args))\n",
        "    save_exp_result(setting, result)\n",
        "    return -1*result['val_losses'][-1]\n",
        "\n",
        "optimizer = BayesianOptimization(\n",
        "            f=exp_set,\n",
        "            pbounds={\n",
        "                    'n_layer': (1, 6),\n",
        "                    'hid_dim': (100, 3000),\n",
        "                    'act': (0,1),\n",
        "                    'dropout': (0.0, 0.5),\n",
        "                    'use_bn': (0,1),\n",
        "                    'l2': (0.000001, 0.001),\n",
        "                    'w_initial': (0,1),\n",
        "                    'optim': (0,3),\n",
        "                    'lr': (0.000001, 0.01),\n",
        "                    'train_batch_size': (4, 256),\n",
        "                    'test_batch_size': (4, 256)\n",
        "                    },\n",
        "            random_state=seed,\n",
        "            verbose = 2\n",
        "            )\n",
        "\n",
        "optimizer.maximize(n_iter=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ot2r4RD6mpHx"
      },
      "source": [
        "# **Visualizing the results**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "93\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "train_losses        [1.9066672624364074, 1.6550578886841634, 1.554...\n",
              "val_losses          [1.694852121464618, 1.589529783694775, 1.51780...\n",
              "train_accs          [31.445, 40.6875, 44.9575, 47.665, 50.095, 51....\n",
              "val_accs            [39.22, 43.76, 46.25, 47.81, 48.85, 49.87, 50....\n",
              "train_acc                                                       58.41\n",
              "val_acc                                                         52.89\n",
              "test_acc                                                        53.51\n",
              "in_dim                                                           3072\n",
              "out_dim                                                            10\n",
              "n_layer                                                             4\n",
              "hid_dim                                                           955\n",
              "act                                                              relu\n",
              "dropout                                                      0.268817\n",
              "use_bn                                                          False\n",
              "l2                                                           0.000644\n",
              "w_initial                                                          he\n",
              "optim                                                             SGD\n",
              "lr                                                           0.005387\n",
              "train_batch_size                                                   27\n",
              "exp_name                                                        exp30\n",
              "Name: 93, dtype: object"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "def load_all_results():\n",
        "    dir_path = './results'\n",
        "    filenames = [f for f in os.listdir(dir_path) if os.path.isfile(os.path.join(dir_path, f)) and f.endswith('.json')]\n",
        "    list_result = []\n",
        "    for filename in filenames:\n",
        "        with open(os.path.join(dir_path, filename), 'r') as infile:\n",
        "            results = json.load(infile)\n",
        "            list_result.append(results)\n",
        "    df = pd.DataFrame(list_result)\n",
        "    return df\n",
        "df = load_all_results()\n",
        "print(df['test_acc'].argmax())\n",
        "df.iloc[93]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 382.5x300 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def plot_acc_(df, **kwargs):\n",
        "\n",
        "    list_data = []\n",
        "\n",
        "\n",
        "    train_accs = list(df.train_accs)[0]\n",
        "    val_accs = list(df.val_accs)[0]\n",
        "    test_acc = list(df.test_acc)[0]\n",
        "\n",
        "    for epoch, train_acc in enumerate(train_accs):\n",
        "        list_data.append({'type':'train', 'Acc':train_acc, 'test_acc':test_acc, 'epoch':epoch})\n",
        "    for epoch, val_acc in enumerate(val_accs):\n",
        "        list_data.append({'type':'val', 'Acc':val_acc, 'test_acc':test_acc, 'epoch':epoch})\n",
        "\n",
        "    df = pd.DataFrame(list_data)\n",
        "    g = sns.FacetGrid(df, hue='type', **kwargs)\n",
        "    g = g.map(plt.plot, 'epoch', 'Acc', marker='.')\n",
        "\n",
        "    def show_acc(x, y, metric, **kwargs):\n",
        "        plt.scatter(x, y, alpha=0.3, s=1)\n",
        "        metric = \"Test Acc: {:1.3f}\".format(list(metric.values)[0])\n",
        "        plt.text(0.05, 0.95, metric,  horizontalalignment='left', verticalalignment='center', transform=plt.gca().transAxes, bbox=dict(facecolor='yellow', alpha=0.5, boxstyle=\"round,pad=0.1\"))\n",
        "    g = g.map(show_acc, 'epoch', 'Acc', 'test_acc')\n",
        "\n",
        "    g.add_legend()\n",
        "    g.fig.suptitle('Train Accuracy vs Val Accuracy')\n",
        "    plt.subplots_adjust(top=0.89)\n",
        "    \n",
        "df = load_exp_result('exp30-4f85e0')\n",
        "plot_acc_(df, margin_titles=True, sharey=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "hps",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
